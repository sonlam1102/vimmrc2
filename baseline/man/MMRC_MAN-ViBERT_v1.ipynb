{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1676547295902,"user":{"displayName":"Tường Phạm Quang","userId":"05102412219596796118"},"user_tz":-420},"id":"kimkG7xp6wvS","outputId":"2ef7dbb1-fde2-4a4d-e25c-9cf065464e82"},"outputs":[{"output_type":"stream","name":"stdout","text":["Thu Feb 16 11:34:55 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   67C    P0    31W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3699,"status":"ok","timestamp":1676547301132,"user":{"displayName":"Tường Phạm Quang","userId":"05102412219596796118"},"user_tz":-420},"id":"n8OLTA2cqdHk","outputId":"603f9505-27d1-4d68-e20c-d1a9e5386136"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1676547301132,"user":{"displayName":"Tường Phạm Quang","userId":"05102412219596796118"},"user_tz":-420},"id":"G4x_F5OtS5V8","outputId":"db7c16e7-3fd9-4a9c-b767-869d928de694"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1A2lqKb4qo-FtxvfSl5gHiEC9QAa1uKxO/ViMMRC_model\n"]}],"source":["%cd '/content/drive/MyDrive/NCKH/ViMMRC_model/'"]},{"cell_type":"markdown","metadata":{"id":"K_Pv9CIYm7BZ"},"source":["# Library\n","\n","https://github.com/mhardalov/exams-qa \\\n","https://github.com/microsoft/MT-DNN \\\n","https://github.com/jind11/MMM-MCQA"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":122436,"status":"ok","timestamp":1676547271429,"user":{"displayName":"Tường Phạm Quang","userId":"05102412219596796118"},"user_tz":-420},"id":"7znvNnvg-0de","outputId":"0c066496-7153-4a1f-e2f4-44468263f135"},"outputs":[{"output_type":"stream","name":"stdout","text":["  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m753.4/753.4 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 KB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.3/114.3 KB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 KB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.3/216.3 KB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m635.7/635.7 KB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 KB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m96.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 KB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m104.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for fairseq (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for fastBPE (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.14.1+cu116 requires torch==1.13.1, but you have torch 1.4.0 which is incompatible.\n","torchtext 0.14.1 requires torch==1.13.1, but you have torch 1.4.0 which is incompatible.\n","torchaudio 0.13.1+cu116 requires torch==1.13.1, but you have torch 1.4.0 which is incompatible.\n","fastai 2.7.10 requires torch<1.14,>=1.7, but you have torch 1.4.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["%pip -q install -e git+https://github.com/microsoft/MT-DNN.git#egg=mtdnn"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":98331,"status":"ok","timestamp":1676547412557,"user":{"displayName":"Tường Phạm Quang","userId":"05102412219596796118"},"user_tz":-420},"id":"ksz16DuhgUtk","outputId":"25d5d7b0-eb69-493f-f20d-0a25ecb4c0da"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.4/887.4 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 KB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","mtdnn 1.1.0 requires torch==1.4.0, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","mtdnn 1.1.0 requires torch==1.4.0, but you have torch 1.13.1 which is incompatible.\n","mtdnn 1.1.0 requires transformers==2.9.0, but you have transformers 4.26.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["%pip -q install torch -U\n","%pip -q install transformers -U\n","%pip -q install tqdm -U\n","# %pip -q install --upgrade  git+https://github.com/lanpa/tensorboardX.git"]},{"cell_type":"markdown","metadata":{"id":"dT2PDGixT400"},"source":["Install apex package: https://stackoverflow.com/a/74561776\n","\n","Query the version Ubuntu Colab is running on:\\\n","`!lsb_release -a`\n","\n","Get the current cuda version run:\\\n","`!nvcc --version`"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1676547412557,"user":{"displayName":"Tường Phạm Quang","userId":"05102412219596796118"},"user_tz":-420},"id":"qWFKkQbKTUBA","outputId":"d3b52965-e024-41c9-fe28-246003223455"},"outputs":[{"output_type":"stream","name":"stdout","text":["nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2022 NVIDIA Corporation\n","Built on Tue_Mar__8_18:18:20_PST_2022\n","Cuda compilation tools, release 11.6, V11.6.124\n","Build cuda_11.6.r11.6/compiler.31057947_0\n"]}],"source":["!nvcc --version"]},{"cell_type":"markdown","metadata":{"id":"ioQPuCNdks1x"},"source":["# Models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"svixfajDMreB"},"outputs":[],"source":["#%pip install git+git@github.com:microsoft/mt-dnn.git@master#egg=mtdnn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4x7vIh8LE40U"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn.parameter import Parameter\n","from mtdnn.common.dropout_wrapper import DropoutWrapper\n","from mtdnn.common.optimizer import weight_norm as WN\n","from mtdnn.common.similarity import FlatSimilarityWrapper, SelfAttnWrapper, SimilarityWrapper\n","from mtdnn.common.activation_functions import activation\n","\n","\n","class DualAttentionWrapper(nn.Module):\n","    def __init__(self, x1_dim, x2_dim, prefix='attention', opt={}, dropout=None):\n","        super(DualAttentionWrapper, self).__init__()\n","        self.prefix = prefix\n","        if dropout is None:\n","            self.att_dropout = DropoutWrapper(opt.get('{}_att_dropout'.format(self.prefix), 0))\n","        else:\n","            self.att_dropout = dropout\n","        self.score_func = SimilarityWrapper(x1_dim, x2_dim, prefix=prefix, opt=opt, dropout=self.att_dropout)\n","\n","    def forward(self, query, key, query_padding_mask=None, key_padding_mask=None, return_scores=False):\n","        logits = self.score_func(query, key)\n","        # print(query.shape, key.shape, logits.shape, key_padding_mask.sum(dim=-1).max(), key_padding_mask.shape)\n","        query_mask = query_padding_mask.unsqueeze(2).expand_as(logits)\n","        key_mask = key_padding_mask.unsqueeze(1).expand_as(logits)\n","\n","        # first get attn_query\n","        logits_key = logits.data.masked_fill(key_mask.data, -float('inf'))\n","        prob_key = F.softmax(logits_key, -1)\n","        # prob_key = prob_key.view(-1, query.size(1), key.size(1))\n","        prob_key = self.att_dropout(prob_key)\n","        attn_query = prob_key.bmm(key)\n","\n","        # then get attn_key\n","        logits_query = logits.data.masked_fill(query_mask.data, -float('inf'))\n","        prob_query = F.softmax(logits_query, 1)\n","        # prob_query = prob_query.view(-1, key.size(1), query.size(1))\n","        prob_query = self.att_dropout(prob_query)\n","        attn_key = prob_query.transpose(1, 2).bmm(query)\n","\n","        if return_scores:\n","            return attn_query, attn_key, prob_query, logits\n","        else:\n","            return attn_query, attn_key\n","\n","\n","class Classifier(nn.Module):\n","    def __init__(self, x_size, y_size, opt, prefix='decoder', dropout=None):\n","        super(Classifier, self).__init__()\n","        self.opt = opt\n","        if dropout is None:\n","            self.dropout = DropoutWrapper(opt.get('{}_dropout_p'.format(prefix), 0))\n","        else:\n","            self.dropout = dropout\n","        self.merge_opt = opt.get('{}_merge_opt'.format(prefix), 0)\n","        self.weight_norm_on = opt.get('{}_weight_norm_on'.format(prefix), False)\n","\n","        if self.merge_opt == 1:\n","            self.proj = nn.Linear(x_size * 4, y_size)\n","        else:\n","            self.proj = nn.Linear(x_size * 2, y_size)\n","\n","        if self.weight_norm_on:\n","            self.proj = WN(self.proj)\n","\n","    def forward(self, x1, x2, mask=None, activation=None):\n","        seq_len = None\n","        if len(x1.size()) == 3:\n","            bz, seq_len, hidden_size = x1.size()\n","            x1 = x1.contiguous().view(-1, hidden_size)\n","            x2 = x2.contiguous().view(-1, hidden_size)\n","\n","        if self.merge_opt == 1:\n","            x = torch.cat([x1, x2, (x1 - x2).abs(), x1 * x2], 1)\n","        else:\n","            x = torch.cat([x1, x2], 1)\n","        x = self.dropout(x)\n","        if activation:\n","            scores = activation(self.proj(x))\n","        else:\n","            scores = self.proj(x)\n","\n","        if seq_len:\n","            return scores.view(bz, seq_len, -1)\n","        else:\n","            return scores\n","\n","# mtdnn.common.san.SANClassifier() super\n","class SANClassifier(nn.Module):\n","    \"\"\"Implementation of Stochastic Answer Networks for Natural Language Inference, Xiaodong Liu, Kevin Duh and Jianfeng Gao\n","    https://arxiv.org/abs/1804.07888\n","    \"\"\"\n","    def __init__(self, x_size, h_size, label_size, opt={}, prefix='decoder', dropout=None):\n","        super(SANClassifier, self).__init__()\n","        self.prefix = prefix\n","        if dropout is None:\n","            self.dropout = DropoutWrapper(opt.get('{}_dropout_p'.format(self.prefix), 0))\n","        else:\n","            self.dropout = dropout\n","        self.query_wsum = SelfAttnWrapper(x_size, prefix='mem_cum', opt=opt, dropout=self.dropout)\n","        self.attn = FlatSimilarityWrapper(x_size, h_size, prefix, opt, self.dropout)\n","        self.rnn_type = '{}{}'.format(opt.get('{}_rnn_type'.format(prefix), 'gru').upper(), 'Cell')\n","        self.rnn = getattr(nn, self.rnn_type)(x_size, h_size)\n","        self.num_turn = opt.get('{}_num_turn'.format(prefix), 5)\n","        self.opt = opt\n","        self.mem_random_drop = opt.get('{}_mem_drop_p'.format(prefix), 0)\n","        self.mem_type = opt.get('{}_mem_type'.format(prefix), 0)\n","        self.weight_norm_on = opt.get('{}_weight_norm_on'.format(prefix), False)\n","        self.label_size = label_size\n","        self.dump_state = opt.get('dump_state_on', False)\n","        self.alpha = Parameter(torch.zeros(1, 1), requires_grad=False)\n","        # self.hyp_attn = None\n","        # if opt.get('hyp_attn_premise', 0):\n","        #     self.hyp_attn = AttentionWrapper(x_size, h_size, prefix=prefix, opt=opt, dropout=self.dropout)\n","        #     self.hyp_merge = Classifier(x_size, x_size, opt, prefix=prefix, dropout=self.dropout)\n","        self.f = activation(opt.get('{}_activation'.format(self.prefix), 'relu'))\n","        if self.weight_norm_on:\n","            self.rnn = WN(self.rnn)\n","\n","        self.classifier = Classifier(x_size, 1, opt, prefix=prefix, dropout=self.dropout)\n","\n","    def _generate_mask(self, new_data, dropout_p=0.0, is_training=False):\n","        if not is_training:\n","            dropout_p = 0.0\n","        new_data = (1 - dropout_p) * (new_data.zero_() + 1)\n","        for i in range(new_data.size(0)):\n","            one = random.randint(0, new_data.size(1) - 1)\n","            new_data[i][one] = 1\n","        mask = 1.0 / (1 - dropout_p) * torch.bernoulli(new_data)\n","        mask.requires_grad = False\n","        return \n","\n","    def forward(self, x, h0, x_mask=None, h_mask=None, is_training=True):\n","        h0 = self.query_wsum(h0, h_mask)\n","        if type(self.rnn) is nn.LSTMCell:\n","            c0 = h0.new(h0.size()).zero_()\n","        scores_list = []\n","        for turn in range(self.num_turn):\n","            att_scores = self.attn(x, h0, x_mask)\n","            x_sum = torch.bmm(F.softmax(att_scores, 1).unsqueeze(1), x).squeeze(1)\n","            scores = self.classifier(x_sum, h0)\n","            scores_list.append(scores)\n","            # next turn\n","            if self.rnn is not None:\n","                h0 = self.dropout(h0)\n","                if type(self.rnn) is nn.LSTMCell:\n","                    h0, c0 = self.rnn(x_sum, (h0, c0))\n","                else:\n","                    h0 = self.rnn(x_sum, h0)\n","        if self.mem_type == 1:\n","            batch_size = x.size(0) // self.label_size\n","            mask = self._generate_mask(self.alpha.data.new(batch_size, self.num_turn), self.mem_random_drop, is_training)\n","            mask = [m.contiguous() for m in torch.unbind(mask, 1)]\n","            tmp_scores_list = [mask[idx].view(batch_size, 1).expand_as(inp.view(-1, self.label_size))\n","                               * F.softmax(inp.view(-1, self.label_size), 1)\n","                               for idx, inp in enumerate(scores_list)]\n","            scores = torch.stack(tmp_scores_list, 2)\n","            scores = torch.mean(scores, 2)\n","            scores = torch.log(scores)\n","        else:\n","            scores = scores_list[-1]\n","        if self.dump_state:\n","            return scores, scores_list\n","        else:\n","            return scores\n","\n","\n","class SANClassifier2(nn.Module):\n","    \"\"\"Implementation of Stochastic Answer Networks for Natural Language Inference, Xiaodong Liu, Kevin Duh and Jianfeng Gao\n","    https://arxiv.org/abs/1804.07888\n","    \"\"\"\n","    def __init__(self, x_size, h_size, label_size, opt={}, prefix='decoder', dropout=None):\n","        super(SANClassifier2, self).__init__()\n","        self.prefix = prefix\n","        if dropout is None:\n","            self.dropout = DropoutWrapper(opt.get('{}_dropout_p'.format(self.prefix), 0))\n","        else:\n","            self.dropout = dropout\n","        self.dual_attn = DualAttentionWrapper(x_size, h_size, prefix, opt, self.dropout)\n","        self.query_wsum = SelfAttnWrapper(x_size, prefix='mem_cum', opt=opt, dropout=self.dropout)\n","        self.attn = FlatSimilarityWrapper(x_size, h_size, prefix, opt, self.dropout)\n","        self.rnn_type = '{}{}'.format(opt.get('{}_rnn_type'.format(prefix), 'gru').upper(), 'Cell')\n","        self.rnn = getattr(nn, self.rnn_type)(x_size, h_size)\n","        self.num_turn = opt.get('{}_num_turn'.format(prefix), 5)\n","        self.opt = opt\n","        self.mem_random_drop = opt.get('{}_mem_drop_p'.format(prefix), 0)\n","        self.mem_type = opt.get('{}_mem_type'.format(prefix), 0)\n","        self.weight_norm_on = opt.get('{}_weight_norm_on'.format(prefix), False)\n","        self.label_size = label_size\n","        self.dump_state = opt.get('dump_state_on', False)\n","        self.alpha = Parameter(torch.zeros(1, 1), requires_grad=False)\n","        self.f = activation(opt.get('{}_activation'.format(self.prefix), 'relu'))\n","        self.hyp_first = opt.get('{}_hyp_first'.format(prefix), 1)\n","        self.hyp_raw = opt.get('{}_hyp_raw'.format(prefix), 0)\n","        if self.weight_norm_on:\n","            self.rnn = WN(self.rnn)\n","\n","        self.classifier = Classifier(x_size, 1, opt, prefix=prefix, dropout=self.dropout)\n","\n","        self.premise_merge = Classifier(x_size, x_size, opt, prefix=prefix, dropout=self.dropout)\n","        self.hyp_merge = Classifier(x_size, x_size, opt, prefix=prefix, dropout=self.dropout)\n","\n","    def _generate_mask(self, new_data, dropout_p=0.0, is_training=False):\n","        if not is_training:\n","            dropout_p = 0.0\n","        new_data = (1 - dropout_p) * (new_data.zero_() + 1)\n","        for i in range(new_data.size(0)):\n","            one = random.randint(0, new_data.size(1) - 1)\n","            new_data[i][one] = 1\n","        mask = 1.0 / (1 - dropout_p) * torch.bernoulli(new_data)\n","        mask.requires_grad = False\n","        return \n","\n","    def forward(self, x, h, x_mask=None, h_mask=None, is_training=True):\n","        if self.hyp_first and self.hyp_raw:\n","            pass\n","        elif self.hyp_first and not self.hyp_raw:\n","            _, h_attn = self.dual_attn(x, h, x_mask, h_mask)\n","            h = self.hyp_merge(h, h_attn, activation=self.f)\n","        else:\n","            raise NotImplementedError\n","\n","        h0 = self.query_wsum(h, h_mask)\n","        if type(self.rnn) is nn.LSTMCell:\n","            c0 = h0.new(h0.size()).zero_()\n","        scores_list = []\n","        for turn in range(self.num_turn):\n","            att_scores = self.attn(x, h0, x_mask)\n","            x_sum = torch.bmm(F.softmax(att_scores, 1).unsqueeze(1), x).squeeze(1)\n","            scores = self.classifier(x_sum, h0)\n","            scores_list.append(scores)\n","            # next turn\n","            if self.rnn is not None:\n","                h0 = self.dropout(h0)\n","                if type(self.rnn) is nn.LSTMCell:\n","                    h0, c0 = self.rnn(x_sum, (h0, c0))\n","                else:\n","                    h0 = self.rnn(x_sum, h0)\n","        if self.mem_type == 1:\n","            batch_size = x.size(0) // self.label_size\n","            mask = self._generate_mask(self.alpha.data.new(batch_size, self.num_turn), self.mem_random_drop, is_training)\n","            mask = [m.contiguous() for m in torch.unbind(mask, 1)]\n","            tmp_scores_list = [mask[idx].view(batch_size, 1).expand_as(inp.view(-1, self.label_size))\n","                               * F.softmax(inp.view(-1, self.label_size), 1)\n","                               for idx, inp in enumerate(scores_list)]\n","            scores = torch.stack(tmp_scores_list, 2)\n","            scores = torch.mean(scores, 2)\n","            scores = torch.log(scores)\n","        else:\n","            scores = scores_list[-1]\n","        if self.dump_state:\n","            return scores, scores_list\n","        else:\n","            return scores"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jJZdHqpLB3B6"},"outputs":[],"source":["# MMM-MCQA from https://github.com/jind11/MMM-MCQA\n","\n","#import copy\n","import math\n","import torch\n","import torch.nn as nn\n","from torch.nn import CrossEntropyLoss, NLLLoss\n","from transformers.modeling_utils import prune_linear_layer\n","from transformers.models.bert.modeling_bert import (\n","    BertPreTrainedModel,\n","    BertEmbeddings,\n","    BertSelfOutput,\n","    BertIntermediate,\n","    BertOutput,\n","    BertPooler,\n","    BertModel,\n",")\n","\n","BertLayerNorm = torch.nn.LayerNorm\n","\n","# Add speaker_embeddings\n","# class BertEmbeddings(nn.Module):\n","#     def __init__(self, config, speaker_embeddings=False):\n","#     def forward(self, input_ids, token_type_ids=None, speaker_ids=None):\n","\n","# Config output_attentions, keep_multihead_output for output\n","class BertSelfAttention(nn.Module):\n","    def __init__(self, config, output_attentions=False, keep_multihead_output=False):\n","        super(BertSelfAttention, self).__init__()\n","        if config.hidden_size % config.num_attention_heads != 0:\n","            raise ValueError(\n","                \"The hidden size (%d) is not a multiple of the number of attention \"\n","                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads))\n","        self.output_attentions = output_attentions\n","        self.keep_multihead_output = keep_multihead_output\n","        self.multihead_output = None\n","\n","        self.num_attention_heads = config.num_attention_heads\n","        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n","        self.all_head_size = self.num_attention_heads * self.attention_head_size\n","\n","        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n","        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n","        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n","\n","        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n","\n","    def transpose_for_scores(self, x):\n","        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n","        x = x.view(*new_x_shape)\n","        return x.permute(0, 2, 1, 3)\n","\n","    def forward(self, hidden_states, attention_mask, head_mask=None):\n","        mixed_query_layer = self.query(hidden_states)\n","        mixed_key_layer = self.key(hidden_states)\n","        mixed_value_layer = self.value(hidden_states)\n","\n","        query_layer = self.transpose_for_scores(mixed_query_layer)\n","        key_layer = self.transpose_for_scores(mixed_key_layer)\n","        value_layer = self.transpose_for_scores(mixed_value_layer)\n","\n","        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n","        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n","        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n","        # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n","        attention_scores = attention_scores + attention_mask\n","\n","        # Normalize the attention scores to probabilities.\n","        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n","\n","        # This is actually dropping out entire tokens to attend to, which might\n","        # seem a bit unusual, but is taken from the original Transformer paper.\n","        attention_probs = self.dropout(attention_probs)\n","\n","        # Mask heads if we want to\n","        if head_mask is not None:\n","            attention_probs = attention_probs * head_mask\n","\n","        context_layer = torch.matmul(attention_probs, value_layer)\n","        if self.keep_multihead_output:\n","            self.multihead_output = context_layer\n","            self.multihead_output.retain_grad()\n","\n","        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n","        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n","        context_layer = context_layer.view(*new_context_layer_shape)\n","        if self.output_attentions:\n","            return attention_probs, context_layer\n","        return context_layer\n","\n","# Config output_attentions, keep_multihead_output for BertSelfAttention(), change head pruned, attention output\n","class BertAttention(nn.Module):\n","    def __init__(self, config, output_attentions=False, keep_multihead_output=False):\n","        super(BertAttention, self).__init__()\n","        self.output_attentions = output_attentions\n","        self.self = BertSelfAttention(config, output_attentions=output_attentions, keep_multihead_output=keep_multihead_output)\n","        self.output = BertSelfOutput(config)\n","\n","    def prune_heads(self, heads):\n","        if len(heads) == 0:\n","            return\n","        mask = torch.ones(self.self.num_attention_heads, self.self.attention_head_size)\n","        for head in heads:\n","            mask[head] = 0\n","        mask = mask.view(-1).contiguous().eq(1)\n","        index = torch.arange(len(mask))[mask].long()\n","\n","        # Prune linear layers\n","        self.self.query = prune_linear_layer(self.self.query, index)\n","        self.self.key = prune_linear_layer(self.self.key, index)\n","        self.self.value = prune_linear_layer(self.self.value, index)\n","        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n","\n","        # Update hyper params\n","        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n","        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n","\n","    def forward(self, input_tensor, attention_mask, head_mask=None):\n","        self_output = self.self(input_tensor, attention_mask, head_mask)\n","        if self.output_attentions:\n","            attentions, self_output = self_output\n","        attention_output = self.output(self_output, input_tensor)\n","        if self.output_attentions:\n","            return attentions, attention_output\n","        return attention_output\n","\n","# Config output_attentions, keep_multihead_output for BertAttention(), change attentions output on forward()\n","class BertLayer(nn.Module):\n","    def __init__(self, config, output_attentions=False, keep_multihead_output=False):\n","        super(BertLayer, self).__init__()\n","        self.output_attentions = output_attentions\n","        self.attention = BertAttention(config, output_attentions=output_attentions, keep_multihead_output=keep_multihead_output)\n","        self.intermediate = BertIntermediate(config)\n","        self.output = BertOutput(config)\n","\n","    def forward(self, hidden_states, attention_mask, head_mask=None):\n","        attention_output = self.attention(hidden_states, attention_mask, head_mask)\n","        if self.output_attentions:\n","            attentions, attention_output = attention_output\n","        intermediate_output = self.intermediate(attention_output)\n","        layer_output = self.output(intermediate_output, attention_output)\n","        if self.output_attentions:\n","            return attentions, layer_output\n","        return layer_output\n","\n","# Config output_attentions, keep_multihead_output for BertLayer(), change attentions layer in forward() \n","class BertEncoder(nn.Module):\n","    def __init__(self, config, output_attentions=False, keep_multihead_output=False):\n","        super(BertEncoder, self).__init__()\n","        self.output_attentions = output_attentions\n","        self.layer = nn.ModuleList([\n","            BertLayer(config, output_attentions=output_attentions, keep_multihead_output=keep_multihead_output)\n","            for _ in range(config.num_hidden_layers)\n","        ])\n","\n","    def forward(self, hidden_states, attention_mask, output_all_encoded_layers=True, head_mask=None):\n","        all_encoder_layers = []\n","        all_attentions = []\n","        for i, layer_module in enumerate(self.layer):\n","            hidden_states = layer_module(hidden_states, attention_mask, head_mask[i])\n","            if self.output_attentions:\n","                attentions, hidden_states = hidden_states\n","                all_attentions.append(attentions)\n","            if output_all_encoded_layers:\n","                all_encoder_layers.append(hidden_states)\n","        if not output_all_encoded_layers:\n","            all_encoder_layers.append(hidden_states)\n","        if self.output_attentions:\n","            return all_attentions, all_encoder_layers\n","        return all_encoder_layers\n","\n","# Config output_attentions, keep_multihead_output\n","class BertModel(BertPreTrainedModel):\n","    def __init__(self, config, output_attentions=False, keep_multihead_output=False, speaker_embeddings=False):\n","        super(BertModel, self).__init__(config)\n","\n","        self.output_attentions = output_attentions\n","        self.embeddings = BertEmbeddings(config)\n","        self.encoder = BertEncoder(config, output_attentions=output_attentions, keep_multihead_output=keep_multihead_output)\n","        self.pooler = BertPooler(config)\n","        self.apply(self._init_weights)#init_bert_weights)\n","\n","    def prune_heads(self, heads_to_prune):\n","        \"\"\" Prunes heads of the model.\n","            heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n","        \"\"\"\n","        for layer, heads in heads_to_prune.items():\n","            self.encoder.layer[layer].attention.prune_heads(heads)\n","\n","    def get_multihead_outputs(self):\n","        \"\"\" Gather all multi-head outputs.\n","            Return: list (layers) of multihead module outputs with gradients\n","        \"\"\"\n","        return [layer.attention.self.multihead_output for layer in self.encoder.layer]\n","\n","    def forward(self, input_ids, token_type_ids=None, attention_mask=None, speaker_ids=None, output_all_encoded_layers=True, head_mask=None):\n","        if attention_mask is None:\n","            attention_mask = torch.ones_like(input_ids)\n","        if token_type_ids is None:\n","            token_type_ids = torch.zeros_like(input_ids)\n","\n","        # We create a 3D attention mask from a 2D tensor mask.\n","        # Sizes are [batch_size, 1, 1, to_seq_length]\n","        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n","        # this attention mask is more simple than the triangular masking of causal attention\n","        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n","        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n","\n","        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n","        # masked positions, this operation will create a tensor which is 0.0 for\n","        # positions we want to attend and -10000.0 for masked positions.\n","        # Since we are adding it to the raw scores before the softmax, this is\n","        # effectively the same as removing these entirely.\n","        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n","        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n","\n","        # Prepare head mask if needed\n","        # 1.0 in head_mask indicate we keep the head\n","        # attention_probs has shape bsz x n_heads x N x N\n","        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n","        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n","        if head_mask is not None:\n","            if head_mask.dim() == 1:\n","                head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n","                head_mask = head_mask.expand_as(self.config.num_hidden_layers, -1, -1, -1, -1)\n","            elif head_mask.dim() == 2:\n","                head_mask = head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)  # We can specify head_mask for each layer\n","            head_mask = head_mask.to(dtype=next(self.parameters()).dtype) # switch to fload if need + fp16 compatibility\n","        else:\n","            head_mask = [None] * self.config.num_hidden_layers\n","\n","        embedding_output = self.embeddings(input_ids, token_type_ids, speaker_ids)\n","        encoded_layers = self.encoder(embedding_output,\n","                                      extended_attention_mask,\n","                                      head_mask=head_mask,\n","                                      output_all_encoded_layers=output_all_encoded_layers)\n","        if self.output_attentions:\n","            all_attentions, encoded_layers = encoded_layers\n","        sequence_output = encoded_layers[-1]\n","        pooled_output = self.pooler(sequence_output)\n","\n","        if not output_all_encoded_layers:\n","            encoded_layers = encoded_layers[-1]\n","        if self.output_attentions:\n","            return all_attentions, encoded_layers, pooled_output\n","        return encoded_layers, pooled_output\n","\n","# Config output_attentions, keep_multihead_output, add premise_mask, hyp_mask attention\n","class BertForMultipleChoice_SAN(BertPreTrainedModel):\n","    def __init__(self, config, opt, num_choices, output_attentions=False, keep_multihead_output=False, same_linear_layer=0):\n","        super(BertForMultipleChoice_SAN, self).__init__(config)\n","        self.output_attentions = output_attentions\n","        self.num_choices = num_choices\n","        self.bert = BertModel(config, output_attentions=output_attentions, keep_multihead_output=keep_multihead_output)\n","        self.opt = opt\n","        self.use_SAN = opt.get('use_SAN', 1)\n","        self.same_linear_layer = same_linear_layer\n","        if not self.use_SAN:\n","            self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","            # self.classifier = nn.ModuleList([nn.Linear(config.hidden_size, 1)] * len(num_choices))\n","            if same_linear_layer:\n","                self.classifier = nn.Linear(config.hidden_size, 1)\n","            else:\n","                self.classifier = nn.ModuleList([nn.Linear(config.hidden_size, 1)] * len(num_choices))\n","        else:\n","            if same_linear_layer:\n","                self.out_proj = SANClassifier(config.hidden_size, config.hidden_size, 1, opt, prefix='answer')\n","            else:\n","                self.out_proj = []\n","                for num_choice in num_choices:\n","                    self.out_proj.append(SANClassifier(config.hidden_size, config.hidden_size, num_choice, opt, prefix='answer'))\n","                self.out_proj = nn.ModuleList(self.out_proj)\n","\n","        self.apply(self._init_weights)\n","\n","    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None, head_mask=None, premise_mask=None, hyp_mask=None, is_training=True, task_id=None):\n","        input_ids = input_ids.view(-1, input_ids.size(-1))\n","        token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n","        attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n","\n","        outputs = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False, head_mask=head_mask)\n","        if self.output_attentions:\n","            all_attentions, sequence_output, pooled_output = outputs\n","        else:\n","            sequence_output, pooled_output = outputs\n","        # pooled_output = self.dropout(pooled_output)\n","        # logits = self.classifier(pooled_output)\n","        # reshaped_logits = logits.view(-1, self.num_choices)\n","\n","        # SAN module\n","        if self.use_SAN:\n","            premise_mask = premise_mask.view(-1, premise_mask.size(-1)) if premise_mask is not None else None\n","            hyp_mask = hyp_mask.view(-1, hyp_mask.size(-1)) if hyp_mask is not None else None\n","            max_query = hyp_mask.size(1)\n","            hyp_mem = sequence_output[:, :max_query]\n","            if self.same_linear_layer:\n","                logits = self.out_proj(sequence_output, hyp_mem, premise_mask, hyp_mask, is_training=is_training)\n","            else:\n","                logits = self.out_proj[task_id](sequence_output, hyp_mem, premise_mask, hyp_mask, is_training=is_training)\n","        else:\n","            pooled_output = self.dropout(pooled_output)\n","            if self.same_linear_layer:\n","                logits = self.classifier(pooled_output)\n","            else:\n","                logits = self.classifier[task_id](pooled_output)\n","\n","        if labels is not None:\n","            if self.opt.get('answer_mem_type', 0):\n","                loss_fct = NLLLoss()\n","            else:\n","                loss_fct = CrossEntropyLoss()\n","                logits = logits.view(-1, self.num_choices[task_id])\n","            labels = labels.view(-1)\n","            loss = loss_fct(logits, labels)\n","            return loss, logits\n","        elif self.output_attentions:\n","            return all_attentions, logits\n","        return logits\n","\n","\n","class BertForMultipleChoice_SAN2(BertPreTrainedModel):\n","    def __init__(self, config, opt, num_choices, output_attentions=False, keep_multihead_output=False):\n","        super(BertForMultipleChoice_SAN2, self).__init__(config)\n","        self.output_attentions = output_attentions\n","        self.num_choices = num_choices\n","        self.bert = BertModel(config, output_attentions=output_attentions, keep_multihead_output=keep_multihead_output)\n","        self.opt = opt\n","        self.use_SAN = opt.get('use_SAN', 1)\n","        if not self.use_SAN:\n","            self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","            self.classifier = nn.ModuleList([nn.Linear(config.hidden_size, 1)] * len(num_choices))\n","        else:\n","            self.out_proj = []\n","            for num_choice in num_choices:\n","                self.out_proj.append(SANClassifier2(config.hidden_size, config.hidden_size, num_choice, opt, prefix='answer'))\n","            self.out_proj = nn.ModuleList(self.out_proj)\n","\n","        self.apply(self._init_weights)#init_bert_weights)\n","\n","    def forward(\n","        self,\n","        input_ids,\n","        token_type_ids=None,\n","        attention_mask=None,\n","        labels=None,\n","        head_mask=None,\n","        premise_mask=None,\n","        hyp_mask=None,\n","        is_training=True,\n","        task_id=None\n","    ):\n","        input_ids = input_ids.view(-1, input_ids.size(-1))\n","        token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n","        attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n","\n","        outputs = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False, head_mask=head_mask)\n","        if self.output_attentions:\n","            all_attentions, sequence_output, pooled_output = outputs\n","        else:\n","            sequence_output, pooled_output = outputs\n","        # pooled_output = self.dropout(pooled_output)\n","        # logits = self.classifier(pooled_output)\n","        # reshaped_logits = logits.view(-1, self.num_choices)\n","\n","        # SAN module\n","        if self.use_SAN:\n","            premise_mask = premise_mask.view(-1, premise_mask.size(-1)) if premise_mask is not None else None\n","            hyp_mask = hyp_mask.view(-1, hyp_mask.size(-1)) if hyp_mask is not None else None\n","            max_query = hyp_mask.size(1)\n","            hyp_mem = sequence_output[:, :max_query]\n","            premise_mem, premise_mask = masked_select(sequence_output, premise_mask)\n","            logits = self.out_proj[task_id](premise_mem, hyp_mem, premise_mask, hyp_mask, is_training=is_training)\n","        else:\n","            pooled_output = self.dropout(pooled_output)\n","            logits = self.classifier[task_id](pooled_output)\n","\n","        if labels is not None:\n","            if self.opt.get('answer_mem_type', 0):\n","                loss_fct = NLLLoss()\n","            else:\n","                loss_fct = CrossEntropyLoss()\n","                logits = logits.view(-1, self.num_choices[task_id])\n","            labels = labels.view(-1)\n","            loss = loss_fct(logits, labels)\n","            return loss, logits\n","        elif self.output_attentions:\n","            return all_attentions, logits\n","        return logits"]},{"cell_type":"markdown","metadata":{"id":"uWwt6WujlC1Q"},"source":["# Utils"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HZxTWqhmlCaO"},"outputs":[],"source":["import csv\n","import glob\n","import json\n","import logging\n","import os\n","from typing import List\n","\n","import numpy as np\n","from tqdm.auto import tqdm, trange\n","from transformers import PreTrainedTokenizer\n","\n","logger = logging.getLogger(__name__)\n","\n","\n","class InputExample(object):\n","    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n","\n","    def __init__(self, guid, text_a, text_b=None, label=None, text_c=None):\n","        \"\"\"Constructs a InputExample.\n","        Args:\n","            guid: Unique id for the example.\n","            text_a: string. The untokenized text of the first sequence. For single sequence tasks, only this sequence must be specified.\n","            text_b: (Optional) string. The untokenized text of the second sequence. Only must be specified for sequence pair tasks.\n","            label: (Optional) string. The label of the example. This should be specified for train and dev examples, but not for test examples.\n","        \"\"\"\n","        self.guid = guid\n","        self.text_a = text_a\n","        self.text_b = text_b\n","        self.text_c = text_c\n","        self.label = label\n","\n","\n","class InputFeatures(object):\n","    \"\"\"A single set of features of data.\"\"\"\n","\n","    def __init__(self, guid, input_ids, input_mask, segment_ids, label_id, premise_mask, hypothesis_mask):\n","        self.guid = guid\n","        self.input_ids = input_ids\n","        self.input_mask = input_mask\n","        self.segment_ids = segment_ids\n","        self.label_id = label_id\n","        self.premise_mask = premise_mask\n","        self.hypothesis_mask = hypothesis_mask\n","\n","\n","class DataProcessor(object):\n","    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n","\n","    def get_train_examples(self, data_dir):\n","        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n","        raise NotImplementedError()\n","\n","    def get_dev_examples(self, data_dir):\n","        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n","        raise NotImplementedError()\n","\n","    def get_labels(self):\n","        \"\"\"Gets the list of labels for this data set.\"\"\"\n","        raise NotImplementedError()\n","\n","    @classmethod\n","    def _read_tsv(cls, input_file, quotechar=None):\n","        \"\"\"Reads a tab separated value file.\"\"\"\n","        with open(input_file, \"r\") as f:\n","            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n","            lines = []\n","            for line in reader:\n","                lines.append(line)\n","            return lines\n","\n","    @classmethod\n","    def _read_json(cls, input_file):\n","        \"\"\"Reads a json file.\"\"\"\n","        with open(input_file, \"r\", encoding='utf-8') as fpr:\n","            raw_list = json.load(fpr)\n","            return raw_list\n","\n","    @classmethod\n","    def _read_jsonl(cls, input_file):\n","        \"\"\"Reads a json file.\"\"\"\n","        with open(input_file, \"r\", encoding='utf-8') as fpr:\n","            raw_list = list()\n","            for json_str in list(fpr):\n","                raw_list.append(json.loads(json_str))\n","            return raw_list\n","\n","\n","class VimmrcProcessor(DataProcessor):\n","    \"\"\"Processor for the MultiNLI data set (GLUE version).\"\"\"\n","    def get_train_examples(self, data_dir):\n","        \"\"\"See base class.\"\"\"\n","        return self._read_samples(data_dir, \"train\")\n","\n","    def get_dev_examples(self, data_dir):\n","        \"\"\"See base class.\"\"\"\n","        return self._read_samples(data_dir, \"dev\")\n","        \n","    def get_test_examples(self, data_dir):\n","        \"\"\"See base class.\"\"\"\n","        return self._read_samples(data_dir, \"test\")\n","\n","    def get_labels(self):\n","        \"\"\"See base class.\"\"\"\n","        return [\"A\", \"B\", \"C\", \"D\"]\n","\n","    def _read_samples(self, data_dir, set_type):\n","        \"\"\"Creates examples for the training and dev sets.\"\"\"\n","        # Init reader\n","        examples = []\n","        #example_id = 0\n","        # with open(filename, 'r', encoding='utf-8') as fpr:\n","        #     raw_list = json.load(fpr)\n","        raw_list = self._read_json(os.path.join(data_dir, set_type + \".json\"))\n","\n","        for data_raw in raw_list:\n","            # data_raw = json.load(fpr)\n","            article = data_raw['content']\n","            example_id = 0\n","            title = '_'.join(data_raw['files'].split('/')[-1].split('_')[:-1])\n","            for i in range(len(data_raw['questions'])):\n","                example_id += 1\n","                #truth = str(ord(data_raw['questions'][i]['answer']) - ord('A'))\n","                truth = data_raw['questions'][i]['answer']\n","                question = data_raw['questions'][i]['question']\n","                options = data_raw['questions'][i]['options']\n","                for k in range(len(options)):\n","                    guid = \"%s-%s-%s-%s\" % (set_type, title, example_id, k)\n","                    option = list(options[k].values())[0]\n","                    examples.append(\n","                        InputExample(guid=guid, text_a=article, text_b=option, label=truth, text_c=question))\n","\n","        return examples\n","\n","\n","class VinliProcessor(DataProcessor):\n","    \"\"\"Processor for the MultiNLI data set (GLUE version).\"\"\"\n","    def get_train_examples(self, data_dir):\n","        \"\"\"See base class.\"\"\"\n","        return self._read_samples(data_dir, \"train\")\n","\n","    def get_dev_examples(self, data_dir):\n","        \"\"\"See base class.\"\"\"\n","        return self._read_samples(data_dir, \"dev\")\n","        \n","    def get_test_examples(self, data_dir):\n","        \"\"\"See base class.\"\"\"\n","        return self._read_samples(data_dir, \"test\")\n","\n","    def get_labels(self):\n","        \"\"\"See base class.\"\"\"\n","        return [\"contradiction\", \"entailment\", \"neutral\"]\n","\n","    def _read_samples(self, data_dir, set_type):\n","        \"\"\"Creates examples for the training and dev sets.\"\"\"\n","        # Init reader\n","        examples = []\n","        raw_list = self._read_jsonl(os.path.join(data_dir, set_type + \".jsonl\"))\n","        for data_line in raw_list:\n","            guid = \"%s-%s\" % (set_type, data_line['pairID'])\n","            text_a = data_line['sentence1']\n","            text_b = data_line['sentence2']\n","            label = data_line['gold_label']\n","            if label in self.get_labels():\n","                examples.append(\n","                    InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n","\n","        return examples\n","\n","\n","class RaceProcessor(DataProcessor):\n","    \"\"\"Processor for the RACE data set.\"\"\"\n","\n","    def get_train_examples(self, data_dir):\n","        \"\"\"See base class.\"\"\"\n","        logger.info(\"LOOKING AT {} train\".format(data_dir))\n","        high = os.path.join(data_dir, \"train_race.json\")\n","        # middle = os.path.join(data_dir, \"train/middle\")\n","        high = self._read_txt(high)\n","        # middle = self._read_txt(middle)\n","        return self._create_examples(high, \"train\")\n","\n","    def get_dev_examples(self, data_dir):\n","        \"\"\"See base class.\"\"\"\n","        logger.info(\"LOOKING AT {} dev\".format(data_dir))\n","        high = os.path.join(data_dir, \"dev_race.json\")\n","        # middle = os.path.join(data_dir, \"dev/middle\")\n","        high = self._read_txt(high)\n","        # middle = self._read_txt(middle)\n","        return self._create_examples(high, \"dev\")\n","\n","    def get_test_examples(self, data_dir):\n","        \"\"\"See base class.\"\"\"\n","        logger.info(\"LOOKING AT {} test\".format(data_dir))\n","        high = os.path.join(data_dir, \"test_race.json\")\n","        # middle = os.path.join(data_dir, \"test/middle\")\n","        high = self._read_txt(high)\n","        # middle = self._read_txt(middle)\n","        return self._create_examples(high, \"test\")\n","\n","    def get_labels(self):\n","        \"\"\"See base class.\"\"\"\n","        return [\"A\", \"B\", \"C\", \"D\"]\n","\n","    def _read_txt(self, input_dir):\n","        lines = []\n","        with open(input_dir, \"r\", encoding=\"utf-8\") as fin:\n","            data_raw = json.load(fin)\n","        for d in data_raw:\n","            d['race_id'] = '_'.join(d['files'].split('/')[-1].split('_')[:-1])\n","            lines.append(d)\n","        return lines\n","\n","    def _create_examples(self, lines, set_type):\n","        \"\"\"Creates examples for the training and dev sets.\"\"\"\n","        examples = []\n","        for (_, data_raw) in enumerate(lines):\n","            race_id = \"%s-%s\" % (set_type, data_raw[\"race_id\"])\n","            article = data_raw[\"article\"]\n","            for i in range(len(data_raw[\"answers\"])):\n","                truth = data_raw[\"answers\"][i]\n","                question = data_raw[\"questions\"][i]\n","                options = data_raw[\"options\"][i]\n","                for k in range(len(options)):\n","                    guid = \"%s-%s\" % (race_id, k)\n","                    examples.append(\n","                        InputExample(\n","                            guid=guid,\n","                            text_a=article,\n","                            text_b=options[k], #list(options[k].values())[0]\n","                            label=truth,\n","                            text_c=question\n","                        )\n","                    )\n","        return examples\n","\n","\n","class InfiniteDataLoader:\n","    def __init__(self, data_loader):\n","        self.data_loader = data_loader\n","        self.data_iter = iter(data_loader)\n","\n","    def get_next(self):\n","        try:\n","            data = next(self.data_iter)\n","        except StopIteration:\n","            # StopIteration is thrown if dataset ends\n","            # reinitialize data loader\n","            self.data_iter = iter(self.data_loader)\n","            data = next(self.data_iter)\n","        return data\n","\n","\n","def convert_examples_to_features(\n","    examples: List[InputExample],\n","    label_list: List[str],\n","    max_seq_length: int,\n","    tokenizer,\n","    n_class,\n","    do_lower_case,\n","    output_mode,\n","    set_type,\n","    is_multi_choice=True\n",") -> List[InputFeatures]:\n","    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n","\n","    label_map = {label: i for i, label in enumerate(label_list)}\n","\n","    if is_multi_choice:\n","        features = [[]]\n","    else:\n","        features = []\n","\n","    feature_iterator = tqdm(examples, desc=\"Convert examples to features\")\n","    for (ex_index, example) in enumerate(feature_iterator):\n","        feature_iterator.set_description(\"Convert %d of %d example to features\" % (ex_index, len(examples)))\n","\n","        tokens_a = tokenizer.tokenize(example.text_a.lower() if do_lower_case else example.text_a)  # dialogues\n","\n","        tokens_b = [] # None\n","        tokens_c = [] # None\n","\n","        if example.text_b:\n","            tokens_b = tokenizer.tokenize(example.text_b.lower() if do_lower_case else example.text_b)  # answers\n","\n","        if example.text_c:\n","            tokens_c = tokenizer.tokenize(example.text_c.lower() if do_lower_case else example.text_c)  # questions\n","\n","        if tokens_c:\n","            _truncate_seq_tuple(tokens_a, tokens_b, tokens_c, max_seq_length - 4)\n","            tokens_b = tokens_c + [\"[SEP]\"] + tokens_b\n","        elif tokens_b:\n","            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n","        else:\n","            if len(tokens_a) > max_seq_length - 2:\n","                tokens_a = tokens_a[0:(max_seq_length - 2)]\n","\n","        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n","        segment_ids = (len(tokens_a) + 2) * [0]\n","        premise_mask = (len(tokens_a) + 2) * [1]\n","        hypothesis_mask = (len(tokens_a) + 2) * [0]\n","\n","        if tokens_b:\n","            tokens += tokens_b + [\"[SEP]\"]\n","            segment_ids += [1] * (len(tokens_b) + 1)\n","            premise_mask += [1] * (len(tokens_c) + 1)\n","            premise_mask += [0] * (len(tokens_b) - len(tokens_c))\n","            hypothesis_mask += [0] * (len(tokens_c) + 1)\n","            hypothesis_mask += [1] * (len(tokens_b) - len(tokens_c))\n","\n","        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n","\n","        # The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.\n","        input_mask = [1] * len(input_ids)\n","\n","        # Zero-pad up to the sequence length.\n","        pad_length = max_seq_length - len(input_ids)\n","        input_ids += [0] * pad_length\n","        input_mask += [0] * pad_length\n","        segment_ids += [0] * pad_length\n","        premise_mask += [0] * pad_length\n","        hypothesis_mask += [0] * pad_length\n","\n","        assert len(input_ids) == max_seq_length\n","        assert len(input_mask) == max_seq_length\n","        assert len(segment_ids) == max_seq_length\n","        assert len(premise_mask) == max_seq_length\n","        assert len(hypothesis_mask) == max_seq_length\n","\n","        if output_mode in [\"classification\", \"multi-choice\"]:\n","            label_id = label_map[example.label]\n","        elif output_mode == \"regression\":\n","            label_id = float(example.label)\n","        else:\n","            raise KeyError(output_mode)\n","\n","        if is_multi_choice:\n","            features[-1].append(\n","                InputFeatures(\n","                    guid=example.guid,\n","                    input_ids=input_ids,\n","                    input_mask=input_mask,\n","                    segment_ids=segment_ids,\n","                    label_id=label_id,\n","                    premise_mask = premise_mask,\n","                    hypothesis_mask = hypothesis_mask))\n","            if len(features[-1]) == n_class:\n","                features.append([])\n","        else:\n","            features.append(\n","                InputFeatures(\n","                    guid=example.guid,\n","                    input_ids=input_ids,\n","                    input_mask=input_mask,\n","                    segment_ids=segment_ids,\n","                    label_id=label_id,\n","                    premise_mask = premise_mask,\n","                    hypothesis_mask = hypothesis_mask))\n","            \n","        ## display some example\n","        if set_type == 'train' and ex_index < 4:\n","            logger.info(\"*** Example ***\")\n","            logger.info(f\"guid: {example.guid}\")\n","            logger.info(f\"tokens: {' '.join(tokens)}\")\n","            logger.info(f\"input_ids: {' '.join(map(str, input_ids))}\")\n","            logger.info(f\"input_mask: {' '.join(map(str, input_mask))}\")\n","            logger.info(f\"segment_ids: {' '.join(map(str, segment_ids))}\")\n","            logger.info(f\"premis_mask: {' '.join(map(str, premise_mask))}\")\n","            logger.info(f\"hypoth_mask: {' '.join(map(str, hypothesis_mask))}\")\n","            try:\n","                logger.info(f\"label: {example.label}\")\n","            except:\n","                pass\n","\n","    if is_multi_choice:\n","        if len(features[-1]) == 0:\n","            features = features[:-1]\n","\n","    return features\n","\n","\n","def convert_features_to_tensors(features, output_mode, is_multi_choice=True):\n","\n","    input_ids = []\n","    input_mask = []\n","    segment_ids = []\n","    label_id = []\n","    premise_mask = []\n","    hypothesis_mask = []\n","\n","    if is_multi_choice:\n","        n_class = len(features[0])\n","        for f in features:\n","            input_ids.append([])\n","            input_mask.append([])\n","            segment_ids.append([])\n","            premise_mask.append([])\n","            hypothesis_mask.append([])\n","            for i in range(n_class):\n","                input_ids[-1].append(f[i].input_ids)\n","                input_mask[-1].append(f[i].input_mask)\n","                segment_ids[-1].append(f[i].segment_ids)\n","                premise_mask[-1].append(f[i].premise_mask)\n","                hypothesis_mask[-1].append(f[i].hypothesis_mask)\n","\n","            label_id.append([f[0].label_id])\n","    else:\n","        for f in features:\n","            input_ids.append(f.input_ids)\n","            input_mask.append(f.input_mask)\n","            segment_ids.append(f.segment_ids)\n","            label_id.append(f.label_id)\n","            premise_mask.append(f.premise_mask)\n","            hypothesis_mask.append(f.hypothesis_mask)\n","\n","    all_input_ids = torch.tensor(input_ids, dtype=torch.long)\n","    all_input_mask = torch.tensor(input_mask, dtype=torch.long)\n","    all_segment_ids = torch.tensor(segment_ids, dtype=torch.long)\n","    all_premise_mask = torch.tensor(premise_mask, dtype=torch.bool)\n","    all_hypothesis_mask = torch.tensor(hypothesis_mask, dtype=torch.bool)\n","\n","    if output_mode in [\"classification\", \"multi-choice\"]:\n","        all_label_ids = torch.tensor(label_id, dtype=torch.long)\n","    elif output_mode == \"regression\":\n","        all_label_ids = torch.tensor(label_id, dtype=torch.float)\n","\n","    data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_premise_mask, all_hypothesis_mask, all_label_ids)\n","\n","    return data\n","\n","\n","def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n","    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n","\n","    # This is a simple heuristic which will always truncate the longer sequence\n","    # one token at a time. This makes more sense than truncating an equal percent\n","    # of tokens from each, since if one sequence is very short then each token\n","    # that's truncated likely contains more information than a longer sequence.\n","    while True:\n","        total_length = len(tokens_a) + len(tokens_b)\n","        if total_length <= max_length:\n","            break\n","        if len(tokens_a) > len(tokens_b):\n","            tokens_a.pop()\n","        else:\n","            tokens_b.pop()\n","\n","\n","def _truncate_seq_tuple(tokens_a, tokens_b, tokens_c, max_length):\n","    \"\"\"Truncates a sequence tuple in place to the maximum length.\"\"\"\n","\n","    # This is a simple heuristic which will always truncate the longer sequence\n","    # one token at a time. This makes more sense than truncating an equal percent\n","    # of tokens from each, since if one sequence is very short then each token\n","    # that's truncated likely contains more information than a longer sequence.\n","    while True:\n","        total_length = len(tokens_a) + len(tokens_b) + len(tokens_c)\n","        if total_length <= max_length:\n","            break\n","        if len(tokens_a) >= len(tokens_b) and len(tokens_a) >= len(tokens_c):\n","            tokens_a.pop()\n","        elif len(tokens_b) >= len(tokens_a) and len(tokens_b) >= len(tokens_c):\n","            tokens_b.pop()\n","        else:\n","            tokens_c.pop()\n","\n","\n","processors = {\n","    \"vimmrc\": VimmrcProcessor,\n","    \"vinli\": VinliProcessor,\n","    \"vimmrc_race\": RaceProcessor,\n","}\n","\n","output_modes = {\n","    \"vimmrc\": 'multi-choice',\n","    \"vimmrc_race\": 'multi-choice',\n","    \"vinli\": \"classification\",\n","}\n","\n","GLUE_TASKS_NUM_LABELS = { \"vimmrc\": 4, \"vimmrc_race\": 4, \"vinli\": 3 }\n","\n","MAX_SEQ_LENGTHS = { \"vimmrc\": 512, \"vimmrc_race\": 512, \"vinli\": 128 }"]},{"cell_type":"markdown","metadata":{"id":"yoiGjVs5kuwD"},"source":["# Module"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VG-MZ9JcbuBo"},"outputs":[],"source":["import argparse\n","import glob\n","import json\n","import logging\n","import os\n","import random\n","\n","import numpy as np\n","import torch\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n","from torch.utils.data.distributed import DistributedSampler\n","#from torch.optim import AdamW\n","from tqdm.auto import tqdm, trange\n","from transformers import (\n","    WEIGHTS_NAME, CONFIG_NAME,\n","    AdamW,\n","    BertConfig,\n","    BertForMultipleChoice,\n","    BertTokenizer,\n","    RobertaConfig,\n","    RobertaForMultipleChoice,\n","    RobertaTokenizer,\n","    XLMRobertaConfig,\n","    XLMRobertaForMultipleChoice,\n","    XLMRobertaTokenizer,\n","    XLNetConfig,\n","    XLNetForMultipleChoice,\n","    XLNetTokenizer,\n","    get_linear_schedule_with_warmup,\n",")\n","\n","from transformers import BERT_PRETRAINED_CONFIG_ARCHIVE_MAP, DISTILBERT_PRETRAINED_CONFIG_ARCHIVE_MAP, XLM_ROBERTA_PRETRAINED_CONFIG_ARCHIVE_MAP\n","from transformers import DistilBertConfig, DistilBertForMultipleChoice, DistilBertTokenizer\n","\n","# try:\n","#     from torch.utils.tensorboard import SummaryWriter\n","# except ImportError:\n","#     from tensorboardX import SummaryWriter\n","\n","logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s', datefmt='%m/%d/%Y %H:%M:%S', level=logging.INFO)\n","logger = logging.getLogger(__name__)\n","logger.setLevel(logging.INFO)\n","\n","ALL_MODELS = tuple(DISTILBERT_PRETRAINED_CONFIG_ARCHIVE_MAP.keys()) + tuple(BERT_PRETRAINED_CONFIG_ARCHIVE_MAP.keys()) + tuple(XLM_ROBERTA_PRETRAINED_CONFIG_ARCHIVE_MAP.keys())\n","\n","MODEL_CLASSES = {\n","    \"bert-man\": (BertConfig, BertForMultipleChoice_SAN, BertTokenizer),\n","    #\"roberta\": (RobertaConfig, RobertaForMultipleChoice, RobertaTokenizer),\n","    #\"xlm-roberta\": (XLMRobertaConfig, XLMRobertaForMultipleChoice, XLMRobertaTokenizer)\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Iow6D0wkEez"},"outputs":[],"source":["def select_field(features, field):\n","    return [[choice[field] for choice in feature.choices_features] for feature in features]\n","\n","\n","def simple_accuracy(preds, labels):\n","    return (preds == labels).mean()\n","\n","\n","def set_seed(args):\n","    random.seed(args.seed)\n","    np.random.seed(args.seed)\n","    torch.manual_seed(args.seed)\n","    if args.n_gpu > 0:\n","        torch.cuda.manual_seed_all(args.seed)\n","\n","\n","def load_and_cache_examples(args, task, tokenizer, set_type='train'):\n","    if args.local_rank not in [-1, 0]:\n","        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n","    \n","    output_mode = output_modes[task]\n","    is_multi_choice = True if output_mode == 'multi-choice' else False\n","    processor = processors[task]()\n","    # Load data features from cache or dataset file\n","    cached_features_file = os.path.join(\n","        args.data_dir,\n","        'cached_{}_{}_{}_{}'.format(\n","            set_type,\n","            list(filter(None, args.model_name_or_path.split('/'))).pop(),\n","            str(MAX_SEQ_LENGTHS[task]),\n","            str(task)\n","        )\n","    )\n","    if os.path.exists(cached_features_file):\n","        logger.info(\"Loading features from cached file %s\", cached_features_file)\n","        features = torch.load(cached_features_file)\n","    else:\n","        logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n","        label_list = processor.get_labels()\n","        if set_type == 'train':\n","            examples = processor.get_train_examples(args.data_dir)\n","        elif set_type == 'dev':\n","            examples = processor.get_dev_examples(args.data_dir)\n","        else:\n","            examples = processor.get_test_examples(args.data_dir)\n","        logger.info(\"Training number: %s\", str(len(examples)))\n","        features = convert_examples_to_features(\n","            examples,\n","            label_list,\n","            MAX_SEQ_LENGTHS[task],\n","            tokenizer,\n","            len(label_list),\n","            output_mode=output_mode,\n","            set_type=set_type,\n","            do_lower_case=args.do_lower_case,\n","            is_multi_choice=is_multi_choice\n","        )\n","        if args.local_rank in [-1, 0]:\n","            logger.info(\"Saving features into cached file %s\", cached_features_file)\n","            torch.save(features, cached_features_file)\n","\n","    # Convert to Tensors and build dataset\n","    dataset = convert_features_to_tensors(\n","        features, output_mode, is_multi_choice=is_multi_choice\n","    )\n","\n","    if set_type == 'dev' or set_type == 'test':\n","        all_example_ids = [x.guid for feature_set in features for x in feature_set]#[x.example_id for x in features]\n","        return dataset, all_example_ids\n","    else:\n","        return dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2gdQTXe8k1Cf"},"outputs":[],"source":["def train(args, train_datasets, model, tokenizer):\n","    \"\"\" Train the model \"\"\"\n","    # if args.local_rank in [-1, 0]:\n","    #     tb_writer = SummaryWriter()\n","    #     pass\n","\n","    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n","    train_iters = []\n","    tr_batches = []\n","    #for idx, train_dataset in enumerate(train_datasets):\n","    train_sampler = RandomSampler(train_datasets) if args.local_rank == -1 else DistributedSampler(train_datasets)\n","    train_dataloader = DataLoader(train_datasets, sampler=train_sampler, batch_size=args.train_batch_size)\n","    train_iters.append(InfiniteDataLoader(train_dataloader))\n","    tr_batches.append(len(train_dataloader))\n","\n","    ## set sampling proportion\n","    total_n_tr_batches = sum(tr_batches)\n","    sampling_prob = [float(n_batches) / total_n_tr_batches for n_batches in tr_batches]\n","    t_total = total_n_tr_batches // args.gradient_accumulation_steps * args.num_train_epochs\n","\n","    # Prepare optimizer and schedule (linear warmup and decay)\n","    no_decay = ['bias', 'LayerNorm.weight']\n","    optimizer_grouped_parameters = [\n","        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","         'weight_decay': args.weight_decay},\n","        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n","         'weight_decay': 0.0}\n","    ]\n","\n","    # optimizer = BertAdam(\n","    #     optimizer_grouped_parameters,\n","    #     lr=args.learning_rate,\n","    #     warmup=args.warmup_proportion,\n","    #     max_grad_norm=args.max_grad_norm, t_total=t_total)\n","    \n","    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, correct_bias=False, no_deprecation_warning=True) # To reproduce BertAdam specific behavior set correct_bias=False\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer,\n","        num_warmup_steps=int(args.warmup_proportion * t_total),\n","        num_training_steps=t_total,\n","    ) # PyTorch scheduler\n","\n","    # Train!\n","    logger.info(\"***** Running training *****\")\n","    logger.info(\"  Num examples = %d\", len(train_datasets))\n","    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n","    logger.info(\"  Instantaneous batch size per GPU = %s\", args.per_gpu_train_batch_size)\n","    logger.info(\n","        \" Total train batch size (w. parallel, distributed & accumulation) = %d\",\n","        args.train_batch_size\n","        * args.gradient_accumulation_steps\n","        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n","    )\n","    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n","    logger.info(\"  Total optimization steps = %d\", t_total)\n","\n","    global_step = 0\n","    tr_loss, logging_loss, tmp_loss = 0.0, 0.0, 0.0\n","    best_dev_acc = 0.0\n","    best_steps = 0\n","    nb_tr_examples = 0\n","    model.zero_grad()\n","    train_iterator = tqdm(range(int(args.num_train_epochs)), desc=\"Epoch\") #, disable=args.local_rank not in [-1, 0])\n","    set_seed(args)  # Added here for reproductibility (even between python 2 and 3)\n","    for epoch, _ in enumerate(train_iterator):\n","        epoch_iterator = tqdm(range(total_n_tr_batches), desc=\"Iteration\") #, disable=args.local_rank not in [-1, 0])\n","        for step, _ in enumerate(epoch_iterator):\n","            epoch_iterator.set_description(\n","                \"train loss: {}\".format(tr_loss / nb_tr_examples if nb_tr_examples else tr_loss)\n","            )\n","            model.train()\n","            # select task id\n","            task_id = np.argmax(np.random.multinomial(1, sampling_prob))\n","            batch = train_iters[task_id].get_next()\n","            batch = tuple(t.to(args.device) for t in batch)\n","            inputs = {'input_ids':      batch[0],\n","                      'attention_mask': batch[1],\n","                      'token_type_ids': batch[2],\n","                      'premise_mask':   batch[3],\n","                      'hyp_mask':       batch[4],\n","                      'labels':         batch[5],\n","                      'task_id':        task_id}\n","            outputs = model(**inputs)  # model outputs are always tuple in transformers (see doc)\n","            loss = outputs[0]\n","\n","            if args.n_gpu > 1:\n","                loss = loss.mean() # mean() to average on multi-gpu parallel training\n","            if args.gradient_accumulation_steps > 1:\n","                loss = loss / args.gradient_accumulation_steps\n","\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n","\n","            tr_loss += loss.item()\n","            nb_tr_examples += inputs['input_ids'].size(0)\n","            if (step + 1) % args.gradient_accumulation_steps == 0:\n","                optimizer.step()\n","                scheduler.step()  # Update learning rate schedule\n","                model.zero_grad()\n","                global_step += 1\n","                #tb_writer.add_scalar(\"train_{}\".format(\"loss_\"), tr_loss - tmp_loss, global_step)\n","                tmp_loss = tr_loss\n","\n","        # Save model checkpoint\n","        # if args.do_epoch_checkpoint:\n","        epoch_output_dir = os.path.join(args.output_dir, 'epoch_{}'.format(epoch+1))\n","        os.makedirs(epoch_output_dir, exist_ok=True)\n","        output_model_file = os.path.join(epoch_output_dir, WEIGHTS_NAME)\n","        output_config_file = os.path.join(epoch_output_dir, CONFIG_NAME)\n","        model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n","        torch.save(model_to_save.state_dict(), output_model_file)\n","        model_to_save.config.to_json_file(output_config_file)\n","        tokenizer.save_vocabulary(epoch_output_dir)\n","\n","        # evaluate(args, model, tokenizer, epoch=epoch, is_test=False)\n","        # evaluate(args, model, tokenizer, epoch=epoch, is_test=True)\n","    # if args.local_rank in [-1, 0]:\n","    #     tb_writer.close()\n","\n","    return global_step, tr_loss / global_step, best_steps"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hUI-m6tlk3iR"},"outputs":[],"source":["def evaluate(args, model, tokenizer, checkpoint='', is_test=False, error=False):\n","    # Loop to handle MNLI double evaluation (matched, mis-matched)\n","    eval_task_names = args.task_name\n","    eval_output_dir = args.output_dir\n","\n","    set_type = 'test' if is_test else 'dev'\n","    results = {}\n","    #for task_id, eval_task in enumerate(eval_task_names):\n","    #for eval_task, eval_output_dir in zip(eval_task_names, eval_outputs_dirs):\n","    eval_dataset, all_example_ids = load_and_cache_examples(args, eval_task_names, tokenizer, set_type)\n","    \n","    if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n","        os.makedirs(eval_output_dir)\n","\n","    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n","    # Note that DistributedSampler samples randomly\n","    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n","    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n","\n","    # Eval!\n","    logger.info(\"***** Running evaluation for {} on {} for {} *****\".format(eval_task_names, set_type, checkpoint))\n","    logger.info(\"  Num examples = %d\", len(eval_dataset))\n","    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n","    eval_loss = 0.0\n","    nb_eval_steps = 0\n","    logits_all = None\n","    out_label_ids = None\n","    preds_softmax = None\n","    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n","        model.eval()\n","        batch = tuple(t.to(args.device) for t in batch)\n","\n","        with torch.no_grad():\n","            inputs = {'input_ids':      batch[0],\n","                      'attention_mask': batch[1],\n","                      'token_type_ids': batch[2],\n","                      'premise_mask':   batch[3],\n","                      'hyp_mask':       batch[4],\n","                      'labels':         batch[5],\n","                      'task_id':        0}\n","            outputs = model(**inputs)\n","            tmp_eval_loss, logits = outputs[:2]\n","            # input_ids, input_mask, segment_ids, label_ids = batch\n","            # tmp_eval_loss, logits = model(input_ids, segment_ids, input_mask, label_ids, task_id=task_id)\n","            eval_loss += tmp_eval_loss.mean().item()\n","        nb_eval_steps += 1\n","        if logits_all is None:\n","            logits_all = logits.detach().cpu().numpy()\n","            preds_softmax = torch.softmax(logits, -1).detach().cpu().numpy()\n","            out_label_ids = inputs['labels'].detach().cpu().numpy()\n","        else:\n","            logits_all = np.append(logits_all, logits.detach().cpu().numpy(), axis=0)\n","            preds_softmax = np.append(preds_softmax, torch.softmax(logits, -1).detach().cpu().numpy(), axis=0)\n","            out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n","\n","    eval_loss = eval_loss / nb_eval_steps\n","    preds_softmax = [[round(x.tolist(), 4) for x in p] for p in preds_softmax]\n","    example_ids = [id[:-2] for id in all_example_ids[::4]]\n","    per_id_pred = dict(zip(example_ids, preds_softmax))\n","    output_mode = output_modes[eval_task_names]\n","\n","    preds = np.argmax(logits_all, axis=1)\n","    acc = simple_accuracy(preds, out_label_ids.reshape(-1))\n","    #result = compute_metrics(eval_task, preds, out_label_ids.reshape(-1))\n","    result = {\"eval_acc\": acc, \"eval_loss\": eval_loss}\n","    results.update(result)\n","\n","    output_eval_file = os.path.join(eval_output_dir, \"eval_results_{}_{}.txt\".format(eval_task_names, set_type))\n","    with open(output_eval_file, \"a\") as writer:\n","        logger.info(\"***** Eval results for {} on {} for {} *****\".format(eval_task_names, set_type, checkpoint))\n","        writer.write(\"***** Eval results for {} *****\\n\".format(checkpoint))\n","        writer.write(\n","            \"total batch size=%d\\n\"\n","            % (\n","                args.per_gpu_train_batch_size\n","                * args.gradient_accumulation_steps\n","                * (torch.distributed.get_world_size() if args.local_rank != -1 else 1)\n","            )\n","        )\n","        writer.write(\"train num epochs=%d\\n\" % args.num_train_epochs)\n","        writer.write(\"max seq length  =%d\\n\" % MAX_SEQ_LENGTHS[eval_task_names])#args.max_seq_length)\n","        for key in sorted(result.keys()):\n","            logger.info(\"  %s = %s\", key, str(result[key]))\n","            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n","        logger.info(\"\\n\")\n","    \n","    # Write pred\n","    output_pred_file = os.path.join(eval_output_dir, \"predictions_\" + str(is_test).lower() + \"_eval_results.json\")\n","    with open(output_pred_file, \"w\") as writer:\n","        json.dump(per_id_pred, writer, indent=4)\n","        logger.info(\"Saved {0}\".format(output_pred_file))\n","\n","    # Write pred with label\n","    processor = processors[args.task_name]()\n","    label_list = processor.get_labels()#['A','B','C','D']\n","    idx2label = {i: label for i, label in enumerate(label_list)}\n","    output_pred_file = os.path.join(eval_output_dir, \"predictions_\" + str(is_test).lower() + \"_eval_results_label.json\")\n","    with open(output_pred_file, \"w\") as writer:\n","        json.dump({'pred': [idx2label[id[0]] for id in out_label_ids.tolist()]}, writer, indent=4)\n","        logger.info(\"Saved {0}\".format(output_pred_file))\n","\n","    # Get error idx\n","    if error:\n","        correct_idx = np.argwhere(preds == out_label_ids).tolist()\n","        wrong_idx = np.argwhere(preds != out_label_ids).tolist()\n","        wrong_idx_dict = {\n","            'correct': correct_idx, 'wrong': wrong_idx,\n","            'preds': preds.tolist(), 'logits': logits_all.tolist(),\n","            'labels': out_label_ids.tolist()\n","        }\n","        output_error_file = os.path.join(eval_output_dir,\"error_idx_{}_{}.json\".format(eval_task_names, set_type))\n","        with open(output_error_file, \"w\") as writer:\n","            json.dump(wrong_idx_dict, writer)\n","            logger.info(\"Saved {0}\".format(output_error_file))\n","\n","    return results"]},{"cell_type":"markdown","metadata":{"id":"fyV2VX7Vk1RA"},"source":["# Main"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"daHi3mJlTi69"},"outputs":[],"source":["PRJ_DIR = '/content/drive/MyDrive/NCKH/ViMMRC_model'\n","OUTPUT_DIR = PRJ_DIR + '/models/vibert_man_v1'"]},{"cell_type":"markdown","metadata":{"id":"42dvg6DqZPg2"},"source":["## Train "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1676547421858,"user":{"displayName":"Tường Phạm Quang","userId":"05102412219596796118"},"user_tz":-420},"id":"Tfky1cL1wKzR","outputId":"f4751ee3-628c-4919-8461-9f3b909d9098"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'data_dir': '/content/drive/MyDrive/NCKH/ViMMRC_model/dataset/ViMMRC_RACE_v1',\n"," 'model_type': 'bert-man',\n"," 'model_name_or_path': 'FPTAI/vibert-base-cased',\n"," 'task_name': 'vimmrc_race',\n"," 'output_predictions': True,\n"," 'output_dir': '/content/drive/MyDrive/NCKH/ViMMRC_model/models/vibert_man_v1',\n"," 'freeze_embeddings': False,\n"," 'freeze_layers': None,\n"," 'tb_log_dir': '',\n"," 'config_name': '',\n"," 'tokenizer_name': '',\n"," 'cache_dir': '/content/drive/MyDrive/NCKH/ViMMRC_model/models/cached',\n"," 'max_seq_length': 512,\n"," 'do_train': True,\n"," 'do_eval': True,\n"," 'do_test': False,\n"," 'evaluate_during_training': True,\n"," 'do_lower_case': False,\n"," 'per_gpu_train_batch_size': 4,\n"," 'per_gpu_eval_batch_size': 4,\n"," 'gradient_accumulation_steps': 8,\n"," 'learning_rate': 3e-05,\n"," 'weight_decay': 0.01,\n"," 'max_grad_norm': 1.0,\n"," 'num_train_epochs': 7,\n"," 'max_steps': -1,\n"," 'warmup_proportion': 0.1,\n"," 'eval_all_checkpoints': True,\n"," 'no_cuda': False,\n"," 'seed': 42,\n"," 'local_rank': -1,\n"," 'f': '/root/.local/share/jupyter/runtime/kernel-457a381d-8bf8-408a-b28d-a195ec3a8599.json'}"]},"metadata":{},"execution_count":15}],"source":["parser = argparse.ArgumentParser()\n","\n","if True:\n","    # Required parameters\n","    parser.add_argument(\n","        \"--data_dir\", default=\"{}/dataset/ViMMRC_RACE_v1\".format(PRJ_DIR), type=str,\n","        help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\",\n","    )\n","\n","    parser.add_argument(\n","        \"--model_type\", default=\"bert-man\", type=str,\n","        help=\"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys()),\n","    )\n","    parser.add_argument(\n","        \"--model_name_or_path\", default=\"FPTAI/vibert-base-cased\", type=str,\n","        help=\"Path to pre-trained model or shortcut name selected in the list: \" + \", \".join(ALL_MODELS),\n","    )\n","    \n","    parser.add_argument(\n","        \"--task_name\", default=\"vimmrc_race\", type=str,\n","        help=\"The name of the task to train selected in the list: \" + \", \".join(processors.keys()),\n","    )\n","    # parser.add_argument(\n","    #     \"--para_type\", default=\"per_choice\", type=str,\n","    #     choices=[\"per_choice\", \"concat_choices\", \"ignore\"],\n","    #     help=\"Paragraph building strategy for ARC (default: %(default)s)\",\n","    # )\n","    parser.add_argument(\n","        \"--output_predictions\", default=True, type=bool, help=\"Whether to export the predictions from the eval step.\",\n","    )\n","    parser.add_argument(\n","        \"--output_dir\", default=OUTPUT_DIR, type=str, help=\"The output directory where the model predictions and checkpoints will be written.\",\n","    )\n","    parser.add_argument(\"--freeze_embeddings\", default=False, action=\"store_true\", help=\"Whether to freeze the embeeding layer.\",)\n","    parser.add_argument(\"--freeze_layers\", nargs=\"*\", help=\"Whether to freeze the embeeding layer.\",)\n","\n","    # Other parameters\n","    parser.add_argument(\"--tb_log_dir\", default=\"\", type=str, help=\"Tensorboard log dir for the current experiment\")\n","    parser.add_argument(\"--config_name\", default=\"\", type=str, help=\"Pretrained config name or path if not the same as model_name\")\n","    parser.add_argument(\"--tokenizer_name\", default=\"\", type=str, help=\"Pretrained tokenizer name or path if not the same as model_name\")\n","    parser.add_argument(\n","        \"--cache_dir\", default=\"{}/models/cached\".format(PRJ_DIR), type=str, help=\"Where do you want to store the pre-trained models downloaded from s3\",\n","    )\n","    parser.add_argument(\n","        \"--max_seq_length\", default=MAX_SEQ_LENGTHS['vimmrc'], type=int,\n","        help=\"The maximum total input sequence length after tokenization. Sequences longer than this will be truncated, sequences shorter will be padded.\",\n","    )\n","    parser.add_argument(\"--do_train\", default = True, action=\"store_true\", help=\"Whether to run training.\")\n","    parser.add_argument(\"--do_eval\", default = True, action=\"store_true\", help=\"Whether to run eval on the dev set.\")\n","    parser.add_argument(\"--do_test\", default = False, action=\"store_true\", help=\"Whether to run test on the test set\")\n","    parser.add_argument(\n","        \"--evaluate_during_training\", action=\"store_true\", default = True, help=\"Run evaluation during training at each logging step.\",\n","    )\n","    parser.add_argument(\n","        \"--do_lower_case\", action=\"store_true\", default = False, help=\"Set this flag if you are using an uncased model.\",\n","    )\n","\n","    parser.add_argument(\"--per_gpu_train_batch_size\", default=4, type=int, help=\"Batch size per GPU/CPU for training.\",)\n","    parser.add_argument(\"--per_gpu_eval_batch_size\", default=4, type=int, help=\"Batch size per GPU/CPU for evaluation.\",)\n","    parser.add_argument(\n","        \"--gradient_accumulation_steps\", type=int, default=8, help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n","    )\n","    parser.add_argument(\"--learning_rate\", default=3e-5, type=float, help=\"The initial learning rate for Adam.\")\n","    parser.add_argument(\"--weight_decay\", default=0.01, type=float, help=\"Weight decay if we apply some.\")\n","    # parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n","    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n","    parser.add_argument(\"--num_train_epochs\", default=7, type=float, help=\"Total number of training epochs to perform.\")\n","    parser.add_argument(\"--max_steps\", default=-1, type=int, help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\")\n","    parser.add_argument(\"--warmup_proportion\", default=0.1, type=float, help=\"Linear warmup over warmup_proportion.\")\n","\n","    # parser.add_argument(\"--logging_steps\", type=int, default=100, help=\"Log every X updates steps.\")\n","    # parser.add_argument(\"--save_steps\", type=int, default=500, help=\"Save checkpoint every X updates steps.\")\n","    parser.add_argument(\n","        \"--eval_all_checkpoints\", default=True, action=\"store_true\",\n","        help=\"Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number\",\n","    )\n","    parser.add_argument(\"--no_cuda\", default=False, action=\"store_true\", help=\"Avoid using CUDA when available\")\n","    # parser.add_argument(\n","    #     \"--overwrite_output_dir\", default = True, action=\"store_true\", help=\"Overwrite the content of the output directory\",\n","    # )\n","    # parser.add_argument(\n","    #     \"--overwrite_cache\", default = True, action=\"store_true\", help=\"Overwrite the cached training and evaluation sets\",\n","    # )\n","    parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization\")\n","\n","    # parser.add_argument(\n","    #     \"--fp16\", action=\"store_true\", help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n","    # )\n","    # parser.add_argument(\n","    #     \"--fp16_opt_level\", type=str, default=\"O1\",\n","    #     help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n","    #     \"See details at https://nvidia.github.io/apex/amp.html\",\n","    # )\n","    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n","    # parser.add_argument(\"--server_ip\", type=str, default=\"\", help=\"For distant debugging.\")\n","    # parser.add_argument(\"--server_port\", type=str, default=\"\", help=\"For distant debugging.\")\n","\n","    parser.add_argument('-f')\n","\n","args = parser.parse_args()\n","vars(args)\n","# options_print = \"\"\n","# logger.info(\"Arg Options - input:\")\n","# for arg in vars(args):\n","#     options_print += \"opt: %s=%s\\r\\n\" % (arg, getattr(args, arg))\n","# logger.info(options_print)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"THNDibdjb9we","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["928c6bed64b340a5860f6e01657fb6c1","007a11145f40479093f6aa8cdd6b1851","836f00a120534f548c114b48d06a4eb3","d2bc2857616641d2ade00540eaa5b0e1","546eda90aa844707a7bcc6b41a380ddb","3d9193c3dbe5429f89c2af656ddf5e4d","7e91f3aba07f44ec8b5770e71c2b00ed","f19bde9dc95141c1bd4b73ed212756fc","cd767fcdd62848ce8dc64f8e73870b1f","f4c9558df23a40ba8d6284e82ec73249","5fc1f362d66d42559ef6ba81a7dab5f5","62907a645291493d8cbe480f2dcb32e3","be0a33254dda4d9285f14c612aa9f0a6","eac38d099b5342d9a8876d85725d3cf9","71d9a275ed47436694f86d2a5147ff2b","20edbcafa3de427583b0232c145edffa","eeeb905dde0d4345aff9ab7bc07e1a2f","2ac8b624191d47868faecdc8ffa1166d","e329d3755ee645ffb7ac7164a5ba3751","f2631092a2ac4949a98b9e3c1f788d1a","87918595c081481398473a997809151c","efaa206d742241b0ba514e70280be23e","a277c15e27c646288b4ab882f0d57bae","6a991099c6484b9485ecd543753a6e83","150950dcc46a41aca5fe32df408e1753","a9714b9875954566a8de5c12f9c4928c","5f4f92c7f4974013a35cc1e8efbae5ae","d4aa0b0834e147fea65249c2f0e9b785","1882267b2f95476eba259b3b8a64a9af","0031650704ec4a3c9c68aa2f4bb5086e","23d5eb2c12c64c09af28ca466552b729","03a56fd0a9644037a159882476cc9e57","d33ec020d1d9464299fb95dcf58b4fd4","c51fafff07d2461593401bf619327ee6","c769f761d17f4ed5bc4a98237955c755","02e4558441d24fd6bca85f60d7a9d9cb","0fdafebf231e4cf7844abf7d444f1655","b37f29d8fb134240ae5bb79a0ba337ae","265870b7457b42a2966bc29a0097d278","307806b5bc7e44628bf49b1ae4b7128e","e3171a456fc54816af95ddb9e2f2fdec","a35c0a9d3cd14b109ff33b0db712bf57","01c2b40a15b24e4f8390314049c26597","b178d345668f41a690565754a96ef100","37a4129add184019bc7bd05bd6481899","13bb8926dc7b444dbfdbbf57ede796bd","ef217433eab8461cb1f456fd98ba6fb6","d312c85f7c254c2787e9cdf240177096","e2e6700637a34e23999838ef50855d45","c826ca35316b412babbe0653ae137b8b","0deff87e801f4efbb2e7e1f7fa5c8f3f","3d44d453a62a466b871bbc07ef8f51a3","4c6af9579e9b46e29579bbe2b5c7617b","ed33f5df9e1e49b0ac3ebf9a90508b56","c50bb81791e746b7aeb05a0302d68274","9244c880f8ae497c81fdc3f7d8621917","ab989e6cd7054f33a621efc92787e0a9","9dfd94f9297646f895e23330a875465a","078f405eea4e410aaa3e3c843ca476b9","7ff56841c8994b16a2bcb54fefc7e115","c4d655d80e0a42f695a6d74e2c15c8f6","a92108b967ae471eb34d920f794fe8da","671b2e8387954448a709e675f224b358","2ad33ca86b9543e9906188761cabe85a","9791fa6a1f154e208eec8418aca807a6","d774ea25395b4f18b71a355b737023ce","d829f4b7c78644e1bb28a16d1a778ba0","73cad12ea36843f688e0da90affde667","2e93f4b3338e4cf9a262d179cf6b413c","36a5afc63d3b442892d0e5eed02210f0","047dc11c5c444a909ff8e1ac77a916a1","2de6a76e48694559ad315e0fe4ddf970","b2dbc2e9440a4467a6aa8fac214017e6","525198096724496c8c1a116fda90e217","a0fca666e54f4fa3b6c91416a00db864","704909a4f42544e0b174af42462953b9","1ca5f6a62d72466a86b30a24b462d532","5652374ffd014e0b84d87c4196ad6003","054ec172113343f59b8b3b2cd9fb705e","4db6794dd30c4e3ab645bd6a9defe893","4305029fae65462d9cd8391e19768b83","3ffbf7999a154362963f669a04f8d62a","75bf4674e28c4fb683338fe2e40fa499","053772db58e34b50b1a0d356bd053512","46a9a18fa9f04be68fd3918fc3380150","4046f86cc600413e8902160e50185173","d7c97d0909bd47f6b09c2f45d8017624","83b8780647d2430fac902370f52a392e","3ce6f531869b4b0b83020422bf8b30df","56eaef3c977a47bbb8f82a185bf158b1","0f3ad0e2df3c4e5ab857413daef6d782","e789dee2bfb0411682249d017a0ebdf7","af908efaa08d4e9abca6f6b4175f98f2","114acad3aaff4e418010def1cbfe79a5","78d62869092a45b9be26047a5058847d","05b55d494dd8459db06a3fd1a1dc5e25","8ddac07875f64b25a1fa6aa438fde71e","275e8e184ac5495c93cb3f29c359c2b4","c384ea25ecf64e1d89bb1ffcf906ee3d","d35e9fb967384f02a8b812489b9dfde5","2f3a36fc2ac04113bf0228f72837e9b4","d755e507a49a4757806efcf85e65d2c4","6b19ddfb56eb43ee9c11ffdb716ff581","44ec7db64ff4436990e4d5e5046af780","e38a4d1499004be78f384ce7bea94b9d","fc41f0a31f564afd9909f19ec64b1184","bc1bf6994fd044ae809fde1ac1fd1366","6565089486a047e3842b3a4c7f9400f0","c37a9b2c80d9404b855e17014a133298","b5c69af39b73469c83fd5dfbbeef6c72","5a9c73ae98d8448b907c1c92e5d6ebd7","008a530497eb49dab86b395eb05a82aa","b77231d9853340acb746932fb2322fc2","cb3c96302e8345998e65531e5b1ded76","a5e1e6ce02df43a59f4037bfa0a3fed0","6b6ccea8d8b54ae48b05ad4bc85d10df","799f1c51ef594ba8afbafda1d22aa29e","7af6599d46444c9186bb4f00dad01bb9","9dd247b168ff476cb476698fa5052f5b","d3a4addce7b04b5c9b33a0f5ad183d36","a62c452bef684161822f74d93b1c5f4d","d5662382754e4f46842e516711b75b59","9db212b6350842f2904f1ded596d9dc5","551a3cffc3f54ad5a7f762e934e2f2a1","85f5ac315fd4438bbcefc0f950c8049f","ce21a3b843a848b7bd6eb9774ca3f327","504cd46176fa43879d1a4c3bb2451d9c","16f8e089a685450fb641f03cdd6b31df","fef79cabd3834100b9a505c06f96ce5e","f93810b9f38b42c2b36589495258a8e4","cd28f0de33a8477d8d7f8df627d05488","8f00ca3dbeba41fab7621ec16b806be9","163075b89b5f4da0a576a50fbb191dba","af1c1e7bf4cc4656beb88d21e91aea89","02e95011ce2845978adb55bc170508cf","2f542a401ce04220b5e3cf92c2b732db","89e3cf8bccd740d5bd32937b9c06245c","31525768d6364efb8c02d6d4698ea84d","8582393449c14d299c423df014326e4c","58aa2e07cff242368f79a6091c4d4f79","603f7e91cd354aa0920af7147b82f289","1a811efc37f24f378dae559636a8e6ba","e752fa080a764463bc2b309d9cb79a55","10e602466abf4eba8ae9375e09ddd0f7","8092db948e32476d84bf7f9e842c561b","20fd96620208415c9a2f8ad3e94fc533","9a53d8b3cf484e839615879088149280","e7c39b797de74d9fafe77b7ad8942160","51985228ac1543f88a608da514bfc491","dca5fc7fe3624452974d739011c435fc","6391826948fc43b7be6fbbf531f510c0","03c4f8e985b146a4a25156d11c6717df","f65c88dee8f64ad5a6295aab351d9c00","ee837e63acda4d02912d52a29ac6f51c","cb4708117b0942f5a5269927c9eee111","01d2b57f317148f69d8124b1759adfd0","9114948f67794ad6ba907d8b3d8d7365","4e1b139835d943bda4729f4e8eaf7322","0a2cf414c115424796f0d7bf7feecabf","8b0dddc29af74559920a6e5811db2bd9","8fb068c6fbc5463e80770bc44170ef0d","ce81538d1e7d404bb78136ec280fbdd1","e1f2990683bb41519a786f51e8aeb16d","4db4b23282cd4421b99c51f629b2b372","ca547bb00543414cbf0cf245f0dd9e15","69d8bdfabbe8437993262c88fd209d77","bab73ea3c940474498f4c34cc511e82d","b3135eae16dd4d71bb1bbbc8c28e44b5","a847a4b5533248d5b9988b7929511d96","7708483a947c4ba9ac8380933101bea5","e9122d06155c4ee397a383e5b7fa610e","99076b4ce68e42969523e89405f46e20","69c8250ed18f40daaa6705aeb4092edd","7ad8616f03584f83a99dee1ac2d17932","80867138d8e742d4b86e01dab4e6ea20","5235c597d5bc4f5082bdbd3a60ec29ed"]},"executionInfo":{"status":"ok","timestamp":1676553530052,"user_tz":-420,"elapsed":6108197,"user":{"displayName":"Tường Phạm Quang","userId":"05102412219596796118"}},"outputId":"16051fa6-11e2-4dc2-b2e7-50e3f6b14a8e"},"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:__main__:device: cuda, n_gpu: 1, distributed training False\n","Some weights of the model checkpoint at FPTAI/vibert-base-cased were not used when initializing BertForMultipleChoice_SAN: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing BertForMultipleChoice_SAN from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMultipleChoice_SAN from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForMultipleChoice_SAN were not initialized from the model checkpoint at FPTAI/vibert-base-cased and are newly initialized: ['out_proj.0.classifier.proj.weight', 'out_proj.0.rnn.weight_hh', 'out_proj.0.rnn.bias_ih', 'out_proj.0.query_wsum.att.linear.bias', 'out_proj.0.rnn.bias_hh', 'out_proj.0.rnn.weight_ih', 'out_proj.0.alpha', 'out_proj.0.attn.score_func.linear.weight', 'out_proj.0.attn.score_func.linear.bias', 'out_proj.0.query_wsum.att.linear.weight', 'out_proj.0.classifier.proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","INFO:__main__:Training/evaluation parameters Namespace(cache_dir='/content/drive/MyDrive/NCKH/ViMMRC_model/models/cached', config_name='', data_dir='/content/drive/MyDrive/NCKH/ViMMRC_model/dataset/ViMMRC_RACE_v1', device=device(type='cuda'), do_eval=True, do_lower_case=False, do_test=False, do_train=True, eval_all_checkpoints=True, evaluate_during_training=True, f='/root/.local/share/jupyter/runtime/kernel-457a381d-8bf8-408a-b28d-a195ec3a8599.json', freeze_embeddings=False, freeze_layers=None, gradient_accumulation_steps=8, learning_rate=3e-05, local_rank=-1, max_grad_norm=1.0, max_seq_length=512, max_steps=-1, model_name_or_path='FPTAI/vibert-base-cased', model_type='bert-man', n_gpu=1, no_cuda=False, num_train_epochs=7, output_dir='/content/drive/MyDrive/NCKH/ViMMRC_model/models/vibert_man_v1', output_predictions=True, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=4, seed=42, task_name='vimmrc_race', tb_log_dir='', tokenizer_name='', warmup_proportion=0.1, weight_decay=0.01)\n","INFO:__main__:Loading features from cached file /content/drive/MyDrive/NCKH/ViMMRC_model/dataset/ViMMRC_RACE_v1/cached_train_vibert-base-cased_512_vimmrc_race\n","INFO:__main__:***** Running training *****\n","INFO:__main__:  Num examples = 1975\n","INFO:__main__:  Num Epochs = 7\n","INFO:__main__:  Instantaneous batch size per GPU = 4\n","INFO:__main__: Total train batch size (w. parallel, distributed & accumulation) = 32\n","INFO:__main__:  Gradient Accumulation steps = 8\n","INFO:__main__:  Total optimization steps = 427\n"]},{"output_type":"display_data","data":{"text/plain":["Epoch:   0%|          | 0/7 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"928c6bed64b340a5860f6e01657fb6c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Iteration:   0%|          | 0/494 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62907a645291493d8cbe480f2dcb32e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Iteration:   0%|          | 0/494 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a277c15e27c646288b4ab882f0d57bae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Iteration:   0%|          | 0/494 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c51fafff07d2461593401bf619327ee6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Iteration:   0%|          | 0/494 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37a4129add184019bc7bd05bd6481899"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Iteration:   0%|          | 0/494 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9244c880f8ae497c81fdc3f7d8621917"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Iteration:   0%|          | 0/494 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d829f4b7c78644e1bb28a16d1a778ba0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Iteration:   0%|          | 0/494 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5652374ffd014e0b84d87c4196ad6003"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:__main__: global_step = 427, average loss = 0.7266074896376141\n","INFO:__main__:Saving model checkpoint to /content/drive/MyDrive/NCKH/ViMMRC_model/models/vibert_man_v1\n","INFO:__main__:Evaluate the following checkpoints: ['/content/drive/MyDrive/NCKH/ViMMRC_model/models/vibert_man_v1/epoch_1', '/content/drive/MyDrive/NCKH/ViMMRC_model/models/vibert_man_v1/epoch_2', '/content/drive/MyDrive/NCKH/ViMMRC_model/models/vibert_man_v1/epoch_3', '/content/drive/MyDrive/NCKH/ViMMRC_model/models/vibert_man_v1/epoch_4', '/content/drive/MyDrive/NCKH/ViMMRC_model/models/vibert_man_v1/epoch_5', '/content/drive/MyDrive/NCKH/ViMMRC_model/models/vibert_man_v1/epoch_6', '/content/drive/MyDrive/NCKH/ViMMRC_model/models/vibert_man_v1/epoch_7', '/content/drive/MyDrive/NCKH/ViMMRC_model/models/vibert_man_v1']\n","INFO:__main__:Loading features from cached file /content/drive/MyDrive/NCKH/ViMMRC_model/dataset/ViMMRC_RACE_v1/cached_dev_vibert-base-cased_512_vimmrc_race\n","INFO:__main__:***** Running evaluation for vimmrc_race on dev for epoch_1 *****\n","INFO:__main__:  Num examples = 294\n","INFO:__main__:  Batch size = 4\n"]},{"output_type":"display_data","data":{"text/plain":["Evaluating:   0%|          | 0/74 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ce6f531869b4b0b83020422bf8b30df"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:__main__:***** Eval results for vimmrc_race on dev for epoch_1 *****\n","INFO:__main__:  eval_acc = 0.6666666666666666\n","INFO:__main__:  eval_loss = 0.936712435572534\n","INFO:__main__:\n","\n","INFO:__main__:Saved /content/drive/MyDrive/NCKH/ViMMRC_model/models/vibert_man_v1/predictions_false_eval_results.json\n","INFO:__main__:Saved /content/drive/MyDrive/NCKH/ViMMRC_model/models/vibert_man_v1/predictions_false_eval_results_label.json\n","INFO:__main__:Loading features from cached file /content/drive/MyDrive/NCKH/ViMMRC_model/dataset/ViMMRC_RACE_v1/cached_dev_vibert-base-cased_512_vimmrc_race\n","INFO:__main__:***** Running evaluation for vimmrc_race on dev for epoch_2 *****\n","INFO:__main__:  Num examples = 294\n","INFO:__main__:  Batch size = 4\n"]},{"output_type":"display_data","data":{"text/plain":["Evaluating:   0%|          | 0/74 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d35e9fb967384f02a8b812489b9dfde5"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:__main__:***** Eval results for vimmrc_race on dev for epoch_2 *****\n","INFO:__main__:  eval_acc = 0.6530612244897959\n","INFO:__main__:  eval_loss = 1.0967752060576066\n","INFO:__main__:\n","\n","INFO:__main__:Saved /content/drive/MyDrive/NCKH/ViMMRC_model/models/vibert_man_v1/predictions_false_eval_results.json\n","INFO:__main__:Saved /content/drive/MyDrive/NCKH/ViMMRC_model/models/vibert_man_v1/predictions_false_eval_results_label.json\n","INFO:__main__:Loading features from cached file /content/drive/MyDrive/NCKH/ViMMRC_model/dataset/ViMMRC_RACE_v1/cached_dev_vibert-base-cased_512_vimmrc_race\n","INFO:__main__:***** Running evaluation for vimmrc_race on dev for epoch_3 *****\n","INFO:__main__:  Num examples = 294\n","INFO:__main__:  Batch size = 4\n"]},{"output_type":"display_data","data":{"text/plain":["Evaluating:   0%|          | 0/74 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a9c73ae98d8448b907c1c92e5d6ebd7"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:__main__:***** Eval results for vimmrc_race on dev for epoch_3 *****\n","INFO:__main__:  eval_acc = 0.7006802721088435\n","INFO:__main__:  eval_loss = 0.9674154010764953\n","INFO:__main__:\n","\n","INFO:__main__:Saved /content/drive/MyDrive/NCKH/ViMMRC_model/models/vibert_man_v1/predictions_false_eval_results.json\n","INFO:__main__:Saved /content/drive/MyDrive/NCKH/ViMMRC_model/models/vibert_man_v1/predictions_false_eval_results_label.json\n","INFO:__main__:Loading features from cached file /content/drive/MyDrive/NCKH/ViMMRC_model/dataset/ViMMRC_RACE_v1/cached_dev_vibert-base-cased_512_vimmrc_race\n","INFO:__main__:***** Running evaluation for vimmrc_race on dev for epoch_4 *****\n","INFO:__main__:  Num examples = 294\n","INFO:__main__:  Batch size = 4\n"]},{"output_type":"display_data","data":{"text/plain":["Evaluating:   0%|          | 0/74 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5662382754e4f46842e516711b75b59"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:__main__:***** Eval results for vimmrc_race on dev for epoch_4 *****\n","INFO:__main__:  eval_acc = 0.7244897959183674\n","INFO:__main__:  eval_loss = 0.9410749488333995\n","INFO:__main__:\n","\n","INFO:__main__:Saved /content/drive/MyDrive/NCKH/ViMMRC_model/models/vibert_man_v1/predictions_false_eval_results.json\n","INFO:__main__:Saved /content/drive/MyDrive/NCKH/ViMMRC_model/models/vibert_man_v1/predictions_false_eval_results_label.json\n","INFO:__main__:Loading features from cached file /content/drive/MyDrive/NCKH/ViMMRC_model/dataset/ViMMRC_RACE_v1/cached_dev_vibert-base-cased_512_vimmrc_race\n","INFO:__main__:***** Running evaluation for vimmrc_race on dev for epoch_5 *****\n","INFO:__main__:  Num examples = 294\n","INFO:__main__:  Batch size = 4\n"]},{"output_type":"display_data","data":{"text/plain":["Evaluating:   0%|          | 0/74 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"163075b89b5f4da0a576a50fbb191dba"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:__main__:***** Eval results for vimmrc_race on dev for epoch_5 *****\n","INFO:__main__:  eval_acc = 0.717687074829932\n","INFO:__main__:  eval_loss = 0.9388289306929486\n","INFO:__main__:\n","\n","INFO:__main__:Saved /content/drive/MyDrive/NCKH/ViMMRC_model/models/vibert_man_v1/predictions_false_eval_results.json\n","INFO:__main__:Saved /content/drive/MyDrive/NCKH/ViMMRC_model/models/vibert_man_v1/predictions_false_eval_results_label.json\n","INFO:__main__:Loading features from cached file /content/drive/MyDrive/NCKH/ViMMRC_model/dataset/ViMMRC_RACE_v1/cached_dev_vibert-base-cased_512_vimmrc_race\n","INFO:__main__:***** Running evaluation for vimmrc_race on dev for epoch_6 *****\n","INFO:__main__:  Num examples = 294\n","INFO:__main__:  Batch size = 4\n"]},{"output_type":"display_data","data":{"text/plain":["Evaluating:   0%|          | 0/74 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10e602466abf4eba8ae9375e09ddd0f7"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:__main__:***** Eval results for vimmrc_race on dev for epoch_6 *****\n","INFO:__main__:  eval_acc = 0.7414965986394558\n","INFO:__main__:  eval_loss = 0.9229352388568726\n","INFO:__main__:\n","\n","INFO:__main__:Saved /content/drive/MyDrive/NCKH/ViMMRC_model/models/vibert_man_v1/predictions_false_eval_results.json\n","INFO:__main__:Saved /content/drive/MyDrive/NCKH/ViMMRC_model/models/vibert_man_v1/predictions_false_eval_results_label.json\n","INFO:__main__:Loading features from cached file /content/drive/MyDrive/NCKH/ViMMRC_model/dataset/ViMMRC_RACE_v1/cached_dev_vibert-base-cased_512_vimmrc_race\n","INFO:__main__:***** Running evaluation for vimmrc_race on dev for epoch_7 *****\n","INFO:__main__:  Num examples = 294\n","INFO:__main__:  Batch size = 4\n"]},{"output_type":"display_data","data":{"text/plain":["Evaluating:   0%|          | 0/74 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb4708117b0942f5a5269927c9eee111"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:__main__:***** Eval results for vimmrc_race on dev for epoch_7 *****\n","INFO:__main__:  eval_acc = 0.7380952380952381\n","INFO:__main__:  eval_loss = 0.9816745944396669\n","INFO:__main__:\n","\n","INFO:__main__:Saved /content/drive/MyDrive/NCKH/ViMMRC_model/models/vibert_man_v1/predictions_false_eval_results.json\n","INFO:__main__:Saved /content/drive/MyDrive/NCKH/ViMMRC_model/models/vibert_man_v1/predictions_false_eval_results_label.json\n","INFO:__main__:Loading features from cached file /content/drive/MyDrive/NCKH/ViMMRC_model/dataset/ViMMRC_RACE_v1/cached_dev_vibert-base-cased_512_vimmrc_race\n","INFO:__main__:***** Running evaluation for vimmrc_race on dev for vibert_man_v1 *****\n","INFO:__main__:  Num examples = 294\n","INFO:__main__:  Batch size = 4\n"]},{"output_type":"display_data","data":{"text/plain":["Evaluating:   0%|          | 0/74 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69d8bdfabbe8437993262c88fd209d77"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:__main__:***** Eval results for vimmrc_race on dev for vibert_man_v1 *****\n","INFO:__main__:  eval_acc = 0.7380952380952381\n","INFO:__main__:  eval_loss = 0.9816745944396669\n","INFO:__main__:\n","\n","INFO:__main__:Saved /content/drive/MyDrive/NCKH/ViMMRC_model/models/vibert_man_v1/predictions_false_eval_results.json\n","INFO:__main__:Saved /content/drive/MyDrive/NCKH/ViMMRC_model/models/vibert_man_v1/predictions_false_eval_results_label.json\n"]}],"source":["def main(args):\n","\n","    # Setup distant debugging if needed\n","    # if args.server_ip and args.server_port:\n","    #     # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n","    #     import ptvsd\n","\n","    #     print(\"Waiting for debugger attach\")\n","    #     ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n","    #     ptvsd.wait_for_attach()\n","\n","    # Setup CUDA, GPU & distributed training\n","    if args.local_rank == -1 or args.no_cuda:\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n","        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n","    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n","        torch.cuda.set_device(args.local_rank)\n","        device = torch.device(\"cuda\", args.local_rank)\n","        torch.distributed.init_process_group(backend=\"nccl\")\n","        args.n_gpu = 1\n","    logger.info(\"device: %s, n_gpu: %d, distributed training %r\", device, args.n_gpu, bool(args.local_rank != -1))\n","    args.device = device\n","\n","    if args.gradient_accumulation_steps < 1:\n","        raise ValueError(\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\".format(\n","            args.gradient_accumulation_steps))\n","\n","    if os.path.exists(args.output_dir) and os.listdir(args.output_dir):\n","        if args.do_train:\n","            print(\"Output directory ({}) already exists and is not empty.\".format(args.output_dir))\n","    else:\n","        os.makedirs(args.output_dir, exist_ok=True)\n","    \n","    # Set seed\n","    set_seed(args)\n","\n","    # Prepare GLUE task\n","    args.task_name = args.task_name.lower()\n","    if args.task_name not in processors:\n","        raise ValueError(\"Task not found: %s\" % (args.task_name))\n","    processor = processors[args.task_name]()\n","    label_list = processor.get_labels()\n","    num_labels = len(label_list)\n","\n","    # Load pretrained model and tokenizer\n","    if args.local_rank not in [-1, 0]:\n","        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n","\n","    args.model_type = args.model_type.lower()\n","    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n","    config = config_class.from_pretrained(\n","        args.config_name if args.config_name else args.model_name_or_path,\n","        num_labels=num_labels,\n","        early_stopping = True,\n","        finetuning_task=args.task_name,\n","        cache_dir=args.cache_dir if args.cache_dir else None,\n","    )\n","    tokenizer = tokenizer_class.from_pretrained(\n","        args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,\n","        do_lower_case=args.do_lower_case,\n","        cache_dir=args.cache_dir if args.cache_dir else None,\n","    )\n","    model = model_class.from_pretrained(\n","        args.model_name_or_path,\n","        from_tf=bool(\".ckpt\" in args.model_name_or_path),\n","        config=config,\n","        cache_dir=args.cache_dir if args.cache_dir else None,\n","        opt={\"use_SAN\": 1},\n","        num_choices=[4]\n","    )\n","\n","    if args.local_rank == 0:\n","        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n","\n","    options_print = \"\"\n","    logging.info(\"Arg Options:\")\n","    for arg in vars(args):\n","        options_print += \"opt: %s=%s\\r\\n\" % (arg, getattr(args, arg))\n","    logging.info(options_print)\n","\n","    model.to(args.device)\n","\n","    def get_model_base_obj(model, model_type):\n","        if model_type == \"bert\":\n","            return model.bert\n","        elif model_type == \"xlm-roberta\" or model_type == \"roberta\":\n","            return model.roberta\n","        elif model_type == \"distilbert\":\n","            return model.distilbert    \n","        else:\n","            raise ValueError(\"model_type='{0}' is not supported!\")\n","\n","    if args.freeze_embeddings:\n","        for param in list(get_model_base_obj(model, args.model_type).embeddings.parameters()):\n","            param.requires_grad = False\n","        logger.info(\"Froze Embedding Layer\")\n","\n","    # freeze_layers is a string \"1,2,3\" representing layer number\n","    if args.freeze_layers:\n","        layer_indexes = [int(x) for x in args.freeze_layers]\n","        for layer_idx in layer_indexes:\n","            for param in list(\n","                get_model_base_obj(model, args.model_type).encoder.layer[layer_idx].parameters()\n","            ):\n","                param.requires_grad = False\n","            logger.info(\"Froze Layer: %s\", layer_idx)\n","\n","    logger.info(\"Training/evaluation parameters %s\", args)\n","    best_steps = 0\n","\n","    # Training\n","    if args.do_train:\n","        train_dataset = load_and_cache_examples(args, args.task_name, tokenizer, set_type='train')\n","        global_step, tr_loss, best_steps = train(args, train_dataset, model, tokenizer)\n","        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n","\n","    # Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n","    if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n","        # Create output directory if needed\n","        if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n","            os.makedirs(args.output_dir)\n","\n","        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n","        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n","        # They can then be reloaded using `from_pretrained()`\n","        model_to_save = (\n","            model.module if hasattr(model, \"module\") else model\n","        )  # Take care of distributed/parallel training\n","        model_to_save.save_pretrained(args.output_dir)\n","        tokenizer.save_pretrained(args.output_dir)\n","\n","        # Good practice: save your training arguments together with the trained model\n","        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n","\n","        # Load a trained model and vocabulary that you have fine-tuned\n","        model = model_class.from_pretrained(args.output_dir, opt={\"use_SAN\": 1}, num_choices=[4])\n","        tokenizer = tokenizer_class.from_pretrained(args.output_dir)\n","        model.to(args.device)\n","\n","    # Evaluation\n","    results = {}\n","    if args.do_eval and args.local_rank in [-1, 0]:\n","        if not args.do_train:\n","            args.output_dir = args.model_name_or_path\n","        checkpoints = [args.output_dir]\n","        if args.eval_all_checkpoints:\n","            checkpoints = list(\n","                os.path.dirname(c)\n","                for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n","            )\n","            logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n","        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n","        for checkpoint in checkpoints:\n","            global_step = checkpoint.split(\"/\")[-1] if len(checkpoints) > 1 else \"\"\n","            model = model_class.from_pretrained(checkpoint, opt={\"use_SAN\": 1}, num_choices=[4])\n","            model.to(args.device)\n","            result = evaluate(args, model, tokenizer, global_step)\n","            result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n","            results.update(result)\n","\n","    if args.do_test and args.local_rank in [-1, 0]:\n","        checkpoints = [args.output_dir]\n","        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n","        for checkpoint in checkpoints:\n","            global_step = checkpoint.split(\"/\")[-1]\n","            model = model_class.from_pretrained(checkpoint, opt={\"use_SAN\": 1}, num_choices=[4])\n","            model.to(args.device)\n","            result = evaluate(args, model, tokenizer, global_step, is_test=True)\n","            result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n","            results.update(result)\n","    # if best_steps:\n","    #     logger.info(\"best steps of eval acc is the following checkpoints: %s\", best_steps)\n","    return results\n","\n","if __name__ == \"__main__\":\n","    main(args)"]},{"cell_type":"markdown","metadata":{"id":"Ix8RCrSpZSoL"},"source":["## Test"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1676553530052,"user":{"displayName":"Tường Phạm Quang","userId":"05102412219596796118"},"user_tz":-420},"id":"z4Vc0QbZ1XY7","outputId":"e54df571-4ac5-4fa1-ee9b-69518b6fa225"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'data_dir': '/content/drive/MyDrive/NCKH/ViMMRC_model/dataset/ViMMRC_RACE_v1',\n"," 'model_type': 'bert-man',\n"," 'model_name_or_path': 'FPTAI/vibert-base-cased',\n"," 'task_name': 'vimmrc_race',\n"," 'output_predictions': True,\n"," 'output_dir': '/content/drive/MyDrive/NCKH/ViMMRC_model/models/vibert_man_v1',\n"," 'freeze_embeddings': False,\n"," 'freeze_layers': None,\n"," 'tb_log_dir': '',\n"," 'config_name': '',\n"," 'tokenizer_name': '',\n"," 'cache_dir': '/content/drive/MyDrive/NCKH/ViMMRC_model/models/cached',\n"," 'max_seq_length': 512,\n"," 'do_train': False,\n"," 'do_eval': False,\n"," 'do_test': True,\n"," 'evaluate_during_training': True,\n"," 'do_lower_case': False,\n"," 'per_gpu_train_batch_size': 8,\n"," 'per_gpu_eval_batch_size': 4,\n"," 'gradient_accumulation_steps': 4,\n"," 'learning_rate': 3e-05,\n"," 'weight_decay': 0.01,\n"," 'max_grad_norm': 1.0,\n"," 'num_train_epochs': 10,\n"," 'max_steps': -1,\n"," 'warmup_proportion': 0.1,\n"," 'eval_all_checkpoints': True,\n"," 'no_cuda': False,\n"," 'seed': 42,\n"," 'local_rank': -1,\n"," 'f': '/root/.local/share/jupyter/runtime/kernel-457a381d-8bf8-408a-b28d-a195ec3a8599.json'}"]},"metadata":{},"execution_count":17}],"source":["parser = argparse.ArgumentParser()\n","\n","if True:\n","    # Required parameters\n","    parser.add_argument(\n","        \"--data_dir\", default=\"{}/dataset/ViMMRC_RACE_v1\".format(PRJ_DIR), type=str,\n","        help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\",\n","    )\n","\n","    parser.add_argument(\n","        \"--model_type\", default=\"bert-man\", type=str,\n","        help=\"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys()),\n","    )\n","    parser.add_argument(\n","        \"--model_name_or_path\", default=\"FPTAI/vibert-base-cased\", type=str,\n","        help=\"Path to pre-trained model or shortcut name selected in the list: \" + \", \".join(ALL_MODELS),\n","    )\n","    \n","    parser.add_argument(\n","        \"--task_name\", default=\"vimmrc_race\", type=str,\n","        help=\"The name of the task to train selected in the list: \" + \", \".join(processors.keys()),\n","    )\n","    # parser.add_argument(\n","    #     \"--para_type\", default=\"per_choice\", type=str,\n","    #     choices=[\"per_choice\", \"concat_choices\", \"ignore\"],\n","    #     help=\"Paragraph building strategy for ARC (default: %(default)s)\",\n","    # )\n","    parser.add_argument(\n","        \"--output_predictions\", default=True, type=bool, help=\"Whether to export the predictions from the eval step.\",\n","    )\n","    parser.add_argument(\n","        \"--output_dir\", default=OUTPUT_DIR, type=str, help=\"The output directory where the model predictions and checkpoints will be written.\",\n","    )\n","    parser.add_argument(\"--freeze_embeddings\", default=False, action=\"store_true\", help=\"Whether to freeze the embeeding layer.\",)\n","    parser.add_argument(\"--freeze_layers\", nargs=\"*\", help=\"Whether to freeze the embeeding layer.\",)\n","\n","    # Other parameters\n","    parser.add_argument(\"--tb_log_dir\", default=\"\", type=str, help=\"Tensorboard log dir for the current experiment\")\n","    parser.add_argument(\"--config_name\", default=\"\", type=str, help=\"Pretrained config name or path if not the same as model_name\")\n","    parser.add_argument(\"--tokenizer_name\", default=\"\", type=str, help=\"Pretrained tokenizer name or path if not the same as model_name\")\n","    parser.add_argument(\n","        \"--cache_dir\", default=\"{}/models/cached\".format(PRJ_DIR), type=str, help=\"Where do you want to store the pre-trained models downloaded from s3\",\n","    )\n","    parser.add_argument(\n","        \"--max_seq_length\", default=MAX_SEQ_LENGTHS['vimmrc'], type=int,\n","        help=\"The maximum total input sequence length after tokenization. Sequences longer than this will be truncated, sequences shorter will be padded.\",\n","    )\n","    parser.add_argument(\"--do_train\", default = False, action=\"store_true\", help=\"Whether to run training.\")\n","    parser.add_argument(\"--do_eval\", default = False, action=\"store_true\", help=\"Whether to run eval on the dev set.\")\n","    parser.add_argument(\"--do_test\", default = True, action=\"store_true\", help=\"Whether to run test on the test set\")\n","    parser.add_argument(\n","        \"--evaluate_during_training\", action=\"store_true\", default = True, help=\"Run evaluation during training at each logging step.\",\n","    )\n","    parser.add_argument(\n","        \"--do_lower_case\", action=\"store_true\", default = False, help=\"Set this flag if you are using an uncased model.\",\n","    )\n","\n","    parser.add_argument(\"--per_gpu_train_batch_size\", default=8, type=int, help=\"Batch size per GPU/CPU for training.\",)\n","    parser.add_argument(\"--per_gpu_eval_batch_size\", default=4, type=int, help=\"Batch size per GPU/CPU for evaluation.\",)\n","    parser.add_argument(\n","        \"--gradient_accumulation_steps\", type=int, default=4, help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n","    )\n","    parser.add_argument(\"--learning_rate\", default=3e-5, type=float, help=\"The initial learning rate for Adam.\")\n","    parser.add_argument(\"--weight_decay\", default=0.01, type=float, help=\"Weight decay if we apply some.\")\n","    # parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n","    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n","    parser.add_argument(\"--num_train_epochs\", default=10, type=float, help=\"Total number of training epochs to perform.\")\n","    parser.add_argument(\"--max_steps\", default=-1, type=int, help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\")\n","    parser.add_argument(\"--warmup_proportion\", default=0.1, type=float, help=\"Linear warmup over warmup_proportion.\")\n","\n","    # parser.add_argument(\"--logging_steps\", type=int, default=100, help=\"Log every X updates steps.\")\n","    # parser.add_argument(\"--save_steps\", type=int, default=500, help=\"Save checkpoint every X updates steps.\")\n","    parser.add_argument(\n","        \"--eval_all_checkpoints\", default=True, action=\"store_true\",\n","        help=\"Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number\",\n","    )\n","    parser.add_argument(\"--no_cuda\", default=False, action=\"store_true\", help=\"Avoid using CUDA when available\")\n","    # parser.add_argument(\n","    #     \"--overwrite_output_dir\", default = True, action=\"store_true\", help=\"Overwrite the content of the output directory\",\n","    # )\n","    # parser.add_argument(\n","    #     \"--overwrite_cache\", default = True, action=\"store_true\", help=\"Overwrite the cached training and evaluation sets\",\n","    # )\n","    parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization\")\n","\n","    # parser.add_argument(\n","    #     \"--fp16\", action=\"store_true\", help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n","    # )\n","    # parser.add_argument(\n","    #     \"--fp16_opt_level\", type=str, default=\"O1\",\n","    #     help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n","    #     \"See details at https://nvidia.github.io/apex/amp.html\",\n","    # )\n","    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n","    # parser.add_argument(\"--server_ip\", type=str, default=\"\", help=\"For distant debugging.\")\n","    # parser.add_argument(\"--server_port\", type=str, default=\"\", help=\"For distant debugging.\")\n","\n","    parser.add_argument('-f')\n","\n","args = parser.parse_args()\n","vars(args)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":399,"referenced_widgets":["60dd8119f62845cab5af2accf91e04b3","d54e224ff7214025a2a6dd8ec28bdc4e","55ca1ee2492247fab8968cd921b41e7e","688d46b27cc44d90a6d6a546f2251033","54c1c0c5959a4bf2b5cad791c4d35c73","a9085a21da11456395451bca8c717f77","ddd18ad298954a9c82cc45d344596a41","31c3033803664ebfa1f25325bf5f0da8","e1afe82c621c43d88bd5f52c4dd1cbfa","5a43c8fb16ff4bc4aca054e0fbd5f5d1","a2be777520a041068a575a232f837668"]},"executionInfo":{"elapsed":92144,"status":"ok","timestamp":1676553622188,"user":{"displayName":"Tường Phạm Quang","userId":"05102412219596796118"},"user_tz":-420},"id":"Q_0j2qEMx2Ry","outputId":"7e634467-b4b8-4f02-b961-a521498cd5c6"},"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:__main__:device: cuda, n_gpu: 1, distributed training False\n","Some weights of the model checkpoint at FPTAI/vibert-base-cased were not used when initializing BertForMultipleChoice_SAN: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing BertForMultipleChoice_SAN from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMultipleChoice_SAN from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForMultipleChoice_SAN were not initialized from the model checkpoint at FPTAI/vibert-base-cased and are newly initialized: ['out_proj.0.classifier.proj.weight', 'out_proj.0.rnn.weight_hh', 'out_proj.0.rnn.bias_ih', 'out_proj.0.query_wsum.att.linear.bias', 'out_proj.0.rnn.bias_hh', 'out_proj.0.rnn.weight_ih', 'out_proj.0.alpha', 'out_proj.0.attn.score_func.linear.weight', 'out_proj.0.attn.score_func.linear.bias', 'out_proj.0.query_wsum.att.linear.weight', 'out_proj.0.classifier.proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","INFO:__main__:Training/evaluation parameters Namespace(cache_dir='/content/drive/MyDrive/NCKH/ViMMRC_model/models/cached', config_name='', data_dir='/content/drive/MyDrive/NCKH/ViMMRC_model/dataset/ViMMRC_RACE_v1', device=device(type='cuda'), do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_all_checkpoints=True, evaluate_during_training=True, f='/root/.local/share/jupyter/runtime/kernel-457a381d-8bf8-408a-b28d-a195ec3a8599.json', freeze_embeddings=False, freeze_layers=None, gradient_accumulation_steps=4, learning_rate=3e-05, local_rank=-1, max_grad_norm=1.0, max_seq_length=512, max_steps=-1, model_name_or_path='FPTAI/vibert-base-cased', model_type='bert-man', n_gpu=1, no_cuda=False, num_train_epochs=10, output_dir='/content/drive/MyDrive/NCKH/ViMMRC_model/models/vibert_man_v1', output_predictions=True, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=8, seed=42, task_name='vimmrc_race', tb_log_dir='', tokenizer_name='', warmup_proportion=0.1, weight_decay=0.01)\n","INFO:__main__:Evaluate the following checkpoints: ['/content/drive/MyDrive/NCKH/ViMMRC_model/models/vibert_man_v1']\n","INFO:__main__:Loading features from cached file /content/drive/MyDrive/NCKH/ViMMRC_model/dataset/ViMMRC_RACE_v1/cached_test_vibert-base-cased_512_vimmrc_race\n","INFO:__main__:***** Running evaluation for vimmrc_race on test for vibert_man_v1 *****\n","INFO:__main__:  Num examples = 514\n","INFO:__main__:  Batch size = 4\n"]},{"output_type":"display_data","data":{"text/plain":["Evaluating:   0%|          | 0/129 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60dd8119f62845cab5af2accf91e04b3"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:__main__:***** Eval results for vimmrc_race on test for vibert_man_v1 *****\n","INFO:__main__:  eval_acc = 0.6478599221789884\n","INFO:__main__:  eval_loss = 1.4205578421012413\n","INFO:__main__:\n","\n","INFO:__main__:Saved /content/drive/MyDrive/NCKH/ViMMRC_model/models/vibert_man_v1/predictions_true_eval_results.json\n","INFO:__main__:Saved /content/drive/MyDrive/NCKH/ViMMRC_model/models/vibert_man_v1/predictions_true_eval_results_label.json\n"]}],"source":["if __name__ == \"__main__\":\n","    main(args)"]},{"cell_type":"markdown","source":["# DEBUG"],"metadata":{"id":"AKVR39ssObJ2"}},{"cell_type":"code","source":["#%debug\n","task = 'vimmrc_race'\n","set_type = 'dev'\n","processor = processors[task]()\n","label_list = processor.get_labels()\n","output_mode = output_modes[task]\n","is_multi_choice = True if output_mode == 'multi-choice' else False\n","\n","from transformers import BertTokenizer\n","\n","tokenizer = BertTokenizer.from_pretrained(args.model_name_or_path)\n","\n","examples = processor.get_test_examples(args.data_dir)\n","features = convert_examples_to_features(\n","            examples,\n","            label_list,\n","            MAX_SEQ_LENGTHS[task],\n","            tokenizer,\n","            len(label_list),\n","            output_mode=output_mode,\n","            set_type=set_type,\n","            do_lower_case=args.do_lower_case,\n","            is_multi_choice=is_multi_choice\n","        )\n","# all_example_ids = [x.guid for feature_set in features for x in feature_set]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":130,"referenced_widgets":["d18c5adb97614fd6b7d905dcca64a3aa","258d29683b354569bc17c90fdaee63f2","0e0e81f2693b45e598f685f8e09c1670","61f61bd84ca144ceb179db65a7ac4857","d49dde65004e4645900186dc7eabcd79","f04ec0bbf58542d281d6f64078ae7b91","eec6fca336d34120a7e3f2bd27109a4e","00712b6fe8ee440b934b505f1b7744ae","92f9119565d847d0a8a8e81f4b7724a4","39413c3ea3674e73b6f11dd37786975f","e49d3ceda2724c94a725adc1c5191521","d43e6592abb74c2894fda00f51ad0009","5fc3c7c4d7c94a2180da06db08d898c9","5bc5941cb161465aa6f0b49fca6b642e","fe46048d43954432a5f98b46c6ed5ddb","ad879f6e4d914a6494dfbcbd3b410ba9","674a7e87d9e142cb8e2a9b2bc1d6e076","55c401fdf46344da9c6030f3406c8bcb","a2d30ce6eb1248368b53aed074bc3de1","706415f3e3454d4b85b9164fdf8b853c","562c5f93269d4b51b014c67b3d890fff","24646e6ad74049cbbc021fc48617d8a1","cce945b99ccb4bfab1fde96fc0faa0c2","d5f5652e44e64186942f365b42641471","54a5b75d316f461c83b8225bb6630bbc","5c5a8b83209647d08c375456c3215629","6353019d5d124906b160e8ff08eecc47","dbf80868c0a741d498061d9bf2a6f606","ba04362d161c48f5a908826122618d04","27a3010902ba4754a7abbd53f9a07d35","48b1b5fb69084a08a27a8f27795d146d","c30e0eace4a3427085cf6d54325fce0c","a75c93ae6e1240149f66c5809fad582e"]},"id":"Wlx5p3zBOdDY","executionInfo":{"status":"ok","timestamp":1676553648835,"user_tz":-420,"elapsed":26660,"user":{"displayName":"Tường Phạm Quang","userId":"05102412219596796118"}},"outputId":"0ff20ce2-6125-4e8c-b317-d36690f1cace"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/255k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d18c5adb97614fd6b7d905dcca64a3aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d43e6592abb74c2894fda00f51ad0009"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:__main__:LOOKING AT /content/drive/MyDrive/NCKH/ViMMRC_model/dataset/ViMMRC_RACE_v1 test\n"]},{"output_type":"display_data","data":{"text/plain":["Convert examples to features:   0%|          | 0/2056 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cce945b99ccb4bfab1fde96fc0faa0c2"}},"metadata":{}}]},{"cell_type":"code","source":["for i in range(4):\n","    display(vars(examples[i]))"],"metadata":{"id":"p8ihT03-V0ta","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1676553648836,"user_tz":-420,"elapsed":23,"user":{"displayName":"Tường Phạm Quang","userId":"05102412219596796118"}},"outputId":"182d7480-e827-4332-d968-a69b280559d8"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["{'guid': 'test-grade_5_5-0',\n"," 'text_a': 'Thảo quả trên rừng Đản Khao đã vào mùa.\\n\\nGió tây lướt thướt bay qua rừng, quyến hương thảo quả đi, rải theo triền núi, đưa hương thảo quả ngọt lựng, thơm nồng vào những thôn xóm Chin San. Gió thơm. Cây cỏ thơm. Đất trời thơm. Người đi từ rừng thảo quả về, hương thơm đậm ủ ấp trong từng nếp áp, nếp khăn.\\n\\nThảo quả trên rừng Đản Khao đã chín nục. Chẳng có thứ quả nào hương thơm lại ngây ngất kì lạ đến như thế. Mới đầu xuân năm kia, những hạt thảo quả gieo trên đất rừng , qua một năm, đã lớn cao tới bụng người. Một năm sau nữa, từ một thân lẻ, thảo quả đâm thêm hai nhánh mới. Sự sinh sôi sao mà mạnh mẽ vậy. Thoáng cái, dưới bóng râm của rừng già, thảo quả lan tỏa nơi tầng rừng thấp, vươn ngọn, xòe lá, lấn chiếm không gian.\\n\\nSự sống cứ tiếp tục trong âm thầm, hoa thảo quả nảy dưới gốc cây kín đáo và lặng lẽ. Ngày qua, trong sương thu ẩm ướt và mưa rây bụi mùa đông, những chùm hoa khép miệng bắt đầu kết trái. Thảo quả chín dần. Dưới đáy rừng, tựa như đột ngột, bỗng rực lên những chùm thảo quả đỏ chon chót, như chứa lửa, chứa nắng. Rừng ngập hương thơm. Rừng sáng như có lửa hắt lên từ dưới đáy rừng.  \\n\\nRừng say ngây và ấm nóng. Thảo quả như những đốm lửa hồng, ngày qua ngày lại tiếp thêm nhiều ngọn mới, nhấp nháy vui mắt.',\n"," 'text_b': 'Hương thơm.',\n"," 'text_c': 'Thảo quả báo hiệu vào mùa bằng dấu hiệu nào?',\n"," 'label': 'A'}"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["{'guid': 'test-grade_5_5-1',\n"," 'text_a': 'Thảo quả trên rừng Đản Khao đã vào mùa.\\n\\nGió tây lướt thướt bay qua rừng, quyến hương thảo quả đi, rải theo triền núi, đưa hương thảo quả ngọt lựng, thơm nồng vào những thôn xóm Chin San. Gió thơm. Cây cỏ thơm. Đất trời thơm. Người đi từ rừng thảo quả về, hương thơm đậm ủ ấp trong từng nếp áp, nếp khăn.\\n\\nThảo quả trên rừng Đản Khao đã chín nục. Chẳng có thứ quả nào hương thơm lại ngây ngất kì lạ đến như thế. Mới đầu xuân năm kia, những hạt thảo quả gieo trên đất rừng , qua một năm, đã lớn cao tới bụng người. Một năm sau nữa, từ một thân lẻ, thảo quả đâm thêm hai nhánh mới. Sự sinh sôi sao mà mạnh mẽ vậy. Thoáng cái, dưới bóng râm của rừng già, thảo quả lan tỏa nơi tầng rừng thấp, vươn ngọn, xòe lá, lấn chiếm không gian.\\n\\nSự sống cứ tiếp tục trong âm thầm, hoa thảo quả nảy dưới gốc cây kín đáo và lặng lẽ. Ngày qua, trong sương thu ẩm ướt và mưa rây bụi mùa đông, những chùm hoa khép miệng bắt đầu kết trái. Thảo quả chín dần. Dưới đáy rừng, tựa như đột ngột, bỗng rực lên những chùm thảo quả đỏ chon chót, như chứa lửa, chứa nắng. Rừng ngập hương thơm. Rừng sáng như có lửa hắt lên từ dưới đáy rừng.  \\n\\nRừng say ngây và ấm nóng. Thảo quả như những đốm lửa hồng, ngày qua ngày lại tiếp thêm nhiều ngọn mới, nhấp nháy vui mắt.',\n"," 'text_b': 'Kích thước.',\n"," 'text_c': 'Thảo quả báo hiệu vào mùa bằng dấu hiệu nào?',\n"," 'label': 'A'}"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["{'guid': 'test-grade_5_5-2',\n"," 'text_a': 'Thảo quả trên rừng Đản Khao đã vào mùa.\\n\\nGió tây lướt thướt bay qua rừng, quyến hương thảo quả đi, rải theo triền núi, đưa hương thảo quả ngọt lựng, thơm nồng vào những thôn xóm Chin San. Gió thơm. Cây cỏ thơm. Đất trời thơm. Người đi từ rừng thảo quả về, hương thơm đậm ủ ấp trong từng nếp áp, nếp khăn.\\n\\nThảo quả trên rừng Đản Khao đã chín nục. Chẳng có thứ quả nào hương thơm lại ngây ngất kì lạ đến như thế. Mới đầu xuân năm kia, những hạt thảo quả gieo trên đất rừng , qua một năm, đã lớn cao tới bụng người. Một năm sau nữa, từ một thân lẻ, thảo quả đâm thêm hai nhánh mới. Sự sinh sôi sao mà mạnh mẽ vậy. Thoáng cái, dưới bóng râm của rừng già, thảo quả lan tỏa nơi tầng rừng thấp, vươn ngọn, xòe lá, lấn chiếm không gian.\\n\\nSự sống cứ tiếp tục trong âm thầm, hoa thảo quả nảy dưới gốc cây kín đáo và lặng lẽ. Ngày qua, trong sương thu ẩm ướt và mưa rây bụi mùa đông, những chùm hoa khép miệng bắt đầu kết trái. Thảo quả chín dần. Dưới đáy rừng, tựa như đột ngột, bỗng rực lên những chùm thảo quả đỏ chon chót, như chứa lửa, chứa nắng. Rừng ngập hương thơm. Rừng sáng như có lửa hắt lên từ dưới đáy rừng.  \\n\\nRừng say ngây và ấm nóng. Thảo quả như những đốm lửa hồng, ngày qua ngày lại tiếp thêm nhiều ngọn mới, nhấp nháy vui mắt.',\n"," 'text_b': 'Màu sắc.',\n"," 'text_c': 'Thảo quả báo hiệu vào mùa bằng dấu hiệu nào?',\n"," 'label': 'A'}"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["{'guid': 'test-grade_5_5-3',\n"," 'text_a': 'Thảo quả trên rừng Đản Khao đã vào mùa.\\n\\nGió tây lướt thướt bay qua rừng, quyến hương thảo quả đi, rải theo triền núi, đưa hương thảo quả ngọt lựng, thơm nồng vào những thôn xóm Chin San. Gió thơm. Cây cỏ thơm. Đất trời thơm. Người đi từ rừng thảo quả về, hương thơm đậm ủ ấp trong từng nếp áp, nếp khăn.\\n\\nThảo quả trên rừng Đản Khao đã chín nục. Chẳng có thứ quả nào hương thơm lại ngây ngất kì lạ đến như thế. Mới đầu xuân năm kia, những hạt thảo quả gieo trên đất rừng , qua một năm, đã lớn cao tới bụng người. Một năm sau nữa, từ một thân lẻ, thảo quả đâm thêm hai nhánh mới. Sự sinh sôi sao mà mạnh mẽ vậy. Thoáng cái, dưới bóng râm của rừng già, thảo quả lan tỏa nơi tầng rừng thấp, vươn ngọn, xòe lá, lấn chiếm không gian.\\n\\nSự sống cứ tiếp tục trong âm thầm, hoa thảo quả nảy dưới gốc cây kín đáo và lặng lẽ. Ngày qua, trong sương thu ẩm ướt và mưa rây bụi mùa đông, những chùm hoa khép miệng bắt đầu kết trái. Thảo quả chín dần. Dưới đáy rừng, tựa như đột ngột, bỗng rực lên những chùm thảo quả đỏ chon chót, như chứa lửa, chứa nắng. Rừng ngập hương thơm. Rừng sáng như có lửa hắt lên từ dưới đáy rừng.  \\n\\nRừng say ngây và ấm nóng. Thảo quả như những đốm lửa hồng, ngày qua ngày lại tiếp thêm nhiều ngọn mới, nhấp nháy vui mắt.',\n"," 'text_b': 'Người mua bán.',\n"," 'text_c': 'Thảo quả báo hiệu vào mùa bằng dấu hiệu nào?',\n"," 'label': 'A'}"]},"metadata":{}}]},{"cell_type":"code","source":["for i in range(4):\n","    for f in vars(features[0][i]):\n","        print(f, getattr(features[0][i], f))\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qjxp3Y4BWsKV","executionInfo":{"status":"ok","timestamp":1676553648837,"user_tz":-420,"elapsed":12,"user":{"displayName":"Tường Phạm Quang","userId":"05102412219596796118"}},"outputId":"088298b9-eb97-4320-e970-f5edfa2b6516"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["guid test-grade_5_5-0\n","input_ids [2, 1327, 110, 2438, 2157, 50, 618, 23, 37, 232, 78, 618, 391, 78, 148, 6, 206, 78, 297, 1072, 2827, 225, 2827, 598, 110, 2157, 50, 5, 627, 1208, 1017, 722, 1327, 110, 84, 5, 7052, 196, 1367, 1208, 2012, 20, 5, 29, 1287, 1017, 722, 1327, 110, 62, 2827, 2661, 5, 54, 2961, 1966, 50, 391, 78, 15, 1705, 54, 708, 112, 2961, 319, 23, 1870, 6, 206, 78, 54, 2961, 6, 461, 49, 382, 54, 2961, 6, 618, 80, 2331, 20, 54, 2961, 6, 62, 1481, 20, 84, 879, 2157, 50, 1327, 110, 2000, 5, 1017, 722, 54, 2961, 618, 33, 606, 4056, 22, 1369, 2583, 146, 4056, 5, 2583, 146, 2764, 6, 1327, 110, 2438, 2157, 50, 618, 23, 37, 232, 78, 618, 319, 23, 2012, 39, 6, 2077, 50, 382, 225, 110, 1144, 78, 1017, 722, 54, 2961, 958, 468, 2234, 80, 439, 523, 889, 15, 628, 1537, 6, 1685, 1026, 112, 4624, 733, 439, 89, 5, 15, 1705, 2986, 1327, 110, 3486, 78, 2438, 618, 80, 2157, 50, 5, 110, 2321, 733, 5, 618, 565, 23, 179, 1643, 2926, 50, 62, 1481, 20, 6, 2321, 733, 97, 1916, 5, 879, 2321, 1382, 2554, 5, 1327, 110, 618, 33, 2684, 197, 582, 1685, 6, 1548, 227, 2639, 178, 812, 1946, 21, 2410, 391, 49, 6, 54, 2085, 50, 1394, 5, 1510, 20, 2475, 44, 33, 672, 89, 2157, 50, 131, 5, 1327, 110, 1521, 2142, 2010, 1511, 50, 2157, 50, 1767, 146, 5, 2271, 708, 62, 708, 5, 112, 3820, 523, 5, 1521, 319, 1488, 37, 1717, 186, 6, 1548, 1454, 672, 2761, 146, 879, 39, 22, 2581, 522, 5, 780, 1327, 110, 182, 1510, 20, 1620, 39, 461, 49, 9445, 618, 78, 391, 2348, 2554, 6, 468, 110, 5, 22, 4773, 65, 225, 2581, 606, 2827, 391, 148, 4599, 759, 20, 148, 937, 65, 5, 15, 1705, 477, 33, 780, 37, 734, 146, 544, 2936, 3497, 1026, 3561, 720, 6, 1327, 110, 319, 23, 1904, 6, 1510, 20, 618, 49, 2157, 50, 5, 2724, 15, 628, 937, 80, 62, 2827, 5, 2475, 1496, 39, 2233, 15, 1705, 477, 33, 1327, 110, 937, 13, 23, 13, 80, 5, 15, 628, 477, 89, 3704, 5, 477, 89, 2317, 6, 2157, 50, 2234, 146, 1017, 722, 54, 2961, 6, 2157, 50, 642, 15, 628, 382, 3704, 2986, 2233, 879, 1510, 20, 618, 49, 2157, 50, 6, 2157, 50, 2183, 468, 391, 2581, 1966, 50, 6, 1327, 110, 15, 628, 15, 1705, 937, 33, 3704, 1943, 50, 5, 468, 110, 468, 958, 2761, 146, 2684, 15, 1670, 130, 62, 708, 1685, 5, 15, 232, 146, 15, 2678, 635, 2311, 6, 3, 1327, 110, 264, 3005, 130, 391, 78, 148, 1787, 1877, 3005, 130, 1144, 78, 444, 3, 1017, 722, 54, 2961, 6, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","input_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","segment_ids [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","label_id 0\n","premise_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","hypothesis_mask [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","\n","guid test-grade_5_5-1\n","input_ids [2, 1327, 110, 2438, 2157, 50, 618, 23, 37, 232, 78, 618, 391, 78, 148, 6, 206, 78, 297, 1072, 2827, 225, 2827, 598, 110, 2157, 50, 5, 627, 1208, 1017, 722, 1327, 110, 84, 5, 7052, 196, 1367, 1208, 2012, 20, 5, 29, 1287, 1017, 722, 1327, 110, 62, 2827, 2661, 5, 54, 2961, 1966, 50, 391, 78, 15, 1705, 54, 708, 112, 2961, 319, 23, 1870, 6, 206, 78, 54, 2961, 6, 461, 49, 382, 54, 2961, 6, 618, 80, 2331, 20, 54, 2961, 6, 62, 1481, 20, 84, 879, 2157, 50, 1327, 110, 2000, 5, 1017, 722, 54, 2961, 618, 33, 606, 4056, 22, 1369, 2583, 146, 4056, 5, 2583, 146, 2764, 6, 1327, 110, 2438, 2157, 50, 618, 23, 37, 232, 78, 618, 319, 23, 2012, 39, 6, 2077, 50, 382, 225, 110, 1144, 78, 1017, 722, 54, 2961, 958, 468, 2234, 80, 439, 523, 889, 15, 628, 1537, 6, 1685, 1026, 112, 4624, 733, 439, 89, 5, 15, 1705, 2986, 1327, 110, 3486, 78, 2438, 618, 80, 2157, 50, 5, 110, 2321, 733, 5, 618, 565, 23, 179, 1643, 2926, 50, 62, 1481, 20, 6, 2321, 733, 97, 1916, 5, 879, 2321, 1382, 2554, 5, 1327, 110, 618, 33, 2684, 197, 582, 1685, 6, 1548, 227, 2639, 178, 812, 1946, 21, 2410, 391, 49, 6, 54, 2085, 50, 1394, 5, 1510, 20, 2475, 44, 33, 672, 89, 2157, 50, 131, 5, 1327, 110, 1521, 2142, 2010, 1511, 50, 2157, 50, 1767, 146, 5, 2271, 708, 62, 708, 5, 112, 3820, 523, 5, 1521, 319, 1488, 37, 1717, 186, 6, 1548, 1454, 672, 2761, 146, 879, 39, 22, 2581, 522, 5, 780, 1327, 110, 182, 1510, 20, 1620, 39, 461, 49, 9445, 618, 78, 391, 2348, 2554, 6, 468, 110, 5, 22, 4773, 65, 225, 2581, 606, 2827, 391, 148, 4599, 759, 20, 148, 937, 65, 5, 15, 1705, 477, 33, 780, 37, 734, 146, 544, 2936, 3497, 1026, 3561, 720, 6, 1327, 110, 319, 23, 1904, 6, 1510, 20, 618, 49, 2157, 50, 5, 2724, 15, 628, 937, 80, 62, 2827, 5, 2475, 1496, 39, 2233, 15, 1705, 477, 33, 1327, 110, 937, 13, 23, 13, 80, 5, 15, 628, 477, 89, 3704, 5, 477, 89, 2317, 6, 2157, 50, 2234, 146, 1017, 722, 54, 2961, 6, 2157, 50, 642, 15, 628, 382, 3704, 2986, 2233, 879, 1510, 20, 618, 49, 2157, 50, 6, 2157, 50, 2183, 468, 391, 2581, 1966, 50, 6, 1327, 110, 15, 628, 15, 1705, 937, 33, 3704, 1943, 50, 5, 468, 110, 468, 958, 2761, 146, 2684, 15, 1670, 130, 62, 708, 1685, 5, 15, 232, 146, 15, 2678, 635, 2311, 6, 3, 1327, 110, 264, 3005, 130, 391, 78, 148, 1787, 1877, 3005, 130, 1144, 78, 444, 3, 439, 338, 225, 2245, 6, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","input_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","segment_ids [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","label_id 0\n","premise_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","hypothesis_mask [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","\n","guid test-grade_5_5-2\n","input_ids [2, 1327, 110, 2438, 2157, 50, 618, 23, 37, 232, 78, 618, 391, 78, 148, 6, 206, 78, 297, 1072, 2827, 225, 2827, 598, 110, 2157, 50, 5, 627, 1208, 1017, 722, 1327, 110, 84, 5, 7052, 196, 1367, 1208, 2012, 20, 5, 29, 1287, 1017, 722, 1327, 110, 62, 2827, 2661, 5, 54, 2961, 1966, 50, 391, 78, 15, 1705, 54, 708, 112, 2961, 319, 23, 1870, 6, 206, 78, 54, 2961, 6, 461, 49, 382, 54, 2961, 6, 618, 80, 2331, 20, 54, 2961, 6, 62, 1481, 20, 84, 879, 2157, 50, 1327, 110, 2000, 5, 1017, 722, 54, 2961, 618, 33, 606, 4056, 22, 1369, 2583, 146, 4056, 5, 2583, 146, 2764, 6, 1327, 110, 2438, 2157, 50, 618, 23, 37, 232, 78, 618, 319, 23, 2012, 39, 6, 2077, 50, 382, 225, 110, 1144, 78, 1017, 722, 54, 2961, 958, 468, 2234, 80, 439, 523, 889, 15, 628, 1537, 6, 1685, 1026, 112, 4624, 733, 439, 89, 5, 15, 1705, 2986, 1327, 110, 3486, 78, 2438, 618, 80, 2157, 50, 5, 110, 2321, 733, 5, 618, 565, 23, 179, 1643, 2926, 50, 62, 1481, 20, 6, 2321, 733, 97, 1916, 5, 879, 2321, 1382, 2554, 5, 1327, 110, 618, 33, 2684, 197, 582, 1685, 6, 1548, 227, 2639, 178, 812, 1946, 21, 2410, 391, 49, 6, 54, 2085, 50, 1394, 5, 1510, 20, 2475, 44, 33, 672, 89, 2157, 50, 131, 5, 1327, 110, 1521, 2142, 2010, 1511, 50, 2157, 50, 1767, 146, 5, 2271, 708, 62, 708, 5, 112, 3820, 523, 5, 1521, 319, 1488, 37, 1717, 186, 6, 1548, 1454, 672, 2761, 146, 879, 39, 22, 2581, 522, 5, 780, 1327, 110, 182, 1510, 20, 1620, 39, 461, 49, 9445, 618, 78, 391, 2348, 2554, 6, 468, 110, 5, 22, 4773, 65, 225, 2581, 606, 2827, 391, 148, 4599, 759, 20, 148, 937, 65, 5, 15, 1705, 477, 33, 780, 37, 734, 146, 544, 2936, 3497, 1026, 3561, 720, 6, 1327, 110, 319, 23, 1904, 6, 1510, 20, 618, 49, 2157, 50, 5, 2724, 15, 628, 937, 80, 62, 2827, 5, 2475, 1496, 39, 2233, 15, 1705, 477, 33, 1327, 110, 937, 13, 23, 13, 80, 5, 15, 628, 477, 89, 3704, 5, 477, 89, 2317, 6, 2157, 50, 2234, 146, 1017, 722, 54, 2961, 6, 2157, 50, 642, 15, 628, 382, 3704, 2986, 2233, 879, 1510, 20, 618, 49, 2157, 50, 6, 2157, 50, 2183, 468, 391, 2581, 1966, 50, 6, 1327, 110, 15, 628, 15, 1705, 937, 33, 3704, 1943, 50, 5, 468, 110, 468, 958, 2761, 146, 2684, 15, 1670, 130, 62, 708, 1685, 5, 15, 232, 146, 15, 2678, 635, 2311, 6, 3, 1327, 110, 264, 3005, 130, 391, 78, 148, 1787, 1877, 3005, 130, 1144, 78, 444, 3, 1160, 2944, 6, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","input_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","segment_ids [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","label_id 0\n","premise_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","hypothesis_mask [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","\n","guid test-grade_5_5-3\n","input_ids [2, 1327, 110, 2438, 2157, 50, 618, 23, 37, 232, 78, 618, 391, 78, 148, 6, 206, 78, 297, 1072, 2827, 225, 2827, 598, 110, 2157, 50, 5, 627, 1208, 1017, 722, 1327, 110, 84, 5, 7052, 196, 1367, 1208, 2012, 20, 5, 29, 1287, 1017, 722, 1327, 110, 62, 2827, 2661, 5, 54, 2961, 1966, 50, 391, 78, 15, 1705, 54, 708, 112, 2961, 319, 23, 1870, 6, 206, 78, 54, 2961, 6, 461, 49, 382, 54, 2961, 6, 618, 80, 2331, 20, 54, 2961, 6, 62, 1481, 20, 84, 879, 2157, 50, 1327, 110, 2000, 5, 1017, 722, 54, 2961, 618, 33, 606, 4056, 22, 1369, 2583, 146, 4056, 5, 2583, 146, 2764, 6, 1327, 110, 2438, 2157, 50, 618, 23, 37, 232, 78, 618, 319, 23, 2012, 39, 6, 2077, 50, 382, 225, 110, 1144, 78, 1017, 722, 54, 2961, 958, 468, 2234, 80, 439, 523, 889, 15, 628, 1537, 6, 1685, 1026, 112, 4624, 733, 439, 89, 5, 15, 1705, 2986, 1327, 110, 3486, 78, 2438, 618, 80, 2157, 50, 5, 110, 2321, 733, 5, 618, 565, 23, 179, 1643, 2926, 50, 62, 1481, 20, 6, 2321, 733, 97, 1916, 5, 879, 2321, 1382, 2554, 5, 1327, 110, 618, 33, 2684, 197, 582, 1685, 6, 1548, 227, 2639, 178, 812, 1946, 21, 2410, 391, 49, 6, 54, 2085, 50, 1394, 5, 1510, 20, 2475, 44, 33, 672, 89, 2157, 50, 131, 5, 1327, 110, 1521, 2142, 2010, 1511, 50, 2157, 50, 1767, 146, 5, 2271, 708, 62, 708, 5, 112, 3820, 523, 5, 1521, 319, 1488, 37, 1717, 186, 6, 1548, 1454, 672, 2761, 146, 879, 39, 22, 2581, 522, 5, 780, 1327, 110, 182, 1510, 20, 1620, 39, 461, 49, 9445, 618, 78, 391, 2348, 2554, 6, 468, 110, 5, 22, 4773, 65, 225, 2581, 606, 2827, 391, 148, 4599, 759, 20, 148, 937, 65, 5, 15, 1705, 477, 33, 780, 37, 734, 146, 544, 2936, 3497, 1026, 3561, 720, 6, 1327, 110, 319, 23, 1904, 6, 1510, 20, 618, 49, 2157, 50, 5, 2724, 15, 628, 937, 80, 62, 2827, 5, 2475, 1496, 39, 2233, 15, 1705, 477, 33, 1327, 110, 937, 13, 23, 13, 80, 5, 15, 628, 477, 89, 3704, 5, 477, 89, 2317, 6, 2157, 50, 2234, 146, 1017, 722, 54, 2961, 6, 2157, 50, 642, 15, 628, 382, 3704, 2986, 2233, 879, 1510, 20, 618, 49, 2157, 50, 6, 2157, 50, 2183, 468, 391, 2581, 1966, 50, 6, 1327, 110, 15, 628, 15, 1705, 937, 33, 3704, 1943, 50, 5, 468, 110, 468, 958, 2761, 146, 2684, 15, 1670, 130, 62, 708, 1685, 5, 15, 232, 146, 15, 2678, 635, 2311, 6, 3, 1327, 110, 264, 3005, 130, 391, 78, 148, 1787, 1877, 3005, 130, 1144, 78, 444, 3, 62, 1481, 20, 148, 356, 6, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","input_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","segment_ids [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","label_id 0\n","premise_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","hypothesis_mask [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["K_Pv9CIYm7BZ","ioQPuCNdks1x","uWwt6WujlC1Q","Ix8RCrSpZSoL","AKVR39ssObJ2"],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.0"},"vscode":{"interpreter":{"hash":"5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"928c6bed64b340a5860f6e01657fb6c1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_007a11145f40479093f6aa8cdd6b1851","IPY_MODEL_836f00a120534f548c114b48d06a4eb3","IPY_MODEL_d2bc2857616641d2ade00540eaa5b0e1"],"layout":"IPY_MODEL_546eda90aa844707a7bcc6b41a380ddb"}},"007a11145f40479093f6aa8cdd6b1851":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d9193c3dbe5429f89c2af656ddf5e4d","placeholder":"​","style":"IPY_MODEL_7e91f3aba07f44ec8b5770e71c2b00ed","value":"Epoch: 100%"}},"836f00a120534f548c114b48d06a4eb3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f19bde9dc95141c1bd4b73ed212756fc","max":7,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cd767fcdd62848ce8dc64f8e73870b1f","value":7}},"d2bc2857616641d2ade00540eaa5b0e1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f4c9558df23a40ba8d6284e82ec73249","placeholder":"​","style":"IPY_MODEL_5fc1f362d66d42559ef6ba81a7dab5f5","value":" 7/7 [1:34:18&lt;00:00, 807.96s/it]"}},"546eda90aa844707a7bcc6b41a380ddb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3d9193c3dbe5429f89c2af656ddf5e4d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e91f3aba07f44ec8b5770e71c2b00ed":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f19bde9dc95141c1bd4b73ed212756fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd767fcdd62848ce8dc64f8e73870b1f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f4c9558df23a40ba8d6284e82ec73249":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5fc1f362d66d42559ef6ba81a7dab5f5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"62907a645291493d8cbe480f2dcb32e3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_be0a33254dda4d9285f14c612aa9f0a6","IPY_MODEL_eac38d099b5342d9a8876d85725d3cf9","IPY_MODEL_71d9a275ed47436694f86d2a5147ff2b"],"layout":"IPY_MODEL_20edbcafa3de427583b0232c145edffa"}},"be0a33254dda4d9285f14c612aa9f0a6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eeeb905dde0d4345aff9ab7bc07e1a2f","placeholder":"​","style":"IPY_MODEL_2ac8b624191d47868faecdc8ffa1166d","value":"train loss: 0.03777128948053651: 100%"}},"eac38d099b5342d9a8876d85725d3cf9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e329d3755ee645ffb7ac7164a5ba3751","max":494,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f2631092a2ac4949a98b9e3c1f788d1a","value":494}},"71d9a275ed47436694f86d2a5147ff2b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_87918595c081481398473a997809151c","placeholder":"​","style":"IPY_MODEL_efaa206d742241b0ba514e70280be23e","value":" 494/494 [13:29&lt;00:00,  1.53s/it]"}},"20edbcafa3de427583b0232c145edffa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eeeb905dde0d4345aff9ab7bc07e1a2f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ac8b624191d47868faecdc8ffa1166d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e329d3755ee645ffb7ac7164a5ba3751":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2631092a2ac4949a98b9e3c1f788d1a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"87918595c081481398473a997809151c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"efaa206d742241b0ba514e70280be23e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a277c15e27c646288b4ab882f0d57bae":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6a991099c6484b9485ecd543753a6e83","IPY_MODEL_150950dcc46a41aca5fe32df408e1753","IPY_MODEL_a9714b9875954566a8de5c12f9c4928c"],"layout":"IPY_MODEL_5f4f92c7f4974013a35cc1e8efbae5ae"}},"6a991099c6484b9485ecd543753a6e83":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d4aa0b0834e147fea65249c2f0e9b785","placeholder":"​","style":"IPY_MODEL_1882267b2f95476eba259b3b8a64a9af","value":"train loss: 0.033108867884018726: 100%"}},"150950dcc46a41aca5fe32df408e1753":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0031650704ec4a3c9c68aa2f4bb5086e","max":494,"min":0,"orientation":"horizontal","style":"IPY_MODEL_23d5eb2c12c64c09af28ca466552b729","value":494}},"a9714b9875954566a8de5c12f9c4928c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_03a56fd0a9644037a159882476cc9e57","placeholder":"​","style":"IPY_MODEL_d33ec020d1d9464299fb95dcf58b4fd4","value":" 494/494 [13:26&lt;00:00,  1.52s/it]"}},"5f4f92c7f4974013a35cc1e8efbae5ae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4aa0b0834e147fea65249c2f0e9b785":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1882267b2f95476eba259b3b8a64a9af":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0031650704ec4a3c9c68aa2f4bb5086e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"23d5eb2c12c64c09af28ca466552b729":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"03a56fd0a9644037a159882476cc9e57":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d33ec020d1d9464299fb95dcf58b4fd4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c51fafff07d2461593401bf619327ee6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c769f761d17f4ed5bc4a98237955c755","IPY_MODEL_02e4558441d24fd6bca85f60d7a9d9cb","IPY_MODEL_0fdafebf231e4cf7844abf7d444f1655"],"layout":"IPY_MODEL_b37f29d8fb134240ae5bb79a0ba337ae"}},"c769f761d17f4ed5bc4a98237955c755":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_265870b7457b42a2966bc29a0097d278","placeholder":"​","style":"IPY_MODEL_307806b5bc7e44628bf49b1ae4b7128e","value":"train loss: 0.030994230328414633: 100%"}},"02e4558441d24fd6bca85f60d7a9d9cb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e3171a456fc54816af95ddb9e2f2fdec","max":494,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a35c0a9d3cd14b109ff33b0db712bf57","value":494}},"0fdafebf231e4cf7844abf7d444f1655":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_01c2b40a15b24e4f8390314049c26597","placeholder":"​","style":"IPY_MODEL_b178d345668f41a690565754a96ef100","value":" 494/494 [13:26&lt;00:00,  1.51s/it]"}},"b37f29d8fb134240ae5bb79a0ba337ae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"265870b7457b42a2966bc29a0097d278":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"307806b5bc7e44628bf49b1ae4b7128e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e3171a456fc54816af95ddb9e2f2fdec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a35c0a9d3cd14b109ff33b0db712bf57":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"01c2b40a15b24e4f8390314049c26597":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b178d345668f41a690565754a96ef100":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"37a4129add184019bc7bd05bd6481899":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_13bb8926dc7b444dbfdbbf57ede796bd","IPY_MODEL_ef217433eab8461cb1f456fd98ba6fb6","IPY_MODEL_d312c85f7c254c2787e9cdf240177096"],"layout":"IPY_MODEL_e2e6700637a34e23999838ef50855d45"}},"13bb8926dc7b444dbfdbbf57ede796bd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c826ca35316b412babbe0653ae137b8b","placeholder":"​","style":"IPY_MODEL_0deff87e801f4efbb2e7e1f7fa5c8f3f","value":"train loss: 0.028548361942167182: 100%"}},"ef217433eab8461cb1f456fd98ba6fb6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d44d453a62a466b871bbc07ef8f51a3","max":494,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4c6af9579e9b46e29579bbe2b5c7617b","value":494}},"d312c85f7c254c2787e9cdf240177096":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed33f5df9e1e49b0ac3ebf9a90508b56","placeholder":"​","style":"IPY_MODEL_c50bb81791e746b7aeb05a0302d68274","value":" 494/494 [13:25&lt;00:00,  1.51s/it]"}},"e2e6700637a34e23999838ef50855d45":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c826ca35316b412babbe0653ae137b8b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0deff87e801f4efbb2e7e1f7fa5c8f3f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3d44d453a62a466b871bbc07ef8f51a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4c6af9579e9b46e29579bbe2b5c7617b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ed33f5df9e1e49b0ac3ebf9a90508b56":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c50bb81791e746b7aeb05a0302d68274":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9244c880f8ae497c81fdc3f7d8621917":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ab989e6cd7054f33a621efc92787e0a9","IPY_MODEL_9dfd94f9297646f895e23330a875465a","IPY_MODEL_078f405eea4e410aaa3e3c843ca476b9"],"layout":"IPY_MODEL_7ff56841c8994b16a2bcb54fefc7e115"}},"ab989e6cd7054f33a621efc92787e0a9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c4d655d80e0a42f695a6d74e2c15c8f6","placeholder":"​","style":"IPY_MODEL_a92108b967ae471eb34d920f794fe8da","value":"train loss: 0.026279802656232996: 100%"}},"9dfd94f9297646f895e23330a875465a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_671b2e8387954448a709e675f224b358","max":494,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2ad33ca86b9543e9906188761cabe85a","value":494}},"078f405eea4e410aaa3e3c843ca476b9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9791fa6a1f154e208eec8418aca807a6","placeholder":"​","style":"IPY_MODEL_d774ea25395b4f18b71a355b737023ce","value":" 494/494 [13:26&lt;00:00,  1.52s/it]"}},"7ff56841c8994b16a2bcb54fefc7e115":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c4d655d80e0a42f695a6d74e2c15c8f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a92108b967ae471eb34d920f794fe8da":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"671b2e8387954448a709e675f224b358":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ad33ca86b9543e9906188761cabe85a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9791fa6a1f154e208eec8418aca807a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d774ea25395b4f18b71a355b737023ce":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d829f4b7c78644e1bb28a16d1a778ba0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_73cad12ea36843f688e0da90affde667","IPY_MODEL_2e93f4b3338e4cf9a262d179cf6b413c","IPY_MODEL_36a5afc63d3b442892d0e5eed02210f0"],"layout":"IPY_MODEL_047dc11c5c444a909ff8e1ac77a916a1"}},"73cad12ea36843f688e0da90affde667":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2de6a76e48694559ad315e0fe4ddf970","placeholder":"​","style":"IPY_MODEL_b2dbc2e9440a4467a6aa8fac214017e6","value":"train loss: 0.02422039806594935: 100%"}},"2e93f4b3338e4cf9a262d179cf6b413c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_525198096724496c8c1a116fda90e217","max":494,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a0fca666e54f4fa3b6c91416a00db864","value":494}},"36a5afc63d3b442892d0e5eed02210f0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_704909a4f42544e0b174af42462953b9","placeholder":"​","style":"IPY_MODEL_1ca5f6a62d72466a86b30a24b462d532","value":" 494/494 [13:26&lt;00:00,  1.52s/it]"}},"047dc11c5c444a909ff8e1ac77a916a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2de6a76e48694559ad315e0fe4ddf970":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2dbc2e9440a4467a6aa8fac214017e6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"525198096724496c8c1a116fda90e217":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0fca666e54f4fa3b6c91416a00db864":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"704909a4f42544e0b174af42462953b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ca5f6a62d72466a86b30a24b462d532":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5652374ffd014e0b84d87c4196ad6003":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_054ec172113343f59b8b3b2cd9fb705e","IPY_MODEL_4db6794dd30c4e3ab645bd6a9defe893","IPY_MODEL_4305029fae65462d9cd8391e19768b83"],"layout":"IPY_MODEL_3ffbf7999a154362963f669a04f8d62a"}},"054ec172113343f59b8b3b2cd9fb705e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_75bf4674e28c4fb683338fe2e40fa499","placeholder":"​","style":"IPY_MODEL_053772db58e34b50b1a0d356bd053512","value":"train loss: 0.022441373311256545: 100%"}},"4db6794dd30c4e3ab645bd6a9defe893":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_46a9a18fa9f04be68fd3918fc3380150","max":494,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4046f86cc600413e8902160e50185173","value":494}},"4305029fae65462d9cd8391e19768b83":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d7c97d0909bd47f6b09c2f45d8017624","placeholder":"​","style":"IPY_MODEL_83b8780647d2430fac902370f52a392e","value":" 494/494 [13:25&lt;00:00,  1.52s/it]"}},"3ffbf7999a154362963f669a04f8d62a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75bf4674e28c4fb683338fe2e40fa499":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"053772db58e34b50b1a0d356bd053512":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"46a9a18fa9f04be68fd3918fc3380150":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4046f86cc600413e8902160e50185173":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d7c97d0909bd47f6b09c2f45d8017624":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"83b8780647d2430fac902370f52a392e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3ce6f531869b4b0b83020422bf8b30df":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_56eaef3c977a47bbb8f82a185bf158b1","IPY_MODEL_0f3ad0e2df3c4e5ab857413daef6d782","IPY_MODEL_e789dee2bfb0411682249d017a0ebdf7"],"layout":"IPY_MODEL_af908efaa08d4e9abca6f6b4175f98f2"}},"56eaef3c977a47bbb8f82a185bf158b1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_114acad3aaff4e418010def1cbfe79a5","placeholder":"​","style":"IPY_MODEL_78d62869092a45b9be26047a5058847d","value":"Evaluating: 100%"}},"0f3ad0e2df3c4e5ab857413daef6d782":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_05b55d494dd8459db06a3fd1a1dc5e25","max":74,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8ddac07875f64b25a1fa6aa438fde71e","value":74}},"e789dee2bfb0411682249d017a0ebdf7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_275e8e184ac5495c93cb3f29c359c2b4","placeholder":"​","style":"IPY_MODEL_c384ea25ecf64e1d89bb1ffcf906ee3d","value":" 74/74 [00:44&lt;00:00,  1.98it/s]"}},"af908efaa08d4e9abca6f6b4175f98f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"114acad3aaff4e418010def1cbfe79a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"78d62869092a45b9be26047a5058847d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"05b55d494dd8459db06a3fd1a1dc5e25":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ddac07875f64b25a1fa6aa438fde71e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"275e8e184ac5495c93cb3f29c359c2b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c384ea25ecf64e1d89bb1ffcf906ee3d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d35e9fb967384f02a8b812489b9dfde5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2f3a36fc2ac04113bf0228f72837e9b4","IPY_MODEL_d755e507a49a4757806efcf85e65d2c4","IPY_MODEL_6b19ddfb56eb43ee9c11ffdb716ff581"],"layout":"IPY_MODEL_44ec7db64ff4436990e4d5e5046af780"}},"2f3a36fc2ac04113bf0228f72837e9b4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e38a4d1499004be78f384ce7bea94b9d","placeholder":"​","style":"IPY_MODEL_fc41f0a31f564afd9909f19ec64b1184","value":"Evaluating: 100%"}},"d755e507a49a4757806efcf85e65d2c4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bc1bf6994fd044ae809fde1ac1fd1366","max":74,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6565089486a047e3842b3a4c7f9400f0","value":74}},"6b19ddfb56eb43ee9c11ffdb716ff581":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c37a9b2c80d9404b855e17014a133298","placeholder":"​","style":"IPY_MODEL_b5c69af39b73469c83fd5dfbbeef6c72","value":" 74/74 [00:42&lt;00:00,  2.01it/s]"}},"44ec7db64ff4436990e4d5e5046af780":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e38a4d1499004be78f384ce7bea94b9d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc41f0a31f564afd9909f19ec64b1184":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bc1bf6994fd044ae809fde1ac1fd1366":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6565089486a047e3842b3a4c7f9400f0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c37a9b2c80d9404b855e17014a133298":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b5c69af39b73469c83fd5dfbbeef6c72":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5a9c73ae98d8448b907c1c92e5d6ebd7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_008a530497eb49dab86b395eb05a82aa","IPY_MODEL_b77231d9853340acb746932fb2322fc2","IPY_MODEL_cb3c96302e8345998e65531e5b1ded76"],"layout":"IPY_MODEL_a5e1e6ce02df43a59f4037bfa0a3fed0"}},"008a530497eb49dab86b395eb05a82aa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6b6ccea8d8b54ae48b05ad4bc85d10df","placeholder":"​","style":"IPY_MODEL_799f1c51ef594ba8afbafda1d22aa29e","value":"Evaluating: 100%"}},"b77231d9853340acb746932fb2322fc2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7af6599d46444c9186bb4f00dad01bb9","max":74,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9dd247b168ff476cb476698fa5052f5b","value":74}},"cb3c96302e8345998e65531e5b1ded76":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d3a4addce7b04b5c9b33a0f5ad183d36","placeholder":"​","style":"IPY_MODEL_a62c452bef684161822f74d93b1c5f4d","value":" 74/74 [00:43&lt;00:00,  2.00it/s]"}},"a5e1e6ce02df43a59f4037bfa0a3fed0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b6ccea8d8b54ae48b05ad4bc85d10df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"799f1c51ef594ba8afbafda1d22aa29e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7af6599d46444c9186bb4f00dad01bb9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9dd247b168ff476cb476698fa5052f5b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d3a4addce7b04b5c9b33a0f5ad183d36":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a62c452bef684161822f74d93b1c5f4d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d5662382754e4f46842e516711b75b59":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9db212b6350842f2904f1ded596d9dc5","IPY_MODEL_551a3cffc3f54ad5a7f762e934e2f2a1","IPY_MODEL_85f5ac315fd4438bbcefc0f950c8049f"],"layout":"IPY_MODEL_ce21a3b843a848b7bd6eb9774ca3f327"}},"9db212b6350842f2904f1ded596d9dc5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_504cd46176fa43879d1a4c3bb2451d9c","placeholder":"​","style":"IPY_MODEL_16f8e089a685450fb641f03cdd6b31df","value":"Evaluating: 100%"}},"551a3cffc3f54ad5a7f762e934e2f2a1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fef79cabd3834100b9a505c06f96ce5e","max":74,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f93810b9f38b42c2b36589495258a8e4","value":74}},"85f5ac315fd4438bbcefc0f950c8049f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cd28f0de33a8477d8d7f8df627d05488","placeholder":"​","style":"IPY_MODEL_8f00ca3dbeba41fab7621ec16b806be9","value":" 74/74 [00:43&lt;00:00,  2.02it/s]"}},"ce21a3b843a848b7bd6eb9774ca3f327":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"504cd46176fa43879d1a4c3bb2451d9c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"16f8e089a685450fb641f03cdd6b31df":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fef79cabd3834100b9a505c06f96ce5e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f93810b9f38b42c2b36589495258a8e4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cd28f0de33a8477d8d7f8df627d05488":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f00ca3dbeba41fab7621ec16b806be9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"163075b89b5f4da0a576a50fbb191dba":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_af1c1e7bf4cc4656beb88d21e91aea89","IPY_MODEL_02e95011ce2845978adb55bc170508cf","IPY_MODEL_2f542a401ce04220b5e3cf92c2b732db"],"layout":"IPY_MODEL_89e3cf8bccd740d5bd32937b9c06245c"}},"af1c1e7bf4cc4656beb88d21e91aea89":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_31525768d6364efb8c02d6d4698ea84d","placeholder":"​","style":"IPY_MODEL_8582393449c14d299c423df014326e4c","value":"Evaluating: 100%"}},"02e95011ce2845978adb55bc170508cf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_58aa2e07cff242368f79a6091c4d4f79","max":74,"min":0,"orientation":"horizontal","style":"IPY_MODEL_603f7e91cd354aa0920af7147b82f289","value":74}},"2f542a401ce04220b5e3cf92c2b732db":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1a811efc37f24f378dae559636a8e6ba","placeholder":"​","style":"IPY_MODEL_e752fa080a764463bc2b309d9cb79a55","value":" 74/74 [00:43&lt;00:00,  2.00it/s]"}},"89e3cf8bccd740d5bd32937b9c06245c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"31525768d6364efb8c02d6d4698ea84d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8582393449c14d299c423df014326e4c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"58aa2e07cff242368f79a6091c4d4f79":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"603f7e91cd354aa0920af7147b82f289":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1a811efc37f24f378dae559636a8e6ba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e752fa080a764463bc2b309d9cb79a55":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"10e602466abf4eba8ae9375e09ddd0f7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8092db948e32476d84bf7f9e842c561b","IPY_MODEL_20fd96620208415c9a2f8ad3e94fc533","IPY_MODEL_9a53d8b3cf484e839615879088149280"],"layout":"IPY_MODEL_e7c39b797de74d9fafe77b7ad8942160"}},"8092db948e32476d84bf7f9e842c561b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_51985228ac1543f88a608da514bfc491","placeholder":"​","style":"IPY_MODEL_dca5fc7fe3624452974d739011c435fc","value":"Evaluating: 100%"}},"20fd96620208415c9a2f8ad3e94fc533":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6391826948fc43b7be6fbbf531f510c0","max":74,"min":0,"orientation":"horizontal","style":"IPY_MODEL_03c4f8e985b146a4a25156d11c6717df","value":74}},"9a53d8b3cf484e839615879088149280":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f65c88dee8f64ad5a6295aab351d9c00","placeholder":"​","style":"IPY_MODEL_ee837e63acda4d02912d52a29ac6f51c","value":" 74/74 [00:43&lt;00:00,  2.02it/s]"}},"e7c39b797de74d9fafe77b7ad8942160":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"51985228ac1543f88a608da514bfc491":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dca5fc7fe3624452974d739011c435fc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6391826948fc43b7be6fbbf531f510c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"03c4f8e985b146a4a25156d11c6717df":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f65c88dee8f64ad5a6295aab351d9c00":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee837e63acda4d02912d52a29ac6f51c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cb4708117b0942f5a5269927c9eee111":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_01d2b57f317148f69d8124b1759adfd0","IPY_MODEL_9114948f67794ad6ba907d8b3d8d7365","IPY_MODEL_4e1b139835d943bda4729f4e8eaf7322"],"layout":"IPY_MODEL_0a2cf414c115424796f0d7bf7feecabf"}},"01d2b57f317148f69d8124b1759adfd0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8b0dddc29af74559920a6e5811db2bd9","placeholder":"​","style":"IPY_MODEL_8fb068c6fbc5463e80770bc44170ef0d","value":"Evaluating: 100%"}},"9114948f67794ad6ba907d8b3d8d7365":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ce81538d1e7d404bb78136ec280fbdd1","max":74,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e1f2990683bb41519a786f51e8aeb16d","value":74}},"4e1b139835d943bda4729f4e8eaf7322":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4db4b23282cd4421b99c51f629b2b372","placeholder":"​","style":"IPY_MODEL_ca547bb00543414cbf0cf245f0dd9e15","value":" 74/74 [00:43&lt;00:00,  2.02it/s]"}},"0a2cf414c115424796f0d7bf7feecabf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b0dddc29af74559920a6e5811db2bd9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8fb068c6fbc5463e80770bc44170ef0d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ce81538d1e7d404bb78136ec280fbdd1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e1f2990683bb41519a786f51e8aeb16d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4db4b23282cd4421b99c51f629b2b372":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca547bb00543414cbf0cf245f0dd9e15":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"69d8bdfabbe8437993262c88fd209d77":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bab73ea3c940474498f4c34cc511e82d","IPY_MODEL_b3135eae16dd4d71bb1bbbc8c28e44b5","IPY_MODEL_a847a4b5533248d5b9988b7929511d96"],"layout":"IPY_MODEL_7708483a947c4ba9ac8380933101bea5"}},"bab73ea3c940474498f4c34cc511e82d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e9122d06155c4ee397a383e5b7fa610e","placeholder":"​","style":"IPY_MODEL_99076b4ce68e42969523e89405f46e20","value":"Evaluating: 100%"}},"b3135eae16dd4d71bb1bbbc8c28e44b5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_69c8250ed18f40daaa6705aeb4092edd","max":74,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7ad8616f03584f83a99dee1ac2d17932","value":74}},"a847a4b5533248d5b9988b7929511d96":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_80867138d8e742d4b86e01dab4e6ea20","placeholder":"​","style":"IPY_MODEL_5235c597d5bc4f5082bdbd3a60ec29ed","value":" 74/74 [00:42&lt;00:00,  2.02it/s]"}},"7708483a947c4ba9ac8380933101bea5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9122d06155c4ee397a383e5b7fa610e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99076b4ce68e42969523e89405f46e20":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"69c8250ed18f40daaa6705aeb4092edd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ad8616f03584f83a99dee1ac2d17932":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"80867138d8e742d4b86e01dab4e6ea20":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5235c597d5bc4f5082bdbd3a60ec29ed":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"60dd8119f62845cab5af2accf91e04b3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d54e224ff7214025a2a6dd8ec28bdc4e","IPY_MODEL_55ca1ee2492247fab8968cd921b41e7e","IPY_MODEL_688d46b27cc44d90a6d6a546f2251033"],"layout":"IPY_MODEL_54c1c0c5959a4bf2b5cad791c4d35c73"}},"d54e224ff7214025a2a6dd8ec28bdc4e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a9085a21da11456395451bca8c717f77","placeholder":"​","style":"IPY_MODEL_ddd18ad298954a9c82cc45d344596a41","value":"Evaluating: 100%"}},"55ca1ee2492247fab8968cd921b41e7e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_31c3033803664ebfa1f25325bf5f0da8","max":129,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e1afe82c621c43d88bd5f52c4dd1cbfa","value":129}},"688d46b27cc44d90a6d6a546f2251033":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a43c8fb16ff4bc4aca054e0fbd5f5d1","placeholder":"​","style":"IPY_MODEL_a2be777520a041068a575a232f837668","value":" 129/129 [01:16&lt;00:00,  2.05it/s]"}},"54c1c0c5959a4bf2b5cad791c4d35c73":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a9085a21da11456395451bca8c717f77":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ddd18ad298954a9c82cc45d344596a41":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"31c3033803664ebfa1f25325bf5f0da8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e1afe82c621c43d88bd5f52c4dd1cbfa":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5a43c8fb16ff4bc4aca054e0fbd5f5d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a2be777520a041068a575a232f837668":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d18c5adb97614fd6b7d905dcca64a3aa":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_258d29683b354569bc17c90fdaee63f2","IPY_MODEL_0e0e81f2693b45e598f685f8e09c1670","IPY_MODEL_61f61bd84ca144ceb179db65a7ac4857"],"layout":"IPY_MODEL_d49dde65004e4645900186dc7eabcd79"}},"258d29683b354569bc17c90fdaee63f2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f04ec0bbf58542d281d6f64078ae7b91","placeholder":"​","style":"IPY_MODEL_eec6fca336d34120a7e3f2bd27109a4e","value":"Downloading (…)solve/main/vocab.txt: 100%"}},"0e0e81f2693b45e598f685f8e09c1670":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_00712b6fe8ee440b934b505f1b7744ae","max":254703,"min":0,"orientation":"horizontal","style":"IPY_MODEL_92f9119565d847d0a8a8e81f4b7724a4","value":254703}},"61f61bd84ca144ceb179db65a7ac4857":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_39413c3ea3674e73b6f11dd37786975f","placeholder":"​","style":"IPY_MODEL_e49d3ceda2724c94a725adc1c5191521","value":" 255k/255k [00:00&lt;00:00, 282kB/s]"}},"d49dde65004e4645900186dc7eabcd79":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f04ec0bbf58542d281d6f64078ae7b91":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eec6fca336d34120a7e3f2bd27109a4e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"00712b6fe8ee440b934b505f1b7744ae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"92f9119565d847d0a8a8e81f4b7724a4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"39413c3ea3674e73b6f11dd37786975f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e49d3ceda2724c94a725adc1c5191521":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d43e6592abb74c2894fda00f51ad0009":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5fc3c7c4d7c94a2180da06db08d898c9","IPY_MODEL_5bc5941cb161465aa6f0b49fca6b642e","IPY_MODEL_fe46048d43954432a5f98b46c6ed5ddb"],"layout":"IPY_MODEL_ad879f6e4d914a6494dfbcbd3b410ba9"}},"5fc3c7c4d7c94a2180da06db08d898c9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_674a7e87d9e142cb8e2a9b2bc1d6e076","placeholder":"​","style":"IPY_MODEL_55c401fdf46344da9c6030f3406c8bcb","value":"Downloading (…)lve/main/config.json: 100%"}},"5bc5941cb161465aa6f0b49fca6b642e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a2d30ce6eb1248368b53aed074bc3de1","max":1403,"min":0,"orientation":"horizontal","style":"IPY_MODEL_706415f3e3454d4b85b9164fdf8b853c","value":1403}},"fe46048d43954432a5f98b46c6ed5ddb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_562c5f93269d4b51b014c67b3d890fff","placeholder":"​","style":"IPY_MODEL_24646e6ad74049cbbc021fc48617d8a1","value":" 1.40k/1.40k [00:00&lt;00:00, 75.0kB/s]"}},"ad879f6e4d914a6494dfbcbd3b410ba9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"674a7e87d9e142cb8e2a9b2bc1d6e076":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55c401fdf46344da9c6030f3406c8bcb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a2d30ce6eb1248368b53aed074bc3de1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"706415f3e3454d4b85b9164fdf8b853c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"562c5f93269d4b51b014c67b3d890fff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"24646e6ad74049cbbc021fc48617d8a1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cce945b99ccb4bfab1fde96fc0faa0c2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d5f5652e44e64186942f365b42641471","IPY_MODEL_54a5b75d316f461c83b8225bb6630bbc","IPY_MODEL_5c5a8b83209647d08c375456c3215629"],"layout":"IPY_MODEL_6353019d5d124906b160e8ff08eecc47"}},"d5f5652e44e64186942f365b42641471":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dbf80868c0a741d498061d9bf2a6f606","placeholder":"​","style":"IPY_MODEL_ba04362d161c48f5a908826122618d04","value":"Convert 2055 of 2056 example to features: 100%"}},"54a5b75d316f461c83b8225bb6630bbc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_27a3010902ba4754a7abbd53f9a07d35","max":2056,"min":0,"orientation":"horizontal","style":"IPY_MODEL_48b1b5fb69084a08a27a8f27795d146d","value":2056}},"5c5a8b83209647d08c375456c3215629":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c30e0eace4a3427085cf6d54325fce0c","placeholder":"​","style":"IPY_MODEL_a75c93ae6e1240149f66c5809fad582e","value":" 2056/2056 [00:18&lt;00:00, 107.77it/s]"}},"6353019d5d124906b160e8ff08eecc47":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dbf80868c0a741d498061d9bf2a6f606":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba04362d161c48f5a908826122618d04":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"27a3010902ba4754a7abbd53f9a07d35":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48b1b5fb69084a08a27a8f27795d146d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c30e0eace4a3427085cf6d54325fce0c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a75c93ae6e1240149f66c5809fad582e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}