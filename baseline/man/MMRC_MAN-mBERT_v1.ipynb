{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1172,"status":"ok","timestamp":1676640118170,"user":{"displayName":"Trọng Khôi Hoàng","userId":"12933784847383051959"},"user_tz":-420},"id":"kimkG7xp6wvS","outputId":"79e70073-e119-4ee6-d323-7cfbedcceafd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Fri Feb 17 13:22:01 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   62C    P0    30W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18412,"status":"ok","timestamp":1676640136580,"user":{"displayName":"Trọng Khôi Hoàng","userId":"12933784847383051959"},"user_tz":-420},"id":"n8OLTA2cqdHk","outputId":"01a5beb6-0ed1-44e4-d547-449efb4c69b3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1676640136581,"user":{"displayName":"Trọng Khôi Hoàng","userId":"12933784847383051959"},"user_tz":-420},"id":"G4x_F5OtS5V8","outputId":"77000c48-6784-4e3e-e02d-3958fc210851"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1A2lqKb4qo-FtxvfSl5gHiEC9QAa1uKxO/ViMMRC_model\n"]}],"source":["%cd '/content/drive/MyDrive/NCKH/ViMMRC_model/'"]},{"cell_type":"markdown","metadata":{"id":"K_Pv9CIYm7BZ"},"source":["# Library\n","\n","https://github.com/mhardalov/exams-qa \\\n","https://github.com/microsoft/MT-DNN \\\n","https://github.com/jind11/MMM-MCQA"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":92646,"status":"ok","timestamp":1676640100343,"user":{"displayName":"Trọng Khôi Hoàng","userId":"12933784847383051959"},"user_tz":-420},"id":"7znvNnvg-0de","outputId":"7f7d4367-29db-45d8-e32e-577429e70307"},"outputs":[{"output_type":"stream","name":"stdout","text":["  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m753.4/753.4 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 KB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.3/114.3 KB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 KB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.3/216.3 KB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m635.7/635.7 KB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 KB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m109.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 KB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for fairseq (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for fastBPE (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.14.1+cu116 requires torch==1.13.1, but you have torch 1.4.0 which is incompatible.\n","torchtext 0.14.1 requires torch==1.13.1, but you have torch 1.4.0 which is incompatible.\n","torchaudio 0.13.1+cu116 requires torch==1.13.1, but you have torch 1.4.0 which is incompatible.\n","fastai 2.7.10 requires torch<1.14,>=1.7, but you have torch 1.4.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["%pip -q install -e git+https://github.com/microsoft/MT-DNN.git#egg=mtdnn"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":88765,"status":"ok","timestamp":1676640225342,"user":{"displayName":"Trọng Khôi Hoàng","userId":"12933784847383051959"},"user_tz":-420},"id":"ksz16DuhgUtk","outputId":"0ea41f03-a990-4d8c-d98c-b9a6e1f3332f"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.4/887.4 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 KB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","mtdnn 1.1.0 requires torch==1.4.0, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m103.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","mtdnn 1.1.0 requires torch==1.4.0, but you have torch 1.13.1 which is incompatible.\n","mtdnn 1.1.0 requires transformers==2.9.0, but you have transformers 4.26.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["%pip -q install torch -U\n","%pip -q install transformers -U\n","%pip -q install tqdm -U\n","# %pip -q install --upgrade  git+https://github.com/lanpa/tensorboardX.git"]},{"cell_type":"markdown","metadata":{"id":"dT2PDGixT400"},"source":["Install apex package: https://stackoverflow.com/a/74561776\n","\n","Query the version Ubuntu Colab is running on:\\\n","`!lsb_release -a`\n","\n","Get the current cuda version run:\\\n","`!nvcc --version`"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1676547412557,"user":{"displayName":"Tường Phạm Quang","userId":"05102412219596796118"},"user_tz":-420},"id":"qWFKkQbKTUBA","outputId":"d3b52965-e024-41c9-fe28-246003223455"},"outputs":[{"output_type":"stream","name":"stdout","text":["nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2022 NVIDIA Corporation\n","Built on Tue_Mar__8_18:18:20_PST_2022\n","Cuda compilation tools, release 11.6, V11.6.124\n","Build cuda_11.6.r11.6/compiler.31057947_0\n"]}],"source":["!nvcc --version"]},{"cell_type":"markdown","metadata":{"id":"ioQPuCNdks1x"},"source":["# Models"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"svixfajDMreB","executionInfo":{"status":"ok","timestamp":1676640225343,"user_tz":-420,"elapsed":6,"user":{"displayName":"Trọng Khôi Hoàng","userId":"12933784847383051959"}}},"outputs":[],"source":["#%pip install git+git@github.com:microsoft/mt-dnn.git@master#egg=mtdnn"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"4x7vIh8LE40U","executionInfo":{"status":"ok","timestamp":1676640225343,"user_tz":-420,"elapsed":4,"user":{"displayName":"Trọng Khôi Hoàng","userId":"12933784847383051959"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn.parameter import Parameter\n","from mtdnn.common.dropout_wrapper import DropoutWrapper\n","from mtdnn.common.optimizer import weight_norm as WN\n","from mtdnn.common.similarity import FlatSimilarityWrapper, SelfAttnWrapper, SimilarityWrapper\n","from mtdnn.common.activation_functions import activation\n","\n","\n","class DualAttentionWrapper(nn.Module):\n","    def __init__(self, x1_dim, x2_dim, prefix='attention', opt={}, dropout=None):\n","        super(DualAttentionWrapper, self).__init__()\n","        self.prefix = prefix\n","        if dropout is None:\n","            self.att_dropout = DropoutWrapper(opt.get('{}_att_dropout'.format(self.prefix), 0))\n","        else:\n","            self.att_dropout = dropout\n","        self.score_func = SimilarityWrapper(x1_dim, x2_dim, prefix=prefix, opt=opt, dropout=self.att_dropout)\n","\n","    def forward(self, query, key, query_padding_mask=None, key_padding_mask=None, return_scores=False):\n","        logits = self.score_func(query, key)\n","        # print(query.shape, key.shape, logits.shape, key_padding_mask.sum(dim=-1).max(), key_padding_mask.shape)\n","        query_mask = query_padding_mask.unsqueeze(2).expand_as(logits)\n","        key_mask = key_padding_mask.unsqueeze(1).expand_as(logits)\n","\n","        # first get attn_query\n","        logits_key = logits.data.masked_fill(key_mask.data, -float('inf'))\n","        prob_key = F.softmax(logits_key, -1)\n","        # prob_key = prob_key.view(-1, query.size(1), key.size(1))\n","        prob_key = self.att_dropout(prob_key)\n","        attn_query = prob_key.bmm(key)\n","\n","        # then get attn_key\n","        logits_query = logits.data.masked_fill(query_mask.data, -float('inf'))\n","        prob_query = F.softmax(logits_query, 1)\n","        # prob_query = prob_query.view(-1, key.size(1), query.size(1))\n","        prob_query = self.att_dropout(prob_query)\n","        attn_key = prob_query.transpose(1, 2).bmm(query)\n","\n","        if return_scores:\n","            return attn_query, attn_key, prob_query, logits\n","        else:\n","            return attn_query, attn_key\n","\n","\n","class Classifier(nn.Module):\n","    def __init__(self, x_size, y_size, opt, prefix='decoder', dropout=None):\n","        super(Classifier, self).__init__()\n","        self.opt = opt\n","        if dropout is None:\n","            self.dropout = DropoutWrapper(opt.get('{}_dropout_p'.format(prefix), 0))\n","        else:\n","            self.dropout = dropout\n","        self.merge_opt = opt.get('{}_merge_opt'.format(prefix), 0)\n","        self.weight_norm_on = opt.get('{}_weight_norm_on'.format(prefix), False)\n","\n","        if self.merge_opt == 1:\n","            self.proj = nn.Linear(x_size * 4, y_size)\n","        else:\n","            self.proj = nn.Linear(x_size * 2, y_size)\n","\n","        if self.weight_norm_on:\n","            self.proj = WN(self.proj)\n","\n","    def forward(self, x1, x2, mask=None, activation=None):\n","        seq_len = None\n","        if len(x1.size()) == 3:\n","            bz, seq_len, hidden_size = x1.size()\n","            x1 = x1.contiguous().view(-1, hidden_size)\n","            x2 = x2.contiguous().view(-1, hidden_size)\n","\n","        if self.merge_opt == 1:\n","            x = torch.cat([x1, x2, (x1 - x2).abs(), x1 * x2], 1)\n","        else:\n","            x = torch.cat([x1, x2], 1)\n","        x = self.dropout(x)\n","        if activation:\n","            scores = activation(self.proj(x))\n","        else:\n","            scores = self.proj(x)\n","\n","        if seq_len:\n","            return scores.view(bz, seq_len, -1)\n","        else:\n","            return scores\n","\n","# mtdnn.common.san.SANClassifier() super\n","class SANClassifier(nn.Module):\n","    \"\"\"Implementation of Stochastic Answer Networks for Natural Language Inference, Xiaodong Liu, Kevin Duh and Jianfeng Gao\n","    https://arxiv.org/abs/1804.07888\n","    \"\"\"\n","    def __init__(self, x_size, h_size, label_size, opt={}, prefix='decoder', dropout=None):\n","        super(SANClassifier, self).__init__()\n","        self.prefix = prefix\n","        if dropout is None:\n","            self.dropout = DropoutWrapper(opt.get('{}_dropout_p'.format(self.prefix), 0))\n","        else:\n","            self.dropout = dropout\n","        self.query_wsum = SelfAttnWrapper(x_size, prefix='mem_cum', opt=opt, dropout=self.dropout)\n","        self.attn = FlatSimilarityWrapper(x_size, h_size, prefix, opt, self.dropout)\n","        self.rnn_type = '{}{}'.format(opt.get('{}_rnn_type'.format(prefix), 'gru').upper(), 'Cell')\n","        self.rnn = getattr(nn, self.rnn_type)(x_size, h_size)\n","        self.num_turn = opt.get('{}_num_turn'.format(prefix), 5)\n","        self.opt = opt\n","        self.mem_random_drop = opt.get('{}_mem_drop_p'.format(prefix), 0)\n","        self.mem_type = opt.get('{}_mem_type'.format(prefix), 0)\n","        self.weight_norm_on = opt.get('{}_weight_norm_on'.format(prefix), False)\n","        self.label_size = label_size\n","        self.dump_state = opt.get('dump_state_on', False)\n","        self.alpha = Parameter(torch.zeros(1, 1), requires_grad=False)\n","        # self.hyp_attn = None\n","        # if opt.get('hyp_attn_premise', 0):\n","        #     self.hyp_attn = AttentionWrapper(x_size, h_size, prefix=prefix, opt=opt, dropout=self.dropout)\n","        #     self.hyp_merge = Classifier(x_size, x_size, opt, prefix=prefix, dropout=self.dropout)\n","        self.f = activation(opt.get('{}_activation'.format(self.prefix), 'relu'))\n","        if self.weight_norm_on:\n","            self.rnn = WN(self.rnn)\n","\n","        self.classifier = Classifier(x_size, 1, opt, prefix=prefix, dropout=self.dropout)\n","\n","    def _generate_mask(self, new_data, dropout_p=0.0, is_training=False):\n","        if not is_training:\n","            dropout_p = 0.0\n","        new_data = (1 - dropout_p) * (new_data.zero_() + 1)\n","        for i in range(new_data.size(0)):\n","            one = random.randint(0, new_data.size(1) - 1)\n","            new_data[i][one] = 1\n","        mask = 1.0 / (1 - dropout_p) * torch.bernoulli(new_data)\n","        mask.requires_grad = False\n","        return \n","\n","    def forward(self, x, h0, x_mask=None, h_mask=None, is_training=True):\n","        h0 = self.query_wsum(h0, h_mask)\n","        if type(self.rnn) is nn.LSTMCell:\n","            c0 = h0.new(h0.size()).zero_()\n","        scores_list = []\n","        for turn in range(self.num_turn):\n","            att_scores = self.attn(x, h0, x_mask)\n","            x_sum = torch.bmm(F.softmax(att_scores, 1).unsqueeze(1), x).squeeze(1)\n","            scores = self.classifier(x_sum, h0)\n","            scores_list.append(scores)\n","            # next turn\n","            if self.rnn is not None:\n","                h0 = self.dropout(h0)\n","                if type(self.rnn) is nn.LSTMCell:\n","                    h0, c0 = self.rnn(x_sum, (h0, c0))\n","                else:\n","                    h0 = self.rnn(x_sum, h0)\n","        if self.mem_type == 1:\n","            batch_size = x.size(0) // self.label_size\n","            mask = self._generate_mask(self.alpha.data.new(batch_size, self.num_turn), self.mem_random_drop, is_training)\n","            mask = [m.contiguous() for m in torch.unbind(mask, 1)]\n","            tmp_scores_list = [mask[idx].view(batch_size, 1).expand_as(inp.view(-1, self.label_size))\n","                               * F.softmax(inp.view(-1, self.label_size), 1)\n","                               for idx, inp in enumerate(scores_list)]\n","            scores = torch.stack(tmp_scores_list, 2)\n","            scores = torch.mean(scores, 2)\n","            scores = torch.log(scores)\n","        else:\n","            scores = scores_list[-1]\n","        if self.dump_state:\n","            return scores, scores_list\n","        else:\n","            return scores\n","\n","\n","class SANClassifier2(nn.Module):\n","    \"\"\"Implementation of Stochastic Answer Networks for Natural Language Inference, Xiaodong Liu, Kevin Duh and Jianfeng Gao\n","    https://arxiv.org/abs/1804.07888\n","    \"\"\"\n","    def __init__(self, x_size, h_size, label_size, opt={}, prefix='decoder', dropout=None):\n","        super(SANClassifier2, self).__init__()\n","        self.prefix = prefix\n","        if dropout is None:\n","            self.dropout = DropoutWrapper(opt.get('{}_dropout_p'.format(self.prefix), 0))\n","        else:\n","            self.dropout = dropout\n","        self.dual_attn = DualAttentionWrapper(x_size, h_size, prefix, opt, self.dropout)\n","        self.query_wsum = SelfAttnWrapper(x_size, prefix='mem_cum', opt=opt, dropout=self.dropout)\n","        self.attn = FlatSimilarityWrapper(x_size, h_size, prefix, opt, self.dropout)\n","        self.rnn_type = '{}{}'.format(opt.get('{}_rnn_type'.format(prefix), 'gru').upper(), 'Cell')\n","        self.rnn = getattr(nn, self.rnn_type)(x_size, h_size)\n","        self.num_turn = opt.get('{}_num_turn'.format(prefix), 5)\n","        self.opt = opt\n","        self.mem_random_drop = opt.get('{}_mem_drop_p'.format(prefix), 0)\n","        self.mem_type = opt.get('{}_mem_type'.format(prefix), 0)\n","        self.weight_norm_on = opt.get('{}_weight_norm_on'.format(prefix), False)\n","        self.label_size = label_size\n","        self.dump_state = opt.get('dump_state_on', False)\n","        self.alpha = Parameter(torch.zeros(1, 1), requires_grad=False)\n","        self.f = activation(opt.get('{}_activation'.format(self.prefix), 'relu'))\n","        self.hyp_first = opt.get('{}_hyp_first'.format(prefix), 1)\n","        self.hyp_raw = opt.get('{}_hyp_raw'.format(prefix), 0)\n","        if self.weight_norm_on:\n","            self.rnn = WN(self.rnn)\n","\n","        self.classifier = Classifier(x_size, 1, opt, prefix=prefix, dropout=self.dropout)\n","\n","        self.premise_merge = Classifier(x_size, x_size, opt, prefix=prefix, dropout=self.dropout)\n","        self.hyp_merge = Classifier(x_size, x_size, opt, prefix=prefix, dropout=self.dropout)\n","\n","    def _generate_mask(self, new_data, dropout_p=0.0, is_training=False):\n","        if not is_training:\n","            dropout_p = 0.0\n","        new_data = (1 - dropout_p) * (new_data.zero_() + 1)\n","        for i in range(new_data.size(0)):\n","            one = random.randint(0, new_data.size(1) - 1)\n","            new_data[i][one] = 1\n","        mask = 1.0 / (1 - dropout_p) * torch.bernoulli(new_data)\n","        mask.requires_grad = False\n","        return \n","\n","    def forward(self, x, h, x_mask=None, h_mask=None, is_training=True):\n","        if self.hyp_first and self.hyp_raw:\n","            pass\n","        elif self.hyp_first and not self.hyp_raw:\n","            _, h_attn = self.dual_attn(x, h, x_mask, h_mask)\n","            h = self.hyp_merge(h, h_attn, activation=self.f)\n","        else:\n","            raise NotImplementedError\n","\n","        h0 = self.query_wsum(h, h_mask)\n","        if type(self.rnn) is nn.LSTMCell:\n","            c0 = h0.new(h0.size()).zero_()\n","        scores_list = []\n","        for turn in range(self.num_turn):\n","            att_scores = self.attn(x, h0, x_mask)\n","            x_sum = torch.bmm(F.softmax(att_scores, 1).unsqueeze(1), x).squeeze(1)\n","            scores = self.classifier(x_sum, h0)\n","            scores_list.append(scores)\n","            # next turn\n","            if self.rnn is not None:\n","                h0 = self.dropout(h0)\n","                if type(self.rnn) is nn.LSTMCell:\n","                    h0, c0 = self.rnn(x_sum, (h0, c0))\n","                else:\n","                    h0 = self.rnn(x_sum, h0)\n","        if self.mem_type == 1:\n","            batch_size = x.size(0) // self.label_size\n","            mask = self._generate_mask(self.alpha.data.new(batch_size, self.num_turn), self.mem_random_drop, is_training)\n","            mask = [m.contiguous() for m in torch.unbind(mask, 1)]\n","            tmp_scores_list = [mask[idx].view(batch_size, 1).expand_as(inp.view(-1, self.label_size))\n","                               * F.softmax(inp.view(-1, self.label_size), 1)\n","                               for idx, inp in enumerate(scores_list)]\n","            scores = torch.stack(tmp_scores_list, 2)\n","            scores = torch.mean(scores, 2)\n","            scores = torch.log(scores)\n","        else:\n","            scores = scores_list[-1]\n","        if self.dump_state:\n","            return scores, scores_list\n","        else:\n","            return scores"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"jJZdHqpLB3B6","executionInfo":{"status":"ok","timestamp":1676640233709,"user_tz":-420,"elapsed":8370,"user":{"displayName":"Trọng Khôi Hoàng","userId":"12933784847383051959"}}},"outputs":[],"source":["# MMM-MCQA from https://github.com/jind11/MMM-MCQA\n","\n","#import copy\n","import math\n","import torch\n","import torch.nn as nn\n","from torch.nn import CrossEntropyLoss, NLLLoss\n","from transformers.modeling_utils import prune_linear_layer\n","from transformers.models.bert.modeling_bert import (\n","    BertPreTrainedModel,\n","    BertEmbeddings,\n","    BertSelfOutput,\n","    BertIntermediate,\n","    BertOutput,\n","    BertPooler,\n","    BertModel,\n",")\n","\n","BertLayerNorm = torch.nn.LayerNorm\n","\n","# Add speaker_embeddings\n","# class BertEmbeddings(nn.Module):\n","#     def __init__(self, config, speaker_embeddings=False):\n","#     def forward(self, input_ids, token_type_ids=None, speaker_ids=None):\n","\n","# Config output_attentions, keep_multihead_output for output\n","class BertSelfAttention(nn.Module):\n","    def __init__(self, config, output_attentions=False, keep_multihead_output=False):\n","        super(BertSelfAttention, self).__init__()\n","        if config.hidden_size % config.num_attention_heads != 0:\n","            raise ValueError(\n","                \"The hidden size (%d) is not a multiple of the number of attention \"\n","                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads))\n","        self.output_attentions = output_attentions\n","        self.keep_multihead_output = keep_multihead_output\n","        self.multihead_output = None\n","\n","        self.num_attention_heads = config.num_attention_heads\n","        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n","        self.all_head_size = self.num_attention_heads * self.attention_head_size\n","\n","        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n","        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n","        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n","\n","        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n","\n","    def transpose_for_scores(self, x):\n","        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n","        x = x.view(*new_x_shape)\n","        return x.permute(0, 2, 1, 3)\n","\n","    def forward(self, hidden_states, attention_mask, head_mask=None):\n","        mixed_query_layer = self.query(hidden_states)\n","        mixed_key_layer = self.key(hidden_states)\n","        mixed_value_layer = self.value(hidden_states)\n","\n","        query_layer = self.transpose_for_scores(mixed_query_layer)\n","        key_layer = self.transpose_for_scores(mixed_key_layer)\n","        value_layer = self.transpose_for_scores(mixed_value_layer)\n","\n","        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n","        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n","        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n","        # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n","        attention_scores = attention_scores + attention_mask\n","\n","        # Normalize the attention scores to probabilities.\n","        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n","\n","        # This is actually dropping out entire tokens to attend to, which might\n","        # seem a bit unusual, but is taken from the original Transformer paper.\n","        attention_probs = self.dropout(attention_probs)\n","\n","        # Mask heads if we want to\n","        if head_mask is not None:\n","            attention_probs = attention_probs * head_mask\n","\n","        context_layer = torch.matmul(attention_probs, value_layer)\n","        if self.keep_multihead_output:\n","            self.multihead_output = context_layer\n","            self.multihead_output.retain_grad()\n","\n","        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n","        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n","        context_layer = context_layer.view(*new_context_layer_shape)\n","        if self.output_attentions:\n","            return attention_probs, context_layer\n","        return context_layer\n","\n","# Config output_attentions, keep_multihead_output for BertSelfAttention(), change head pruned, attention output\n","class BertAttention(nn.Module):\n","    def __init__(self, config, output_attentions=False, keep_multihead_output=False):\n","        super(BertAttention, self).__init__()\n","        self.output_attentions = output_attentions\n","        self.self = BertSelfAttention(config, output_attentions=output_attentions, keep_multihead_output=keep_multihead_output)\n","        self.output = BertSelfOutput(config)\n","\n","    def prune_heads(self, heads):\n","        if len(heads) == 0:\n","            return\n","        mask = torch.ones(self.self.num_attention_heads, self.self.attention_head_size)\n","        for head in heads:\n","            mask[head] = 0\n","        mask = mask.view(-1).contiguous().eq(1)\n","        index = torch.arange(len(mask))[mask].long()\n","\n","        # Prune linear layers\n","        self.self.query = prune_linear_layer(self.self.query, index)\n","        self.self.key = prune_linear_layer(self.self.key, index)\n","        self.self.value = prune_linear_layer(self.self.value, index)\n","        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n","\n","        # Update hyper params\n","        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n","        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n","\n","    def forward(self, input_tensor, attention_mask, head_mask=None):\n","        self_output = self.self(input_tensor, attention_mask, head_mask)\n","        if self.output_attentions:\n","            attentions, self_output = self_output\n","        attention_output = self.output(self_output, input_tensor)\n","        if self.output_attentions:\n","            return attentions, attention_output\n","        return attention_output\n","\n","# Config output_attentions, keep_multihead_output for BertAttention(), change attentions output on forward()\n","class BertLayer(nn.Module):\n","    def __init__(self, config, output_attentions=False, keep_multihead_output=False):\n","        super(BertLayer, self).__init__()\n","        self.output_attentions = output_attentions\n","        self.attention = BertAttention(config, output_attentions=output_attentions, keep_multihead_output=keep_multihead_output)\n","        self.intermediate = BertIntermediate(config)\n","        self.output = BertOutput(config)\n","\n","    def forward(self, hidden_states, attention_mask, head_mask=None):\n","        attention_output = self.attention(hidden_states, attention_mask, head_mask)\n","        if self.output_attentions:\n","            attentions, attention_output = attention_output\n","        intermediate_output = self.intermediate(attention_output)\n","        layer_output = self.output(intermediate_output, attention_output)\n","        if self.output_attentions:\n","            return attentions, layer_output\n","        return layer_output\n","\n","# Config output_attentions, keep_multihead_output for BertLayer(), change attentions layer in forward() \n","class BertEncoder(nn.Module):\n","    def __init__(self, config, output_attentions=False, keep_multihead_output=False):\n","        super(BertEncoder, self).__init__()\n","        self.output_attentions = output_attentions\n","        self.layer = nn.ModuleList([\n","            BertLayer(config, output_attentions=output_attentions, keep_multihead_output=keep_multihead_output)\n","            for _ in range(config.num_hidden_layers)\n","        ])\n","\n","    def forward(self, hidden_states, attention_mask, output_all_encoded_layers=True, head_mask=None):\n","        all_encoder_layers = []\n","        all_attentions = []\n","        for i, layer_module in enumerate(self.layer):\n","            hidden_states = layer_module(hidden_states, attention_mask, head_mask[i])\n","            if self.output_attentions:\n","                attentions, hidden_states = hidden_states\n","                all_attentions.append(attentions)\n","            if output_all_encoded_layers:\n","                all_encoder_layers.append(hidden_states)\n","        if not output_all_encoded_layers:\n","            all_encoder_layers.append(hidden_states)\n","        if self.output_attentions:\n","            return all_attentions, all_encoder_layers\n","        return all_encoder_layers\n","\n","# Config output_attentions, keep_multihead_output\n","class BertModel(BertPreTrainedModel):\n","    def __init__(self, config, output_attentions=False, keep_multihead_output=False, speaker_embeddings=False):\n","        super(BertModel, self).__init__(config)\n","\n","        self.output_attentions = output_attentions\n","        self.embeddings = BertEmbeddings(config)\n","        self.encoder = BertEncoder(config, output_attentions=output_attentions, keep_multihead_output=keep_multihead_output)\n","        self.pooler = BertPooler(config)\n","        self.apply(self._init_weights)#init_bert_weights)\n","\n","    def prune_heads(self, heads_to_prune):\n","        \"\"\" Prunes heads of the model.\n","            heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n","        \"\"\"\n","        for layer, heads in heads_to_prune.items():\n","            self.encoder.layer[layer].attention.prune_heads(heads)\n","\n","    def get_multihead_outputs(self):\n","        \"\"\" Gather all multi-head outputs.\n","            Return: list (layers) of multihead module outputs with gradients\n","        \"\"\"\n","        return [layer.attention.self.multihead_output for layer in self.encoder.layer]\n","\n","    def forward(self, input_ids, token_type_ids=None, attention_mask=None, speaker_ids=None, output_all_encoded_layers=True, head_mask=None):\n","        if attention_mask is None:\n","            attention_mask = torch.ones_like(input_ids)\n","        if token_type_ids is None:\n","            token_type_ids = torch.zeros_like(input_ids)\n","\n","        # We create a 3D attention mask from a 2D tensor mask.\n","        # Sizes are [batch_size, 1, 1, to_seq_length]\n","        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n","        # this attention mask is more simple than the triangular masking of causal attention\n","        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n","        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n","\n","        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n","        # masked positions, this operation will create a tensor which is 0.0 for\n","        # positions we want to attend and -10000.0 for masked positions.\n","        # Since we are adding it to the raw scores before the softmax, this is\n","        # effectively the same as removing these entirely.\n","        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n","        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n","\n","        # Prepare head mask if needed\n","        # 1.0 in head_mask indicate we keep the head\n","        # attention_probs has shape bsz x n_heads x N x N\n","        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n","        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n","        if head_mask is not None:\n","            if head_mask.dim() == 1:\n","                head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n","                head_mask = head_mask.expand_as(self.config.num_hidden_layers, -1, -1, -1, -1)\n","            elif head_mask.dim() == 2:\n","                head_mask = head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)  # We can specify head_mask for each layer\n","            head_mask = head_mask.to(dtype=next(self.parameters()).dtype) # switch to fload if need + fp16 compatibility\n","        else:\n","            head_mask = [None] * self.config.num_hidden_layers\n","\n","        embedding_output = self.embeddings(input_ids, token_type_ids, speaker_ids)\n","        encoded_layers = self.encoder(embedding_output,\n","                                      extended_attention_mask,\n","                                      head_mask=head_mask,\n","                                      output_all_encoded_layers=output_all_encoded_layers)\n","        if self.output_attentions:\n","            all_attentions, encoded_layers = encoded_layers\n","        sequence_output = encoded_layers[-1]\n","        pooled_output = self.pooler(sequence_output)\n","\n","        if not output_all_encoded_layers:\n","            encoded_layers = encoded_layers[-1]\n","        if self.output_attentions:\n","            return all_attentions, encoded_layers, pooled_output\n","        return encoded_layers, pooled_output\n","\n","# Config output_attentions, keep_multihead_output, add premise_mask, hyp_mask attention\n","class BertForMultipleChoice_SAN(BertPreTrainedModel):\n","    def __init__(self, config, opt, num_choices, output_attentions=False, keep_multihead_output=False, same_linear_layer=0):\n","        super(BertForMultipleChoice_SAN, self).__init__(config)\n","        self.output_attentions = output_attentions\n","        self.num_choices = num_choices\n","        self.bert = BertModel(config, output_attentions=output_attentions, keep_multihead_output=keep_multihead_output)\n","        self.opt = opt\n","        self.use_SAN = opt.get('use_SAN', 1)\n","        self.same_linear_layer = same_linear_layer\n","        if not self.use_SAN:\n","            self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","            # self.classifier = nn.ModuleList([nn.Linear(config.hidden_size, 1)] * len(num_choices))\n","            if same_linear_layer:\n","                self.classifier = nn.Linear(config.hidden_size, 1)\n","            else:\n","                self.classifier = nn.ModuleList([nn.Linear(config.hidden_size, 1)] * len(num_choices))\n","        else:\n","            if same_linear_layer:\n","                self.out_proj = SANClassifier(config.hidden_size, config.hidden_size, 1, opt, prefix='answer')\n","            else:\n","                self.out_proj = []\n","                for num_choice in num_choices:\n","                    self.out_proj.append(SANClassifier(config.hidden_size, config.hidden_size, num_choice, opt, prefix='answer'))\n","                self.out_proj = nn.ModuleList(self.out_proj)\n","\n","        self.apply(self._init_weights)\n","\n","    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None, head_mask=None, premise_mask=None, hyp_mask=None, is_training=True, task_id=None):\n","        input_ids = input_ids.view(-1, input_ids.size(-1))\n","        token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n","        attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n","\n","        outputs = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False, head_mask=head_mask)\n","        if self.output_attentions:\n","            all_attentions, sequence_output, pooled_output = outputs\n","        else:\n","            sequence_output, pooled_output = outputs\n","        # pooled_output = self.dropout(pooled_output)\n","        # logits = self.classifier(pooled_output)\n","        # reshaped_logits = logits.view(-1, self.num_choices)\n","\n","        # SAN module\n","        if self.use_SAN:\n","            premise_mask = premise_mask.view(-1, premise_mask.size(-1)) if premise_mask is not None else None\n","            hyp_mask = hyp_mask.view(-1, hyp_mask.size(-1)) if hyp_mask is not None else None\n","            max_query = hyp_mask.size(1)\n","            hyp_mem = sequence_output[:, :max_query]\n","            if self.same_linear_layer:\n","                logits = self.out_proj(sequence_output, hyp_mem, premise_mask, hyp_mask, is_training=is_training)\n","            else:\n","                logits = self.out_proj[task_id](sequence_output, hyp_mem, premise_mask, hyp_mask, is_training=is_training)\n","        else:\n","            pooled_output = self.dropout(pooled_output)\n","            if self.same_linear_layer:\n","                logits = self.classifier(pooled_output)\n","            else:\n","                logits = self.classifier[task_id](pooled_output)\n","\n","        if labels is not None:\n","            if self.opt.get('answer_mem_type', 0):\n","                loss_fct = NLLLoss()\n","            else:\n","                loss_fct = CrossEntropyLoss()\n","                logits = logits.view(-1, self.num_choices[task_id])\n","            labels = labels.view(-1)\n","            loss = loss_fct(logits, labels)\n","            return loss, logits\n","        elif self.output_attentions:\n","            return all_attentions, logits\n","        return logits\n","\n","\n","class BertForMultipleChoice_SAN2(BertPreTrainedModel):\n","    def __init__(self, config, opt, num_choices, output_attentions=False, keep_multihead_output=False):\n","        super(BertForMultipleChoice_SAN2, self).__init__(config)\n","        self.output_attentions = output_attentions\n","        self.num_choices = num_choices\n","        self.bert = BertModel(config, output_attentions=output_attentions, keep_multihead_output=keep_multihead_output)\n","        self.opt = opt\n","        self.use_SAN = opt.get('use_SAN', 1)\n","        if not self.use_SAN:\n","            self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","            self.classifier = nn.ModuleList([nn.Linear(config.hidden_size, 1)] * len(num_choices))\n","        else:\n","            self.out_proj = []\n","            for num_choice in num_choices:\n","                self.out_proj.append(SANClassifier2(config.hidden_size, config.hidden_size, num_choice, opt, prefix='answer'))\n","            self.out_proj = nn.ModuleList(self.out_proj)\n","\n","        self.apply(self._init_weights)#init_bert_weights)\n","\n","    def forward(\n","        self,\n","        input_ids,\n","        token_type_ids=None,\n","        attention_mask=None,\n","        labels=None,\n","        head_mask=None,\n","        premise_mask=None,\n","        hyp_mask=None,\n","        is_training=True,\n","        task_id=None\n","    ):\n","        input_ids = input_ids.view(-1, input_ids.size(-1))\n","        token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n","        attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n","\n","        outputs = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False, head_mask=head_mask)\n","        if self.output_attentions:\n","            all_attentions, sequence_output, pooled_output = outputs\n","        else:\n","            sequence_output, pooled_output = outputs\n","        # pooled_output = self.dropout(pooled_output)\n","        # logits = self.classifier(pooled_output)\n","        # reshaped_logits = logits.view(-1, self.num_choices)\n","\n","        # SAN module\n","        if self.use_SAN:\n","            premise_mask = premise_mask.view(-1, premise_mask.size(-1)) if premise_mask is not None else None\n","            hyp_mask = hyp_mask.view(-1, hyp_mask.size(-1)) if hyp_mask is not None else None\n","            max_query = hyp_mask.size(1)\n","            hyp_mem = sequence_output[:, :max_query]\n","            premise_mem, premise_mask = masked_select(sequence_output, premise_mask)\n","            logits = self.out_proj[task_id](premise_mem, hyp_mem, premise_mask, hyp_mask, is_training=is_training)\n","        else:\n","            pooled_output = self.dropout(pooled_output)\n","            logits = self.classifier[task_id](pooled_output)\n","\n","        if labels is not None:\n","            if self.opt.get('answer_mem_type', 0):\n","                loss_fct = NLLLoss()\n","            else:\n","                loss_fct = CrossEntropyLoss()\n","                logits = logits.view(-1, self.num_choices[task_id])\n","            labels = labels.view(-1)\n","            loss = loss_fct(logits, labels)\n","            return loss, logits\n","        elif self.output_attentions:\n","            return all_attentions, logits\n","        return logits"]},{"cell_type":"markdown","metadata":{"id":"uWwt6WujlC1Q"},"source":["# Utils"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"HZxTWqhmlCaO","executionInfo":{"status":"ok","timestamp":1676640233710,"user_tz":-420,"elapsed":9,"user":{"displayName":"Trọng Khôi Hoàng","userId":"12933784847383051959"}}},"outputs":[],"source":["import csv\n","import glob\n","import json\n","import logging\n","import os\n","from typing import List\n","\n","import numpy as np\n","from tqdm.auto import tqdm, trange\n","from transformers import PreTrainedTokenizer\n","\n","logger = logging.getLogger(__name__)\n","\n","\n","class InputExample(object):\n","    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n","\n","    def __init__(self, guid, text_a, text_b=None, label=None, text_c=None):\n","        \"\"\"Constructs a InputExample.\n","        Args:\n","            guid: Unique id for the example.\n","            text_a: string. The untokenized text of the first sequence. For single sequence tasks, only this sequence must be specified.\n","            text_b: (Optional) string. The untokenized text of the second sequence. Only must be specified for sequence pair tasks.\n","            label: (Optional) string. The label of the example. This should be specified for train and dev examples, but not for test examples.\n","        \"\"\"\n","        self.guid = guid\n","        self.text_a = text_a\n","        self.text_b = text_b\n","        self.text_c = text_c\n","        self.label = label\n","\n","\n","class InputFeatures(object):\n","    \"\"\"A single set of features of data.\"\"\"\n","\n","    def __init__(self, guid, input_ids, input_mask, segment_ids, label_id, premise_mask, hypothesis_mask):\n","        self.guid = guid\n","        self.input_ids = input_ids\n","        self.input_mask = input_mask\n","        self.segment_ids = segment_ids\n","        self.label_id = label_id\n","        self.premise_mask = premise_mask\n","        self.hypothesis_mask = hypothesis_mask\n","\n","\n","class DataProcessor(object):\n","    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n","\n","    def get_train_examples(self, data_dir):\n","        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n","        raise NotImplementedError()\n","\n","    def get_dev_examples(self, data_dir):\n","        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n","        raise NotImplementedError()\n","\n","    def get_labels(self):\n","        \"\"\"Gets the list of labels for this data set.\"\"\"\n","        raise NotImplementedError()\n","\n","    @classmethod\n","    def _read_tsv(cls, input_file, quotechar=None):\n","        \"\"\"Reads a tab separated value file.\"\"\"\n","        with open(input_file, \"r\") as f:\n","            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n","            lines = []\n","            for line in reader:\n","                lines.append(line)\n","            return lines\n","\n","    @classmethod\n","    def _read_json(cls, input_file):\n","        \"\"\"Reads a json file.\"\"\"\n","        with open(input_file, \"r\", encoding='utf-8') as fpr:\n","            raw_list = json.load(fpr)\n","            return raw_list\n","\n","    @classmethod\n","    def _read_jsonl(cls, input_file):\n","        \"\"\"Reads a json file.\"\"\"\n","        with open(input_file, \"r\", encoding='utf-8') as fpr:\n","            raw_list = list()\n","            for json_str in list(fpr):\n","                raw_list.append(json.loads(json_str))\n","            return raw_list\n","\n","\n","class VimmrcProcessor(DataProcessor):\n","    \"\"\"Processor for the MultiNLI data set (GLUE version).\"\"\"\n","    def get_train_examples(self, data_dir):\n","        \"\"\"See base class.\"\"\"\n","        return self._read_samples(data_dir, \"train\")\n","\n","    def get_dev_examples(self, data_dir):\n","        \"\"\"See base class.\"\"\"\n","        return self._read_samples(data_dir, \"dev\")\n","        \n","    def get_test_examples(self, data_dir):\n","        \"\"\"See base class.\"\"\"\n","        return self._read_samples(data_dir, \"test\")\n","\n","    def get_labels(self):\n","        \"\"\"See base class.\"\"\"\n","        return [\"A\", \"B\", \"C\", \"D\"]\n","\n","    def _read_samples(self, data_dir, set_type):\n","        \"\"\"Creates examples for the training and dev sets.\"\"\"\n","        # Init reader\n","        examples = []\n","        #example_id = 0\n","        # with open(filename, 'r', encoding='utf-8') as fpr:\n","        #     raw_list = json.load(fpr)\n","        raw_list = self._read_json(os.path.join(data_dir, set_type + \".json\"))\n","\n","        for data_raw in raw_list:\n","            # data_raw = json.load(fpr)\n","            article = data_raw['content']\n","            example_id = 0\n","            title = '_'.join(data_raw['files'].split('/')[-1].split('_')[:-1])\n","            for i in range(len(data_raw['questions'])):\n","                example_id += 1\n","                #truth = str(ord(data_raw['questions'][i]['answer']) - ord('A'))\n","                truth = data_raw['questions'][i]['answer']\n","                question = data_raw['questions'][i]['question']\n","                options = data_raw['questions'][i]['options']\n","                for k in range(len(options)):\n","                    guid = \"%s-%s-%s-%s\" % (set_type, title, example_id, k)\n","                    option = list(options[k].values())[0]\n","                    examples.append(\n","                        InputExample(guid=guid, text_a=article, text_b=option, label=truth, text_c=question))\n","\n","        return examples\n","\n","\n","class VinliProcessor(DataProcessor):\n","    \"\"\"Processor for the MultiNLI data set (GLUE version).\"\"\"\n","    def get_train_examples(self, data_dir):\n","        \"\"\"See base class.\"\"\"\n","        return self._read_samples(data_dir, \"train\")\n","\n","    def get_dev_examples(self, data_dir):\n","        \"\"\"See base class.\"\"\"\n","        return self._read_samples(data_dir, \"dev\")\n","        \n","    def get_test_examples(self, data_dir):\n","        \"\"\"See base class.\"\"\"\n","        return self._read_samples(data_dir, \"test\")\n","\n","    def get_labels(self):\n","        \"\"\"See base class.\"\"\"\n","        return [\"contradiction\", \"entailment\", \"neutral\"]\n","\n","    def _read_samples(self, data_dir, set_type):\n","        \"\"\"Creates examples for the training and dev sets.\"\"\"\n","        # Init reader\n","        examples = []\n","        raw_list = self._read_jsonl(os.path.join(data_dir, set_type + \".jsonl\"))\n","        for data_line in raw_list:\n","            guid = \"%s-%s\" % (set_type, data_line['pairID'])\n","            text_a = data_line['sentence1']\n","            text_b = data_line['sentence2']\n","            label = data_line['gold_label']\n","            if label in self.get_labels():\n","                examples.append(\n","                    InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n","\n","        return examples\n","\n","\n","class RaceProcessor(DataProcessor):\n","    \"\"\"Processor for the RACE data set.\"\"\"\n","\n","    def get_train_examples(self, data_dir):\n","        \"\"\"See base class.\"\"\"\n","        logger.info(\"LOOKING AT {} train\".format(data_dir))\n","        high = os.path.join(data_dir, \"train_race.json\")\n","        # middle = os.path.join(data_dir, \"train/middle\")\n","        high = self._read_txt(high)\n","        # middle = self._read_txt(middle)\n","        return self._create_examples(high, \"train\")\n","\n","    def get_dev_examples(self, data_dir):\n","        \"\"\"See base class.\"\"\"\n","        logger.info(\"LOOKING AT {} dev\".format(data_dir))\n","        high = os.path.join(data_dir, \"dev_race.json\")\n","        # middle = os.path.join(data_dir, \"dev/middle\")\n","        high = self._read_txt(high)\n","        # middle = self._read_txt(middle)\n","        return self._create_examples(high, \"dev\")\n","\n","    def get_test_examples(self, data_dir):\n","        \"\"\"See base class.\"\"\"\n","        logger.info(\"LOOKING AT {} test\".format(data_dir))\n","        high = os.path.join(data_dir, \"test_race.json\")\n","        # middle = os.path.join(data_dir, \"test/middle\")\n","        high = self._read_txt(high)\n","        # middle = self._read_txt(middle)\n","        return self._create_examples(high, \"test\")\n","\n","    def get_labels(self):\n","        \"\"\"See base class.\"\"\"\n","        return [\"A\", \"B\", \"C\", \"D\"]\n","\n","    def _read_txt(self, input_dir):\n","        lines = []\n","        with open(input_dir, \"r\", encoding=\"utf-8\") as fin:\n","            data_raw = json.load(fin)\n","        for d in data_raw:\n","            d['race_id'] = '_'.join(d['files'].split('/')[-1].split('_')[:-1])\n","            lines.append(d)\n","        return lines\n","\n","    def _create_examples(self, lines, set_type):\n","        \"\"\"Creates examples for the training and dev sets.\"\"\"\n","        examples = []\n","        for (_, data_raw) in enumerate(lines):\n","            race_id = \"%s-%s\" % (set_type, data_raw[\"race_id\"])\n","            article = data_raw[\"article\"]\n","            for i in range(len(data_raw[\"answers\"])):\n","                truth = data_raw[\"answers\"][i]\n","                question = data_raw[\"questions\"][i]\n","                options = data_raw[\"options\"][i]\n","                for k in range(len(options)):\n","                    guid = \"%s-%s\" % (race_id, k)\n","                    examples.append(\n","                        InputExample(\n","                            guid=guid,\n","                            text_a=article,\n","                            text_b=options[k], #list(options[k].values())[0]\n","                            label=truth,\n","                            text_c=question\n","                        )\n","                    )\n","\n","                # examples.append(\n","                #     InputExample(\n","                #         example_id=race_id,\n","                #         question=question,\n","                #         contexts=[\n","                #             article,\n","                #             article,\n","                #             article,\n","                #             article,\n","                #         ],  # this is not efficient but convenient\n","                #         endings=[options[0], options[1], options[2], options[3]],\n","                #         label=truth,\n","                #     )\n","                # )\n","        return examples\n","\n","\n","class InfiniteDataLoader:\n","    def __init__(self, data_loader):\n","        self.data_loader = data_loader\n","        self.data_iter = iter(data_loader)\n","\n","    def get_next(self):\n","        try:\n","            data = next(self.data_iter)\n","        except StopIteration:\n","            # StopIteration is thrown if dataset ends\n","            # reinitialize data loader\n","            self.data_iter = iter(self.data_loader)\n","            data = next(self.data_iter)\n","        return data\n","\n","\n","def convert_examples_to_features(\n","    examples: List[InputExample],\n","    label_list: List[str],\n","    max_seq_length: int,\n","    tokenizer,\n","    n_class,\n","    do_lower_case,\n","    output_mode,\n","    set_type,\n","    is_multi_choice=True\n",") -> List[InputFeatures]:\n","    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n","\n","    label_map = {label: i for i, label in enumerate(label_list)}\n","\n","    if is_multi_choice:\n","        features = [[]]\n","    else:\n","        features = []\n","\n","    feature_iterator = tqdm(examples, desc=\"Convert examples to features\")\n","    for (ex_index, example) in enumerate(feature_iterator):\n","        feature_iterator.set_description(\"Convert %d of %d example to features\" % (ex_index, len(examples)))\n","\n","        tokens_a = tokenizer.tokenize(example.text_a.lower() if do_lower_case else example.text_a)  # dialogues\n","\n","        tokens_b = [] # None\n","        tokens_c = [] # None\n","\n","        if example.text_b:\n","            tokens_b = tokenizer.tokenize(example.text_b.lower() if do_lower_case else example.text_b)  # answers\n","\n","        if example.text_c:\n","            tokens_c = tokenizer.tokenize(example.text_c.lower() if do_lower_case else example.text_c)  # questions\n","\n","        if tokens_c:\n","            _truncate_seq_tuple(tokens_a, tokens_b, tokens_c, max_seq_length - 4)\n","            tokens_b = tokens_c + [\"[SEP]\"] + tokens_b\n","        elif tokens_b:\n","            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n","        else:\n","            if len(tokens_a) > max_seq_length - 2:\n","                tokens_a = tokens_a[0:(max_seq_length - 2)]\n","\n","        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n","        segment_ids = (len(tokens_a) + 2) * [0]\n","        premise_mask = (len(tokens_a) + 2) * [1]\n","        hypothesis_mask = (len(tokens_a) + 2) * [0]\n","\n","        if tokens_b:\n","            tokens += tokens_b + [\"[SEP]\"]\n","            segment_ids += [1] * (len(tokens_b) + 1)\n","            premise_mask += [1] * (len(tokens_c) + 1)\n","            premise_mask += [0] * (len(tokens_b) - len(tokens_c))\n","            hypothesis_mask += [0] * (len(tokens_c) + 1)\n","            hypothesis_mask += [1] * (len(tokens_b) - len(tokens_c))\n","\n","        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n","\n","        # The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.\n","        input_mask = [1] * len(input_ids)\n","\n","        # Zero-pad up to the sequence length.\n","        pad_length = max_seq_length - len(input_ids)\n","        input_ids += [0] * pad_length\n","        input_mask += [0] * pad_length\n","        segment_ids += [0] * pad_length\n","        premise_mask += [0] * pad_length\n","        hypothesis_mask += [0] * pad_length\n","\n","        assert len(input_ids) == max_seq_length\n","        assert len(input_mask) == max_seq_length\n","        assert len(segment_ids) == max_seq_length\n","        assert len(premise_mask) == max_seq_length\n","        assert len(hypothesis_mask) == max_seq_length\n","\n","        if output_mode in [\"classification\", \"multi-choice\"]:\n","            label_id = label_map[example.label]\n","        elif output_mode == \"regression\":\n","            label_id = float(example.label)\n","        else:\n","            raise KeyError(output_mode)\n","\n","        if is_multi_choice:\n","            features[-1].append(\n","                InputFeatures(\n","                    guid=example.guid,\n","                    input_ids=input_ids,\n","                    input_mask=input_mask,\n","                    segment_ids=segment_ids,\n","                    label_id=label_id,\n","                    premise_mask = premise_mask,\n","                    hypothesis_mask = hypothesis_mask))\n","            if len(features[-1]) == n_class:\n","                features.append([])\n","        else:\n","            features.append(\n","                InputFeatures(\n","                    guid=example.guid,\n","                    input_ids=input_ids,\n","                    input_mask=input_mask,\n","                    segment_ids=segment_ids,\n","                    label_id=label_id,\n","                    premise_mask = premise_mask,\n","                    hypothesis_mask = hypothesis_mask))\n","            \n","        ## display some example\n","        if set_type == 'train' and ex_index < 4:\n","            logger.info(\"*** Example ***\")\n","            logger.info(f\"guid: {example.guid}\")\n","            logger.info(f\"tokens: {' '.join(tokens)}\")\n","            logger.info(f\"input_ids: {' '.join(map(str, input_ids))}\")\n","            logger.info(f\"input_mask: {' '.join(map(str, input_mask))}\")\n","            logger.info(f\"segment_ids: {' '.join(map(str, segment_ids))}\")\n","            logger.info(f\"premis_mask: {' '.join(map(str, premise_mask))}\")\n","            logger.info(f\"hypoth_mask: {' '.join(map(str, hypothesis_mask))}\")\n","            try:\n","                logger.info(f\"label: {example.label}\")\n","            except:\n","                pass\n","\n","    if is_multi_choice:\n","        if len(features[-1]) == 0:\n","            features = features[:-1]\n","\n","    return features\n","\n","\n","def convert_features_to_tensors(features, output_mode, is_multi_choice=True):\n","\n","    input_ids = []\n","    input_mask = []\n","    segment_ids = []\n","    label_id = []\n","    premise_mask = []\n","    hypothesis_mask = []\n","\n","    if is_multi_choice:\n","        n_class = len(features[0])\n","        for f in features:\n","            input_ids.append([])\n","            input_mask.append([])\n","            segment_ids.append([])\n","            premise_mask.append([])\n","            hypothesis_mask.append([])\n","            for i in range(n_class):\n","                input_ids[-1].append(f[i].input_ids)\n","                input_mask[-1].append(f[i].input_mask)\n","                segment_ids[-1].append(f[i].segment_ids)\n","                premise_mask[-1].append(f[i].premise_mask)\n","                hypothesis_mask[-1].append(f[i].hypothesis_mask)\n","\n","            label_id.append([f[0].label_id])\n","    else:\n","        for f in features:\n","            input_ids.append(f.input_ids)\n","            input_mask.append(f.input_mask)\n","            segment_ids.append(f.segment_ids)\n","            label_id.append(f.label_id)\n","            premise_mask.append(f.premise_mask)\n","            hypothesis_mask.append(f.hypothesis_mask)\n","\n","    all_input_ids = torch.tensor(input_ids, dtype=torch.long)\n","    all_input_mask = torch.tensor(input_mask, dtype=torch.long)\n","    all_segment_ids = torch.tensor(segment_ids, dtype=torch.long)\n","    all_premise_mask = torch.tensor(premise_mask, dtype=torch.bool)\n","    all_hypothesis_mask = torch.tensor(hypothesis_mask, dtype=torch.bool)\n","\n","    if output_mode in [\"classification\", \"multi-choice\"]:\n","        all_label_ids = torch.tensor(label_id, dtype=torch.long)\n","    elif output_mode == \"regression\":\n","        all_label_ids = torch.tensor(label_id, dtype=torch.float)\n","\n","    data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_premise_mask, all_hypothesis_mask, all_label_ids)\n","\n","    return data\n","\n","\n","def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n","    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n","\n","    # This is a simple heuristic which will always truncate the longer sequence\n","    # one token at a time. This makes more sense than truncating an equal percent\n","    # of tokens from each, since if one sequence is very short then each token\n","    # that's truncated likely contains more information than a longer sequence.\n","    while True:\n","        total_length = len(tokens_a) + len(tokens_b)\n","        if total_length <= max_length:\n","            break\n","        if len(tokens_a) > len(tokens_b):\n","            tokens_a.pop()\n","        else:\n","            tokens_b.pop()\n","\n","\n","def _truncate_seq_tuple(tokens_a, tokens_b, tokens_c, max_length):\n","    \"\"\"Truncates a sequence tuple in place to the maximum length.\"\"\"\n","\n","    # This is a simple heuristic which will always truncate the longer sequence\n","    # one token at a time. This makes more sense than truncating an equal percent\n","    # of tokens from each, since if one sequence is very short then each token\n","    # that's truncated likely contains more information than a longer sequence.\n","    while True:\n","        total_length = len(tokens_a) + len(tokens_b) + len(tokens_c)\n","        if total_length <= max_length:\n","            break\n","        if len(tokens_a) >= len(tokens_b) and len(tokens_a) >= len(tokens_c):\n","            tokens_a.pop()\n","        elif len(tokens_b) >= len(tokens_a) and len(tokens_b) >= len(tokens_c):\n","            tokens_b.pop()\n","        else:\n","            tokens_c.pop()\n","\n","\n","processors = {\n","    \"vimmrc\": VimmrcProcessor,\n","    \"vinli\": VinliProcessor,\n","    \"vimmrc_race\": RaceProcessor,\n","}\n","\n","output_modes = {\n","    \"vimmrc\": 'multi-choice',\n","    \"vimmrc_race\": 'multi-choice',\n","    \"vinli\": \"classification\",\n","}\n","\n","GLUE_TASKS_NUM_LABELS = { \"vimmrc\": 4, \"vimmrc_race\": 4, \"vinli\": 3 }\n","\n","MAX_SEQ_LENGTHS = { \"vimmrc\": 512, \"vimmrc_race\": 512, \"vinli\": 128 }"]},{"cell_type":"markdown","metadata":{"id":"yoiGjVs5kuwD"},"source":["# Module"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"VG-MZ9JcbuBo","executionInfo":{"status":"ok","timestamp":1676640233710,"user_tz":-420,"elapsed":8,"user":{"displayName":"Trọng Khôi Hoàng","userId":"12933784847383051959"}}},"outputs":[],"source":["import argparse\n","import glob\n","import json\n","import logging\n","import os\n","import random\n","\n","import numpy as np\n","import torch\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n","from torch.utils.data.distributed import DistributedSampler\n","#from torch.optim import AdamW\n","from tqdm.auto import tqdm, trange\n","from transformers import (\n","    WEIGHTS_NAME, CONFIG_NAME,\n","    AdamW,\n","    BertConfig,\n","    BertForMultipleChoice,\n","    BertTokenizer,\n","    RobertaConfig,\n","    RobertaForMultipleChoice,\n","    RobertaTokenizer,\n","    XLMRobertaConfig,\n","    XLMRobertaForMultipleChoice,\n","    XLMRobertaTokenizer,\n","    XLNetConfig,\n","    XLNetForMultipleChoice,\n","    XLNetTokenizer,\n","    get_linear_schedule_with_warmup,\n",")\n","\n","from transformers import BERT_PRETRAINED_CONFIG_ARCHIVE_MAP, DISTILBERT_PRETRAINED_CONFIG_ARCHIVE_MAP, XLM_ROBERTA_PRETRAINED_CONFIG_ARCHIVE_MAP\n","from transformers import DistilBertConfig, DistilBertForMultipleChoice, DistilBertTokenizer\n","\n","# try:\n","#     from torch.utils.tensorboard import SummaryWriter\n","# except ImportError:\n","#     from tensorboardX import SummaryWriter\n","\n","logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s', datefmt='%m/%d/%Y %H:%M:%S', level=logging.INFO)\n","logger = logging.getLogger(__name__)\n","logger.setLevel(logging.INFO)\n","\n","ALL_MODELS = tuple(DISTILBERT_PRETRAINED_CONFIG_ARCHIVE_MAP.keys()) + tuple(BERT_PRETRAINED_CONFIG_ARCHIVE_MAP.keys()) + tuple(XLM_ROBERTA_PRETRAINED_CONFIG_ARCHIVE_MAP.keys())\n","\n","MODEL_CLASSES = {\n","    \"bert-man\": (BertConfig, BertForMultipleChoice_SAN, BertTokenizer),\n","    #\"roberta\": (RobertaConfig, RobertaForMultipleChoice, RobertaTokenizer),\n","    #\"xlm-roberta\": (XLMRobertaConfig, XLMRobertaForMultipleChoice, XLMRobertaTokenizer)\n","}"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"9Iow6D0wkEez","executionInfo":{"status":"ok","timestamp":1676640233710,"user_tz":-420,"elapsed":7,"user":{"displayName":"Trọng Khôi Hoàng","userId":"12933784847383051959"}}},"outputs":[],"source":["def select_field(features, field):\n","    return [[choice[field] for choice in feature.choices_features] for feature in features]\n","\n","\n","def simple_accuracy(preds, labels):\n","    return (preds == labels).mean()\n","\n","\n","def set_seed(args):\n","    random.seed(args.seed)\n","    np.random.seed(args.seed)\n","    torch.manual_seed(args.seed)\n","    if args.n_gpu > 0:\n","        torch.cuda.manual_seed_all(args.seed)\n","\n","\n","def load_and_cache_examples(args, task, tokenizer, set_type='train'):\n","    if args.local_rank not in [-1, 0]:\n","        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n","    \n","    output_mode = output_modes[task]\n","    is_multi_choice = True if output_mode == 'multi-choice' else False\n","    processor = processors[task]()\n","    # Load data features from cache or dataset file\n","    cached_features_file = os.path.join(\n","        args.data_dir,\n","        'cached_{}_{}_{}_{}'.format(\n","            set_type,\n","            list(filter(None, args.model_name_or_path.split('/'))).pop(),\n","            str(MAX_SEQ_LENGTHS[task]),\n","            str(task)\n","        )\n","    )\n","    if os.path.exists(cached_features_file):\n","        logger.info(\"Loading features from cached file %s\", cached_features_file)\n","        features = torch.load(cached_features_file)\n","    else:\n","        logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n","        label_list = processor.get_labels()\n","        if set_type == 'train':\n","            examples = processor.get_train_examples(args.data_dir)\n","        elif set_type == 'dev':\n","            examples = processor.get_dev_examples(args.data_dir)\n","        else:\n","            examples = processor.get_test_examples(args.data_dir)\n","        logger.info(\"Training number: %s\", str(len(examples)))\n","        features = convert_examples_to_features(\n","            examples,\n","            label_list,\n","            MAX_SEQ_LENGTHS[task],\n","            tokenizer,\n","            len(label_list),\n","            output_mode=output_mode,\n","            set_type=set_type,\n","            do_lower_case=args.do_lower_case,\n","            is_multi_choice=is_multi_choice\n","        )\n","        if args.local_rank in [-1, 0]:\n","            logger.info(\"Saving features into cached file %s\", cached_features_file)\n","            torch.save(features, cached_features_file)\n","\n","    # Convert to Tensors and build dataset\n","    dataset = convert_features_to_tensors(\n","        features, output_mode, is_multi_choice=is_multi_choice\n","    )\n","\n","    if set_type == 'dev' or set_type == 'test':\n","        all_example_ids = [x.guid for feature_set in features for x in feature_set]#[x.example_id for x in features]\n","        return dataset, all_example_ids\n","    else:\n","        return dataset"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"2gdQTXe8k1Cf","executionInfo":{"status":"ok","timestamp":1676640233711,"user_tz":-420,"elapsed":7,"user":{"displayName":"Trọng Khôi Hoàng","userId":"12933784847383051959"}}},"outputs":[],"source":["def train(args, train_datasets, model, tokenizer):\n","    \"\"\" Train the model \"\"\"\n","    # if args.local_rank in [-1, 0]:\n","    #     tb_writer = SummaryWriter()\n","    #     pass\n","\n","    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n","    train_iters = []\n","    tr_batches = []\n","    #for idx, train_dataset in enumerate(train_datasets):\n","    train_sampler = RandomSampler(train_datasets) if args.local_rank == -1 else DistributedSampler(train_datasets)\n","    train_dataloader = DataLoader(train_datasets, sampler=train_sampler, batch_size=args.train_batch_size)\n","    train_iters.append(InfiniteDataLoader(train_dataloader))\n","    tr_batches.append(len(train_dataloader))\n","\n","    ## set sampling proportion\n","    total_n_tr_batches = sum(tr_batches)\n","    sampling_prob = [float(n_batches) / total_n_tr_batches for n_batches in tr_batches]\n","    t_total = total_n_tr_batches // args.gradient_accumulation_steps * args.num_train_epochs\n","\n","    # Prepare optimizer and schedule (linear warmup and decay)\n","    no_decay = ['bias', 'LayerNorm.weight']\n","    optimizer_grouped_parameters = [\n","        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","         'weight_decay': args.weight_decay},\n","        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n","         'weight_decay': 0.0}\n","    ]\n","\n","    # optimizer = BertAdam(\n","    #     optimizer_grouped_parameters,\n","    #     lr=args.learning_rate,\n","    #     warmup=args.warmup_proportion,\n","    #     max_grad_norm=args.max_grad_norm, t_total=t_total)\n","    \n","    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, correct_bias=False, no_deprecation_warning=True) # To reproduce BertAdam specific behavior set correct_bias=False\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer,\n","        num_warmup_steps=int(args.warmup_proportion * t_total),\n","        num_training_steps=t_total,\n","    ) # PyTorch scheduler\n","\n","    # Train!\n","    logger.info(\"***** Running training *****\")\n","    logger.info(\"  Num examples = %d\", len(train_datasets))\n","    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n","    logger.info(\"  Instantaneous batch size per GPU = %s\", args.per_gpu_train_batch_size)\n","    logger.info(\n","        \" Total train batch size (w. parallel, distributed & accumulation) = %d\",\n","        args.train_batch_size\n","        * args.gradient_accumulation_steps\n","        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n","    )\n","    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n","    logger.info(\"  Total optimization steps = %d\", t_total)\n","\n","    global_step = 0\n","    tr_loss, logging_loss, tmp_loss = 0.0, 0.0, 0.0\n","    best_dev_acc = 0.0\n","    best_steps = 0\n","    nb_tr_examples = 0\n","    model.zero_grad()\n","    train_iterator = tqdm(range(int(args.num_train_epochs)), desc=\"Epoch\") #, disable=args.local_rank not in [-1, 0])\n","    set_seed(args)  # Added here for reproductibility (even between python 2 and 3)\n","    for epoch, _ in enumerate(train_iterator):\n","        epoch_iterator = tqdm(range(total_n_tr_batches), desc=\"Iteration\") #, disable=args.local_rank not in [-1, 0])\n","        for step, _ in enumerate(epoch_iterator):\n","            epoch_iterator.set_description(\n","                \"train loss: {}\".format(tr_loss / nb_tr_examples if nb_tr_examples else tr_loss)\n","            )\n","            model.train()\n","            # select task id\n","            task_id = np.argmax(np.random.multinomial(1, sampling_prob))\n","            batch = train_iters[task_id].get_next()\n","            batch = tuple(t.to(args.device) for t in batch)\n","            inputs = {'input_ids':      batch[0],\n","                      'attention_mask': batch[1],\n","                      'token_type_ids': batch[2],\n","                      'premise_mask':   batch[3],\n","                      'hyp_mask':       batch[4],\n","                      'labels':         batch[5],\n","                      'task_id':        task_id}\n","            outputs = model(**inputs)  # model outputs are always tuple in transformers (see doc)\n","            loss = outputs[0]\n","\n","            if args.n_gpu > 1:\n","                loss = loss.mean() # mean() to average on multi-gpu parallel training\n","            if args.gradient_accumulation_steps > 1:\n","                loss = loss / args.gradient_accumulation_steps\n","\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n","\n","            tr_loss += loss.item()\n","            nb_tr_examples += inputs['input_ids'].size(0)\n","            if (step + 1) % args.gradient_accumulation_steps == 0:\n","                optimizer.step()\n","                scheduler.step()  # Update learning rate schedule\n","                model.zero_grad()\n","                global_step += 1\n","                #tb_writer.add_scalar(\"train_{}\".format(\"loss_\"), tr_loss - tmp_loss, global_step)\n","                tmp_loss = tr_loss\n","\n","        # Save model checkpoint\n","        # if args.do_epoch_checkpoint:\n","        epoch_output_dir = os.path.join(args.output_dir, 'epoch_{}'.format(epoch+1))\n","        os.makedirs(epoch_output_dir, exist_ok=True)\n","        output_model_file = os.path.join(epoch_output_dir, WEIGHTS_NAME)\n","        output_config_file = os.path.join(epoch_output_dir, CONFIG_NAME)\n","        model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n","        torch.save(model_to_save.state_dict(), output_model_file)\n","        model_to_save.config.to_json_file(output_config_file)\n","        tokenizer.save_vocabulary(epoch_output_dir)\n","\n","        # evaluate(args, model, tokenizer, epoch=epoch, is_test=False)\n","        # evaluate(args, model, tokenizer, epoch=epoch, is_test=True)\n","    # if args.local_rank in [-1, 0]:\n","    #     tb_writer.close()\n","\n","    return global_step, tr_loss / global_step, best_steps"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"hUI-m6tlk3iR","executionInfo":{"status":"ok","timestamp":1676640233711,"user_tz":-420,"elapsed":7,"user":{"displayName":"Trọng Khôi Hoàng","userId":"12933784847383051959"}}},"outputs":[],"source":["def evaluate(args, model, tokenizer, checkpoint='', is_test=False, error=False):\n","    # Loop to handle MNLI double evaluation (matched, mis-matched)\n","    eval_task_names = args.task_name\n","    eval_output_dir = args.output_dir\n","\n","    set_type = 'test' if is_test else 'dev'\n","    results = {}\n","    #for task_id, eval_task in enumerate(eval_task_names):\n","    #for eval_task, eval_output_dir in zip(eval_task_names, eval_outputs_dirs):\n","    eval_dataset, all_example_ids = load_and_cache_examples(args, eval_task_names, tokenizer, set_type)\n","    \n","    if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n","        os.makedirs(eval_output_dir)\n","\n","    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n","    # Note that DistributedSampler samples randomly\n","    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n","    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n","\n","    # Eval!\n","    logger.info(\"***** Running evaluation for {} on {} for {} *****\".format(eval_task_names, set_type, checkpoint))\n","    logger.info(\"  Num examples = %d\", len(eval_dataset))\n","    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n","    eval_loss = 0.0\n","    nb_eval_steps = 0\n","    logits_all = None\n","    out_label_ids = None\n","    preds_softmax = None\n","    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n","        model.eval()\n","        batch = tuple(t.to(args.device) for t in batch)\n","\n","        with torch.no_grad():\n","            inputs = {'input_ids':      batch[0],\n","                      'attention_mask': batch[1],\n","                      'token_type_ids': batch[2],\n","                      'premise_mask':   batch[3],\n","                      'hyp_mask':       batch[4],\n","                      'labels':         batch[5],\n","                      'task_id':        0}\n","            outputs = model(**inputs)\n","            tmp_eval_loss, logits = outputs[:2]\n","            # input_ids, input_mask, segment_ids, label_ids = batch\n","            # tmp_eval_loss, logits = model(input_ids, segment_ids, input_mask, label_ids, task_id=task_id)\n","            eval_loss += tmp_eval_loss.mean().item()\n","        nb_eval_steps += 1\n","        if logits_all is None:\n","            logits_all = logits.detach().cpu().numpy()\n","            preds_softmax = torch.softmax(logits, -1).detach().cpu().numpy()\n","            out_label_ids = inputs['labels'].detach().cpu().numpy()\n","        else:\n","            logits_all = np.append(logits_all, logits.detach().cpu().numpy(), axis=0)\n","            preds_softmax = np.append(preds_softmax, torch.softmax(logits, -1).detach().cpu().numpy(), axis=0)\n","            out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n","\n","    eval_loss = eval_loss / nb_eval_steps\n","    preds_softmax = [[round(x.tolist(), 4) for x in p] for p in preds_softmax]\n","    example_ids = [id[:-2] for id in all_example_ids[::4]]\n","    per_id_pred = dict(zip(example_ids, preds_softmax))\n","    output_mode = output_modes[eval_task_names]\n","\n","    preds = np.argmax(logits_all, axis=1)\n","    acc = simple_accuracy(preds, out_label_ids.reshape(-1))\n","    #result = compute_metrics(eval_task, preds, out_label_ids.reshape(-1))\n","    result = {\"eval_acc\": acc, \"eval_loss\": eval_loss}\n","    results.update(result)\n","\n","    output_eval_file = os.path.join(eval_output_dir, \"eval_results_{}_{}.txt\".format(eval_task_names, set_type))\n","    with open(output_eval_file, \"a\") as writer:\n","        logger.info(\"***** Eval results for {} on {} for {} *****\".format(eval_task_names, set_type, checkpoint))\n","        writer.write(\"***** Eval results for {} *****\\n\".format(checkpoint))\n","        writer.write(\n","            \"total batch size=%d\\n\"\n","            % (\n","                args.per_gpu_train_batch_size\n","                * args.gradient_accumulation_steps\n","                * (torch.distributed.get_world_size() if args.local_rank != -1 else 1)\n","            )\n","        )\n","        writer.write(\"train num epochs=%d\\n\" % args.num_train_epochs)\n","        writer.write(\"max seq length  =%d\\n\" % MAX_SEQ_LENGTHS[eval_task_names])#args.max_seq_length)\n","        for key in sorted(result.keys()):\n","            logger.info(\"  %s = %s\", key, str(result[key]))\n","            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n","        logger.info(\"\\n\")\n","    \n","    # Write pred\n","    output_pred_file = os.path.join(eval_output_dir, \"predictions_\" + str(is_test).lower() + \"_eval_results.json\")\n","    with open(output_pred_file, \"w\") as writer:\n","        json.dump(per_id_pred, writer, indent=4)\n","        logger.info(\"Saved {0}\".format(output_pred_file))\n","\n","    # Write pred with label\n","    processor = processors[args.task_name]()\n","    label_list = processor.get_labels()#['A','B','C','D']\n","    idx2label = {i: label for i, label in enumerate(label_list)}\n","    output_pred_file = os.path.join(eval_output_dir, \"predictions_\" + str(is_test).lower() + \"_eval_results_label.json\")\n","    with open(output_pred_file, \"w\") as writer:\n","        json.dump({'pred': [idx2label[id[0]] for id in out_label_ids.tolist()]}, writer, indent=4)\n","        logger.info(\"Saved {0}\".format(output_pred_file))\n","\n","    # Get error idx\n","    if error:\n","        correct_idx = np.argwhere(preds == out_label_ids).tolist()\n","        wrong_idx = np.argwhere(preds != out_label_ids).tolist()\n","        wrong_idx_dict = {\n","            'correct': correct_idx, 'wrong': wrong_idx,\n","            'preds': preds.tolist(), 'logits': logits_all.tolist(),\n","            'labels': out_label_ids.tolist()\n","        }\n","        output_error_file = os.path.join(eval_output_dir,\"error_idx_{}_{}.json\".format(eval_task_names, set_type))\n","        with open(output_error_file, \"w\") as writer:\n","            json.dump(wrong_idx_dict, writer)\n","            logger.info(\"Saved {0}\".format(output_error_file))\n","\n","    return results"]},{"cell_type":"markdown","metadata":{"id":"fyV2VX7Vk1RA"},"source":["# Main"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"daHi3mJlTi69","executionInfo":{"status":"ok","timestamp":1676640233711,"user_tz":-420,"elapsed":6,"user":{"displayName":"Trọng Khôi Hoàng","userId":"12933784847383051959"}}},"outputs":[],"source":["PRJ_DIR = '/content/drive/MyDrive/NCKH/ViMMRC_model'\n","OUTPUT_DIR = PRJ_DIR + '/models/mbert_man_v1'"]},{"cell_type":"code","source":["def main(args):\n","\n","    # Setup distant debugging if needed\n","    # if args.server_ip and args.server_port:\n","    #     # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n","    #     import ptvsd\n","\n","    #     print(\"Waiting for debugger attach\")\n","    #     ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n","    #     ptvsd.wait_for_attach()\n","\n","    # Setup CUDA, GPU & distributed training\n","    if args.local_rank == -1 or args.no_cuda:\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n","        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n","    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n","        torch.cuda.set_device(args.local_rank)\n","        device = torch.device(\"cuda\", args.local_rank)\n","        torch.distributed.init_process_group(backend=\"nccl\")\n","        args.n_gpu = 1\n","    logger.info(\"device: %s, n_gpu: %d, distributed training %r\", device, args.n_gpu, bool(args.local_rank != -1))\n","    args.device = device\n","\n","    if args.gradient_accumulation_steps < 1:\n","        raise ValueError(\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\".format(\n","            args.gradient_accumulation_steps))\n","\n","    if os.path.exists(args.output_dir) and os.listdir(args.output_dir):\n","        if args.do_train:\n","            print(\"Output directory ({}) already exists and is not empty.\".format(args.output_dir))\n","    else:\n","        os.makedirs(args.output_dir, exist_ok=True)\n","    \n","    # Set seed\n","    set_seed(args)\n","\n","    # Prepare GLUE task\n","    args.task_name = args.task_name.lower()\n","    if args.task_name not in processors:\n","        raise ValueError(\"Task not found: %s\" % (args.task_name))\n","    processor = processors[args.task_name]()\n","    label_list = processor.get_labels()\n","    num_labels = len(label_list)\n","\n","    # Load pretrained model and tokenizer\n","    if args.local_rank not in [-1, 0]:\n","        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n","\n","    args.model_type = args.model_type.lower()\n","    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n","    config = config_class.from_pretrained(\n","        args.config_name if args.config_name else args.model_name_or_path,\n","        num_labels=num_labels,\n","        early_stopping = True,\n","        finetuning_task=args.task_name,\n","        cache_dir=args.cache_dir if args.cache_dir else None,\n","    )\n","    tokenizer = tokenizer_class.from_pretrained(\n","        args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,\n","        do_lower_case=args.do_lower_case,\n","        cache_dir=args.cache_dir if args.cache_dir else None,\n","    )\n","    model = model_class.from_pretrained(\n","        args.model_name_or_path,\n","        from_tf=bool(\".ckpt\" in args.model_name_or_path),\n","        config=config,\n","        cache_dir=args.cache_dir if args.cache_dir else None,\n","        opt={\"use_SAN\": 1},\n","        num_choices=[4]\n","    )\n","\n","    if args.local_rank == 0:\n","        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n","\n","    options_print = \"\"\n","    logging.info(\"Arg Options:\")\n","    for arg in vars(args):\n","        options_print += \"opt: %s=%s\\r\\n\" % (arg, getattr(args, arg))\n","    logging.info(options_print)\n","\n","    model.to(args.device)\n","\n","    def get_model_base_obj(model, model_type):\n","        if model_type == \"bert\":\n","            return model.bert\n","        elif model_type == \"xlm-roberta\" or model_type == \"roberta\":\n","            return model.roberta\n","        elif model_type == \"distilbert\":\n","            return model.distilbert    \n","        else:\n","            raise ValueError(\"model_type='{0}' is not supported!\")\n","\n","    if args.freeze_embeddings:\n","        for param in list(get_model_base_obj(model, args.model_type).embeddings.parameters()):\n","            param.requires_grad = False\n","        logger.info(\"Froze Embedding Layer\")\n","\n","    # freeze_layers is a string \"1,2,3\" representing layer number\n","    if args.freeze_layers:\n","        layer_indexes = [int(x) for x in args.freeze_layers]\n","        for layer_idx in layer_indexes:\n","            for param in list(\n","                get_model_base_obj(model, args.model_type).encoder.layer[layer_idx].parameters()\n","            ):\n","                param.requires_grad = False\n","            logger.info(\"Froze Layer: %s\", layer_idx)\n","\n","    logger.info(\"Training/evaluation parameters %s\", args)\n","    best_steps = 0\n","\n","    # Training\n","    if args.do_train:\n","        train_dataset = load_and_cache_examples(args, args.task_name, tokenizer, set_type='train')\n","        global_step, tr_loss, best_steps = train(args, train_dataset, model, tokenizer)\n","        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n","\n","    # Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n","    if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n","        # Create output directory if needed\n","        if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n","            os.makedirs(args.output_dir)\n","\n","        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n","        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n","        # They can then be reloaded using `from_pretrained()`\n","        model_to_save = (\n","            model.module if hasattr(model, \"module\") else model\n","        )  # Take care of distributed/parallel training\n","        model_to_save.save_pretrained(args.output_dir)\n","        tokenizer.save_pretrained(args.output_dir)\n","\n","        # Good practice: save your training arguments together with the trained model\n","        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n","\n","        # Load a trained model and vocabulary that you have fine-tuned\n","        model = model_class.from_pretrained(args.output_dir, opt={\"use_SAN\": 1}, num_choices=[4])\n","        tokenizer = tokenizer_class.from_pretrained(args.output_dir)\n","        model.to(args.device)\n","\n","    # Evaluation\n","    results = {}\n","    if args.do_eval and args.local_rank in [-1, 0]:\n","        if not args.do_train:\n","            args.output_dir = args.model_name_or_path\n","        checkpoints = [args.output_dir]\n","        if args.eval_all_checkpoints:\n","            checkpoints = list(\n","                os.path.dirname(c)\n","                for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n","            )\n","            logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n","        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n","        for checkpoint in checkpoints:\n","            global_step = checkpoint.split(\"/\")[-1] if len(checkpoints) > 1 else \"\"\n","            model = model_class.from_pretrained(checkpoint, opt={\"use_SAN\": 1}, num_choices=[4])\n","            model.to(args.device)\n","            result = evaluate(args, model, tokenizer, global_step)\n","            result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n","            results.update(result)\n","\n","    if args.do_test and args.local_rank in [-1, 0]:\n","        checkpoints = [args.output_dir]\n","        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n","        for checkpoint in checkpoints:\n","            global_step = checkpoint.split(\"/\")[-1]\n","            model = model_class.from_pretrained(checkpoint, opt={\"use_SAN\": 1}, num_choices=[4])\n","            model.to(args.device)\n","            result = evaluate(args, model, tokenizer, global_step, is_test=True)\n","            result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n","            results.update(result)\n","    # if best_steps:\n","    #     logger.info(\"best steps of eval acc is the following checkpoints: %s\", best_steps)\n","    return results\n"],"metadata":{"id":"pspnKkgEhAn2","executionInfo":{"status":"ok","timestamp":1676640235928,"user_tz":-420,"elapsed":6,"user":{"displayName":"Trọng Khôi Hoàng","userId":"12933784847383051959"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"42dvg6DqZPg2"},"source":["## Train "]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1676640235928,"user":{"displayName":"Trọng Khôi Hoàng","userId":"12933784847383051959"},"user_tz":-420},"id":"Tfky1cL1wKzR","outputId":"968e7e1e-bcf1-48dc-98be-f22aec6e4206"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'data_dir': '/content/drive/MyDrive/NCKH/ViMMRC_model/dataset/ViMMRC_RACE_v1',\n"," 'model_type': 'bert-man',\n"," 'model_name_or_path': 'bert-base-multilingual-cased',\n"," 'task_name': 'vimmrc_race',\n"," 'output_predictions': True,\n"," 'output_dir': '/content/drive/MyDrive/NCKH/ViMMRC_model/models/mbert_man_v1',\n"," 'freeze_embeddings': False,\n"," 'freeze_layers': None,\n"," 'tb_log_dir': '',\n"," 'config_name': '',\n"," 'tokenizer_name': '',\n"," 'cache_dir': '/content/drive/MyDrive/NCKH/ViMMRC_model/models/cached',\n"," 'max_seq_length': 512,\n"," 'do_train': True,\n"," 'do_eval': True,\n"," 'do_test': False,\n"," 'evaluate_during_training': True,\n"," 'do_lower_case': False,\n"," 'per_gpu_train_batch_size': 4,\n"," 'per_gpu_eval_batch_size': 4,\n"," 'gradient_accumulation_steps': 8,\n"," 'learning_rate': 3e-05,\n"," 'weight_decay': 0.01,\n"," 'max_grad_norm': 1.0,\n"," 'num_train_epochs': 7,\n"," 'max_steps': -1,\n"," 'warmup_proportion': 0.1,\n"," 'eval_all_checkpoints': True,\n"," 'no_cuda': False,\n"," 'seed': 42,\n"," 'local_rank': -1,\n"," 'f': '/root/.local/share/jupyter/runtime/kernel-5a49dc75-99d7-4dd9-abd6-f69bcc3207c1.json'}"]},"metadata":{},"execution_count":15}],"source":["parser = argparse.ArgumentParser()\n","\n","if True:\n","    # Required parameters\n","    parser.add_argument(\n","        \"--data_dir\", default=\"{}/dataset/ViMMRC_RACE_v1\".format(PRJ_DIR), type=str,\n","        help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\",\n","    )\n","\n","    parser.add_argument(\n","        \"--model_type\", default=\"bert-man\", type=str,\n","        help=\"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys()),\n","    )\n","    parser.add_argument(\n","        \"--model_name_or_path\", default=\"bert-base-multilingual-cased\", type=str,\n","        help=\"Path to pre-trained model or shortcut name selected in the list: \" + \", \".join(ALL_MODELS),\n","    )\n","    \n","    parser.add_argument(\n","        \"--task_name\", default=\"vimmrc_race\", type=str,\n","        help=\"The name of the task to train selected in the list: \" + \", \".join(processors.keys()),\n","    )\n","    # parser.add_argument(\n","    #     \"--para_type\", default=\"per_choice\", type=str,\n","    #     choices=[\"per_choice\", \"concat_choices\", \"ignore\"],\n","    #     help=\"Paragraph building strategy for ARC (default: %(default)s)\",\n","    # )\n","    parser.add_argument(\n","        \"--output_predictions\", default=True, type=bool, help=\"Whether to export the predictions from the eval step.\",\n","    )\n","    parser.add_argument(\n","        \"--output_dir\", default=OUTPUT_DIR, type=str, help=\"The output directory where the model predictions and checkpoints will be written.\",\n","    )\n","    parser.add_argument(\"--freeze_embeddings\", default=False, action=\"store_true\", help=\"Whether to freeze the embeeding layer.\",)\n","    parser.add_argument(\"--freeze_layers\", nargs=\"*\", help=\"Whether to freeze the embeeding layer.\",)\n","\n","    # Other parameters\n","    parser.add_argument(\"--tb_log_dir\", default=\"\", type=str, help=\"Tensorboard log dir for the current experiment\")\n","    parser.add_argument(\"--config_name\", default=\"\", type=str, help=\"Pretrained config name or path if not the same as model_name\")\n","    parser.add_argument(\"--tokenizer_name\", default=\"\", type=str, help=\"Pretrained tokenizer name or path if not the same as model_name\")\n","    parser.add_argument(\n","        \"--cache_dir\", default=\"{}/models/cached\".format(PRJ_DIR), type=str, help=\"Where do you want to store the pre-trained models downloaded from s3\",\n","    )\n","    parser.add_argument(\n","        \"--max_seq_length\", default=MAX_SEQ_LENGTHS['vimmrc'], type=int,\n","        help=\"The maximum total input sequence length after tokenization. Sequences longer than this will be truncated, sequences shorter will be padded.\",\n","    )\n","    parser.add_argument(\"--do_train\", default = True, action=\"store_true\", help=\"Whether to run training.\")\n","    parser.add_argument(\"--do_eval\", default = True, action=\"store_true\", help=\"Whether to run eval on the dev set.\")\n","    parser.add_argument(\"--do_test\", default = False, action=\"store_true\", help=\"Whether to run test on the test set\")\n","    parser.add_argument(\n","        \"--evaluate_during_training\", action=\"store_true\", default = True, help=\"Run evaluation during training at each logging step.\",\n","    )\n","    parser.add_argument(\n","        \"--do_lower_case\", action=\"store_true\", default = False, help=\"Set this flag if you are using an uncased model.\",\n","    )\n","\n","    parser.add_argument(\"--per_gpu_train_batch_size\", default=4, type=int, help=\"Batch size per GPU/CPU for training.\",)\n","    parser.add_argument(\"--per_gpu_eval_batch_size\", default=4, type=int, help=\"Batch size per GPU/CPU for evaluation.\",)\n","    parser.add_argument(\n","        \"--gradient_accumulation_steps\", type=int, default=8, help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n","    )\n","    parser.add_argument(\"--learning_rate\", default=3e-5, type=float, help=\"The initial learning rate for Adam.\")\n","    parser.add_argument(\"--weight_decay\", default=0.01, type=float, help=\"Weight decay if we apply some.\")\n","    # parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n","    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n","    parser.add_argument(\"--num_train_epochs\", default=7, type=float, help=\"Total number of training epochs to perform.\")\n","    parser.add_argument(\"--max_steps\", default=-1, type=int, help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\")\n","    parser.add_argument(\"--warmup_proportion\", default=0.1, type=float, help=\"Linear warmup over warmup_proportion.\")\n","\n","    # parser.add_argument(\"--logging_steps\", type=int, default=100, help=\"Log every X updates steps.\")\n","    # parser.add_argument(\"--save_steps\", type=int, default=500, help=\"Save checkpoint every X updates steps.\")\n","    parser.add_argument(\n","        \"--eval_all_checkpoints\", default=True, action=\"store_true\",\n","        help=\"Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number\",\n","    )\n","    parser.add_argument(\"--no_cuda\", default=False, action=\"store_true\", help=\"Avoid using CUDA when available\")\n","    # parser.add_argument(\n","    #     \"--overwrite_output_dir\", default = True, action=\"store_true\", help=\"Overwrite the content of the output directory\",\n","    # )\n","    # parser.add_argument(\n","    #     \"--overwrite_cache\", default = True, action=\"store_true\", help=\"Overwrite the cached training and evaluation sets\",\n","    # )\n","    parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization\")\n","\n","    # parser.add_argument(\n","    #     \"--fp16\", action=\"store_true\", help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n","    # )\n","    # parser.add_argument(\n","    #     \"--fp16_opt_level\", type=str, default=\"O1\",\n","    #     help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n","    #     \"See details at https://nvidia.github.io/apex/amp.html\",\n","    # )\n","    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n","    # parser.add_argument(\"--server_ip\", type=str, default=\"\", help=\"For distant debugging.\")\n","    # parser.add_argument(\"--server_port\", type=str, default=\"\", help=\"For distant debugging.\")\n","\n","    parser.add_argument('-f')\n","\n","args = parser.parse_args()\n","vars(args)\n","# options_print = \"\"\n","# logger.info(\"Arg Options - input:\")\n","# for arg in vars(args):\n","#     options_print += \"opt: %s=%s\\r\\n\" % (arg, getattr(args, arg))\n","# logger.info(options_print)"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"THNDibdjb9we","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["546db4339d1e4727bd290c60132d4adf","ee50a8d1e6fe4f8cbe8f2cc593aab03f","273c6d792bc248a7a31e935612415204","187555ddecd7498a87de85d75566672f","8f72c5e2754f44b686aa120bf14f92e7","0d868db0effb481e80ee2e7c4545caee","4710682448cf47f99d105af224a82980","6e5e26e895184a0d9b3497d392bc3962","a32fcc1870f44b059896e7a2d83ddc1f","051484f892f940149ae6fdfa33c99f1b","5376825aec0545e1b234f71d32f9249c","2940689cc7de41cf850d8919f0d712ed","f1aebffc384b49fa9a382cdb103747e7","06080b5ef0a749dd802417b1bda24bfd","f145af19e69f4074bc7d76b974366625","cf6c19cbcf79448e921638b7f544bdb1","d6dcdde3c6cc4747bf074a9ef382392a","eb0a86b6d2fd47ef9140ae8e1f98a20f","ac40fa7a54cf4b0fa98d0256348b2310","51bc97d7704140f388109bfb6aeacd8c","d9605b5d70614b4b9ea50c336940708d","4f4cb984ab244caf9dc386b0be6f83f5","0ec0aca53e0a4f11af933de5da13b5b6","72e182b0c07f41ad8b3e97557d997e35","2d97ff95304b4c4d8cff6c55deb0b169","bd77177cae774499bbbfd5b2b2845d7a","119da493c48d4894965e7389c6b54643","9892208a74e941ef9da5b38cac841628","c2fe6ad7461a4e0ea4ec4f925efcce15","63da01f947e44a979919d287422d5d81","6aad4b3ebe5745a9b6344e5e01659143","446f0bef69b74a4cac5acbc9ce7865c1","d1b122e85f0842a2888bb7e5bcbb95e0","2a6050d7a80a418186ca621b53ab56e7","2965f767f0584da2ba136408579a1124","9fe8b74309cc41ba87d4875b7a7b8244","248bb769141d4ef9884020516fe80a27","dbc0fc1886d34275be9ba0f3c9addaa5","ab5015d859fb49b0a7b75c3d5cae9899","e9955434aa024c728c212d04f85b7788","f7fb0ace73ec42ea855e41a0747411a9","7ff4b5a03dce496389520be1f5c263ee","ea1e38b1cb214eafb2cfc3fb6fa46839","3b5933ab448a41bc8aabe665f25deade","d3b40f71ead844a9863f49672e0ab6c1","72d1adce85234f36afafc55433178ef4","242e829edba9406699209b345e8301e0","ac6a683badfe48c990d07f6e56feaaf0","9f269c1632744432a6316e475252e7ef","c56c6947078a457692baf82e891a8b1f","01a81d72fec645f88d80cbf44a1ee0f1","2df61e6c32ec4019bdd4d80e490dc722","846616ce04cd48f0a6a11083fb5b9029","3139c978ae9b464d956d1f4998c354e5","578a456d08bb40bc90e946665dc80faa","3141fa33153141ed84fa060ad2dc283b","ad049bc1898a4f8c8c4f235b393bb2bc","b67ef86936d04fc6bb76edeea312bcc1","a9f860f663f44d558d3709f148dee995","1703ec9b9081412cb7152807d09d99df","a045170a184f4799a0340fcfe8ca2a05","ff31eeb17fda428bb54ad3f885d1ed71","480ee467c3d446618ff20ed8ac67dcf6","81fc311babe849b2ae9f7663361e4a63","9ce5349e9d364331a7ace1869548330f","3694d4f3b6684038bb35ee4bae617fda","dd9b52b86cd045e093dd3e67a7059972","16ce1d6987104277af83623ec505f477","e04dd6fa176340cd8d50d742b9c45bb9","6313512a60714a82a8fc9cc0d488ed8f","f22a79c2c7d2494fa5b3dc3756e0b1b7","325ba93e81074345bd5b695aa43b4465","0aaea4cd386d48d0bc5991aae8d5d30e","936c7716cff34116b74383c74e29ac6f","4237a825b3794004b7c61511ae44e4f0","136045282cf746e38a2f9dd3b001a116","2db613782f3d40d1b0abcf8c06f1e942","099ce8255ff24acbb7ac36cd873b662c","86cc5c153f464a6a889d18e551f80cad","4fa46cefdc024cf4bde647b5f081d7c0","943d38b8df0845c0a612bf74a2987652","ff9296b6d4374fd0855de9e84d6ec8a6","2e797e7e59dd4f208f801a0856820591","45d13e195f64437591b4561a8f39d878","42ec193b62a64a95bb33fc9949adad81","c0832a7dbedb46f2bb98b92deff9ad57","2d7ff008b1614fea89de947edfff804a","e2c8bfffc03b4405ba2f293a0da36cac","52b90b04b9f4481d9604110c2ea6af95","34155e5c85a841c983614b021bb172a1","541c46bc191944bcb65d9bd70f6ca613","a97267aa1aec4d0db1baed3a9b9ddb89","11bd9ca46cb7461f9532a9cfe240dc1b","305f3577f9aa49f6aa41d5dc305ccde0","aa7f8491592d4244b81eb5d14016d2d7","3019de181a0745db8180a39543c34516","e2423eed7c4944a781c620eca9dea3aa","a0a519a943924a20be80919f4c570781","14fc6840c1314f54b25369096dbec376","cd22b093c623480ca00dc166fdab0132","717e32d9eb4f4540becc6d28fd935043","d047bfb936bb425b884745d47028d14d","67e457329e2e406096769acb83bcd398","97b87fca24614030a726a8b090a5cbd5","c9057e0479584f32b62b22033d84d410","1a5d0a70d5324a50bd3340e187e17281","740a97c16ace439eb4002ec6b4c7fcf5","2e2c8a67611b4d0db666f4eee91dc06f","7b59f46a941647b1a9080d286853f7ad","74126c9cccb94e42a62ae29f0cefe687","fd24d301f4d4410bb688db152851ea6d","a6210fb2abe345c5a050ab99068de248","2fe35dae4db0477687668fb638b01c1c","8360cd3bed3c4bc192911ed0b68b1801","8ab78b5adc074d4eb9ff721f7e2e5c31","8a3d1ae4c79b433c940f21bbb8c2c27b","7f1a7afcb45a46ca89d8b4ade843b7d7","d22829f755884ddca67f034ed3017e89","6d748b18bdb245fe92b0eed27b28c89a","8911c631ac7c44499ccc1709f30a26c7","a374e6206dfa4a53b7bccba2d3c869f9","4d1e7f8641cd4a77b2a1c1e4e244cded","d70d5524294a4f8181b616da808d723f","6f595a7f32fb4887ac56013177201c25","e05085091a784db48c2dd48afb137d6b","2765ee73ccbd448ba689d695f3e86f08","848ac377fe9140469ea98b1aa42aa984","317fd3e2da6f4d9ca69800203037b1c8","2adcacc94d5742a8ba940ae092ee0994","fca588ac7a744f80a6b1d4b7a4e33dc2","e2fce2121b5640db8b9f269dafc1ceff","e8cea84b82a0443d931483ca71b17440","da83ae6d44074b00aec5cb3996b7256b","67ab440769a34196af3cd15608ed8da9","46679d1448bb4ac0909f9b681d342fcd","99b581c03b2e43d2b67905023388f57e","3e5112a923674908b84ec97785bb8387","accf3a1add7c45d6bfd0d7220dc805c1","d6511b0b2b6c4e6baf27661c21bfe2af","22a51a4df34144a3b0cd022d1ac643d6","f17ebe834ae4402a89abb04fc73c8c78","a837aa8db72f4bacad9de7a0f1bab126","7e6660aa22d7411ea4711b02c0b3584a","76dd0b4cb337499ab626890cd149034f","7b48e40ffb4c4c198aee27c99c035ffd","a9f546a1373d49c29fdb49245376e378","8663e83da66f49e2985d7976cad21c9f","e97c170e036543f9beb04a2029ce5d98","98c7e04e19674c4685144f23f0198a00","65b1ef44f23840428ec3e67b0db4af70","e2d3a1f281fd4f08b54e8114464e58a9","a1d0b1c2f2b848d2849ef0029386cebb","cbd8b478659840afa9e4ea75f48e63a3","63e56a54a2824c59bd6061ebf6864e99","dfa73fc351d24b85a599105ca53f1b09","dd25dbcbd9c94be3a7592c6ecdf1fabf","17e5f47e043f4c5a8588991a28af3377","60356b25e6fc4c4393eccd7880988d58","a46866d73be04c38985f2fa2cb34eebb","a84c9f56682343079d0d55c070d85bf2","4dd0841a3a5c4c4aba991384a38c0bb4","efad1cfa61ae4f18b1d55f9cacbc4ec3","35f3f9212d8447b7a78612571a57b3f4","1261976e246b4d91a06b649c291870a3","795ccfdb7b034c7da4c580cb729ad0de","1988c63161b0458d8b5d52305dcc002d","1446e908747b4b91a33265e59620eb21","26d2e99082fa4ba6bdf989a7daf9b1a6","4bf228891cb745e68228a8d06a84af7e","8d49fa21622149f8a47280dfb595b4de","d862600f71c54354b697221f1fe55b6f","60916cf7c18a4d92bd1cff3f15b9f105","be472366e9f34cfc9088c75f2a308ccd","421812ae3eea42e9a48da9fe2e936392","44cfe1ba23ac47f895718e782a97854d","50871d7536594c818fd152eb9193ceba","0c764720ff024cb1b374685dad174144","816ab2d516e64ca88a82e4b400689e90","4b1dd15717d24f69a43f9cf10970a62b","59879abf033f438b87700214ed8b482f","5de9166a0d664c0ead143dc0a7250c92","633b5c711a02416a98ddaaa556b2e914","0d890b2e52b14fe4bdefb37e9d47ba5e","b2f9dd3d50a3493397062c4527a7f5bc","931298ffb8ad43b994933ef599300375","abe7487c647a4af2a4bc5916f2352e7e","f2db0d68ae674647af8cfb284d758a5e","b31d111f95664fdcbb90679800f65d6b","f555138f7dbf4064aa344b237657671e","6d94d263c2be4506ac698a6737779913","5a0a87d0c8854a6d9b0228c1b9ee955c","63862220656249caa96f66efcd97ab12","b18b0e895aea464086ca95fa6d816589","7e1fe1baafdc4b1c9dfafcb3ceafd175","8128e0092be24bd298c1ec1f8e638c73","7c0f7c6203cc4597b6e76182c29a80e5","62e45f4ef44f4c799b7be1a800dd9e66","a1a628628824473cbabf514d2834d103"]},"executionInfo":{"status":"ok","timestamp":1676646439013,"user_tz":-420,"elapsed":6188113,"user":{"displayName":"Trọng Khôi Hoàng","userId":"12933784847383051959"}},"outputId":"fe0753a7-d0fd-4b15-caf1-cd6a330f0f4d"},"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:__main__:device: cuda, n_gpu: 1, distributed training False\n","Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMultipleChoice_SAN: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMultipleChoice_SAN from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMultipleChoice_SAN from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForMultipleChoice_SAN were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['out_proj.0.rnn.bias_ih', 'out_proj.0.query_wsum.att.linear.weight', 'out_proj.0.attn.score_func.linear.weight', 'out_proj.0.rnn.weight_ih', 'out_proj.0.attn.score_func.linear.bias', 'out_proj.0.rnn.weight_hh', 'out_proj.0.alpha', 'out_proj.0.query_wsum.att.linear.bias', 'out_proj.0.rnn.bias_hh', 'out_proj.0.classifier.proj.bias', 'out_proj.0.classifier.proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","INFO:__main__:Training/evaluation parameters Namespace(cache_dir='/content/drive/MyDrive/NCKH/ViMMRC_model/models/cached', config_name='', data_dir='/content/drive/MyDrive/NCKH/ViMMRC_model/dataset/ViMMRC_RACE_v1', device=device(type='cuda'), do_eval=True, do_lower_case=False, do_test=False, do_train=True, eval_all_checkpoints=True, evaluate_during_training=True, f='/root/.local/share/jupyter/runtime/kernel-5a49dc75-99d7-4dd9-abd6-f69bcc3207c1.json', freeze_embeddings=False, freeze_layers=None, gradient_accumulation_steps=8, learning_rate=3e-05, local_rank=-1, max_grad_norm=1.0, max_seq_length=512, max_steps=-1, model_name_or_path='bert-base-multilingual-cased', model_type='bert-man', n_gpu=1, no_cuda=False, num_train_epochs=7, output_dir='/content/drive/MyDrive/NCKH/ViMMRC_model/models/mbert_man_v1', output_predictions=True, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=4, seed=42, task_name='vimmrc_race', tb_log_dir='', tokenizer_name='', warmup_proportion=0.1, weight_decay=0.01)\n","INFO:__main__:Creating features from dataset file at /content/drive/MyDrive/NCKH/ViMMRC_model/dataset/ViMMRC_RACE_v1\n","INFO:__main__:LOOKING AT /content/drive/MyDrive/NCKH/ViMMRC_model/dataset/ViMMRC_RACE_v1 train\n","INFO:__main__:Training number: 7900\n"]},{"output_type":"display_data","data":{"text/plain":["Convert examples to features:   0%|          | 0/7900 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"546db4339d1e4727bd290c60132d4adf"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:__main__:*** Example ***\n","INFO:__main__:guid: train-grade_5_74-0\n","INFO:__main__:tokens: [CLS] C ##ụ Ú ##n làm nghề th ##ầy c ##ún ##g đã lâu năm . K ##h ##ắp làng xa bản gần , nhà nào có người ố ##m cũng nhờ cụ đến c ##ún ##g để đuổi t ##à ma . Nhiều người tôn cụ làm th ##ầy , c ##ắp sách theo cụ học nghề c ##ún ##g bá ##i . V ##ậ ##y mà gần một năm nay , chẳng hiểu cái ma nào làm cụ Ú ##n ố ##m . B ##ụng cụ đau qu ##ặ ##n , l ##ắm lúc tưởng như có con dao cứ ##a mạnh vào từng khúc ru ##ột . các học trò của cụ đã nhiều lần c ##ún ##g cho th ##ầy mà bệnh tình không thu ##yên giảm . Th ##ấy cha ngày càng đau nặng , con trai cụ k ##h ##ân khoản xin đưa cụ đi bệnh viện . Anh nói mã ##i , n ##ể lời , cụ mới đi . B ##ác sĩ bảo cụ bị s ##ỏ ##i th ##ận , phải m ##ổ lấy s ##ỏ ##i ra . C ##ụ sợ m ##ổ . H ##ơn nữa , cụ không tin bác sĩ người Kinh bắt được con ma người Thái . Thế là cụ tr ##ốn về nhà , Nhưng về đến nhà , cụ lại lên cơ ##n đau qu ##ằn qu ##ại . C ##ụ bắt con mời th ##ầy Vu ##i , học trò gi ##ỏ ##i nhất của cụ , đến c ##ún ##g trừ ma . C ##ún ##g suốt ngày đêm , bệnh vẫn không lui . Sá ##ng hôm sau , b ##ỗ ##ng có hai người mặc áo trắng tất tả phi ngựa đến . Hóa ra họ là bác sĩ và y tá bệnh viện đi tìm cụ Ú ##n . B ##ác sĩ ti ##êm thuốc giảm đau , cụ Ú ##n thấy đỡ . Ng ##ồi bên gi ##ường người bệnh , ông bác sĩ ô ##n tồn giải thích . Gia đình lại đưa cụ lên bệnh viện . N ##ử ##a tháng sau , cụ Ú ##n khỏi bệnh . Về nhà , cụ nói với bà con : - Từ nay , tôi d ##ứt k ##ho ##át bỏ nghề th ##ầy c ##ún ##g . Bà con ố ##m đau nên đi bệnh viện . [SEP] C ##ụ Ú ##n làm nghề gì ? [SEP] Th ##ầy c ##ún ##g trừ ma chữa bệnh . [SEP]\n","INFO:__main__:input_ids: 101 140 36075 249 10115 12984 96199 77586 64761 171 20858 10240 11213 46281 10558 119 148 10237 46916 26862 21772 15194 23594 117 13265 27258 10601 11027 1674 10147 13284 71038 53989 12002 171 20858 10240 12460 97520 188 10816 10824 119 81529 11027 50522 53989 12984 77586 64761 117 171 46916 24544 13951 53989 11125 96199 171 20858 10240 74686 10116 119 159 111575 10157 15542 23594 10417 10558 21537 117 82202 75067 28296 10824 27258 12984 53989 249 10115 1674 10147 119 139 92492 53989 110175 10608 111580 10115 117 180 57022 28298 42502 12552 10601 10173 49575 43735 10113 29172 11603 26258 46729 13483 46700 119 10792 11125 32457 10447 53989 11213 13710 18581 171 20858 10240 11257 77586 64761 15542 32686 23403 11755 23886 50431 39140 119 51635 48215 18939 12137 57255 110175 47428 117 10173 34101 53989 179 10237 15218 106591 77178 25576 53989 16895 32686 27805 119 15212 22072 63432 10116 117 182 62649 34619 117 53989 18652 16895 119 139 36700 18228 22916 53989 12505 187 36091 10116 77586 29762 117 15723 181 51417 29937 187 36091 10116 11859 119 140 36075 93663 181 51417 119 145 66316 42259 117 53989 11755 21629 98709 18228 11027 44367 18019 10476 10173 10824 11027 20985 119 39556 10331 53989 32221 51221 12420 13265 117 64737 12420 12002 13265 117 53989 13148 16732 16579 10115 110175 10608 77344 10608 37513 119 140 36075 18019 10173 102668 77586 64761 100154 10116 117 11125 32457 38356 36091 10116 13346 10447 53989 117 12002 171 20858 10240 70352 10824 119 140 20858 10240 63403 12137 16823 117 32686 22666 11755 10830 119 103843 10376 108113 11731 117 170 33078 10376 10601 13080 11027 48037 85525 52666 31734 11632 36500 53786 12002 119 81621 11859 10876 10331 98709 18228 10432 193 30185 32686 27805 16895 23375 53989 249 10115 119 139 36700 18228 14382 24495 63950 39140 110175 117 53989 249 10115 19787 100156 119 72959 36825 24429 38356 38259 11027 32686 117 12660 98709 18228 274 10115 44691 16658 32272 119 42106 25309 13148 25576 53989 16732 32686 27805 119 151 66499 10113 11642 11731 117 53989 249 10115 33718 32686 119 74605 13265 117 53989 22072 11182 27083 10173 131 118 29341 21537 117 40813 172 76290 179 10758 11969 27115 96199 77586 64761 171 20858 10240 119 56138 10173 1674 10147 110175 19114 16895 32686 27805 119 102 140 36075 249 10115 12984 96199 49309 136 102 51635 64761 171 20858 10240 70352 10824 100222 32686 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:__main__:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:__main__:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:__main__:premis_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:__main__:hypoth_mask: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:__main__:label: A\n","INFO:__main__:*** Example ***\n","INFO:__main__:guid: train-grade_5_74-1\n","INFO:__main__:tokens: [CLS] C ##ụ Ú ##n làm nghề th ##ầy c ##ún ##g đã lâu năm . K ##h ##ắp làng xa bản gần , nhà nào có người ố ##m cũng nhờ cụ đến c ##ún ##g để đuổi t ##à ma . Nhiều người tôn cụ làm th ##ầy , c ##ắp sách theo cụ học nghề c ##ún ##g bá ##i . V ##ậ ##y mà gần một năm nay , chẳng hiểu cái ma nào làm cụ Ú ##n ố ##m . B ##ụng cụ đau qu ##ặ ##n , l ##ắm lúc tưởng như có con dao cứ ##a mạnh vào từng khúc ru ##ột . các học trò của cụ đã nhiều lần c ##ún ##g cho th ##ầy mà bệnh tình không thu ##yên giảm . Th ##ấy cha ngày càng đau nặng , con trai cụ k ##h ##ân khoản xin đưa cụ đi bệnh viện . Anh nói mã ##i , n ##ể lời , cụ mới đi . B ##ác sĩ bảo cụ bị s ##ỏ ##i th ##ận , phải m ##ổ lấy s ##ỏ ##i ra . C ##ụ sợ m ##ổ . H ##ơn nữa , cụ không tin bác sĩ người Kinh bắt được con ma người Thái . Thế là cụ tr ##ốn về nhà , Nhưng về đến nhà , cụ lại lên cơ ##n đau qu ##ằn qu ##ại . C ##ụ bắt con mời th ##ầy Vu ##i , học trò gi ##ỏ ##i nhất của cụ , đến c ##ún ##g trừ ma . C ##ún ##g suốt ngày đêm , bệnh vẫn không lui . Sá ##ng hôm sau , b ##ỗ ##ng có hai người mặc áo trắng tất tả phi ngựa đến . Hóa ra họ là bác sĩ và y tá bệnh viện đi tìm cụ Ú ##n . B ##ác sĩ ti ##êm thuốc giảm đau , cụ Ú ##n thấy đỡ . Ng ##ồi bên gi ##ường người bệnh , ông bác sĩ ô ##n tồn giải thích . Gia đình lại đưa cụ lên bệnh viện . N ##ử ##a tháng sau , cụ Ú ##n khỏi bệnh . Về nhà , cụ nói với bà con : - Từ nay , tôi d ##ứt k ##ho ##át bỏ nghề th ##ầy c ##ún ##g . Bà con ố ##m đau nên đi bệnh viện . [SEP] C ##ụ Ú ##n làm nghề gì ? [SEP] Ông lang chữa bệnh bằng thuốc nam . [SEP]\n","INFO:__main__:input_ids: 101 140 36075 249 10115 12984 96199 77586 64761 171 20858 10240 11213 46281 10558 119 148 10237 46916 26862 21772 15194 23594 117 13265 27258 10601 11027 1674 10147 13284 71038 53989 12002 171 20858 10240 12460 97520 188 10816 10824 119 81529 11027 50522 53989 12984 77586 64761 117 171 46916 24544 13951 53989 11125 96199 171 20858 10240 74686 10116 119 159 111575 10157 15542 23594 10417 10558 21537 117 82202 75067 28296 10824 27258 12984 53989 249 10115 1674 10147 119 139 92492 53989 110175 10608 111580 10115 117 180 57022 28298 42502 12552 10601 10173 49575 43735 10113 29172 11603 26258 46729 13483 46700 119 10792 11125 32457 10447 53989 11213 13710 18581 171 20858 10240 11257 77586 64761 15542 32686 23403 11755 23886 50431 39140 119 51635 48215 18939 12137 57255 110175 47428 117 10173 34101 53989 179 10237 15218 106591 77178 25576 53989 16895 32686 27805 119 15212 22072 63432 10116 117 182 62649 34619 117 53989 18652 16895 119 139 36700 18228 22916 53989 12505 187 36091 10116 77586 29762 117 15723 181 51417 29937 187 36091 10116 11859 119 140 36075 93663 181 51417 119 145 66316 42259 117 53989 11755 21629 98709 18228 11027 44367 18019 10476 10173 10824 11027 20985 119 39556 10331 53989 32221 51221 12420 13265 117 64737 12420 12002 13265 117 53989 13148 16732 16579 10115 110175 10608 77344 10608 37513 119 140 36075 18019 10173 102668 77586 64761 100154 10116 117 11125 32457 38356 36091 10116 13346 10447 53989 117 12002 171 20858 10240 70352 10824 119 140 20858 10240 63403 12137 16823 117 32686 22666 11755 10830 119 103843 10376 108113 11731 117 170 33078 10376 10601 13080 11027 48037 85525 52666 31734 11632 36500 53786 12002 119 81621 11859 10876 10331 98709 18228 10432 193 30185 32686 27805 16895 23375 53989 249 10115 119 139 36700 18228 14382 24495 63950 39140 110175 117 53989 249 10115 19787 100156 119 72959 36825 24429 38356 38259 11027 32686 117 12660 98709 18228 274 10115 44691 16658 32272 119 42106 25309 13148 25576 53989 16732 32686 27805 119 151 66499 10113 11642 11731 117 53989 249 10115 33718 32686 119 74605 13265 117 53989 22072 11182 27083 10173 131 118 29341 21537 117 40813 172 76290 179 10758 11969 27115 96199 77586 64761 171 20858 10240 119 56138 10173 1674 10147 110175 19114 16895 32686 27805 119 102 140 36075 249 10115 12984 96199 49309 136 102 20646 12603 100222 32686 15991 63950 14441 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:__main__:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:__main__:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:__main__:premis_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:__main__:hypoth_mask: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:__main__:label: A\n","INFO:__main__:*** Example ***\n","INFO:__main__:guid: train-grade_5_74-2\n","INFO:__main__:tokens: [CLS] C ##ụ Ú ##n làm nghề th ##ầy c ##ún ##g đã lâu năm . K ##h ##ắp làng xa bản gần , nhà nào có người ố ##m cũng nhờ cụ đến c ##ún ##g để đuổi t ##à ma . Nhiều người tôn cụ làm th ##ầy , c ##ắp sách theo cụ học nghề c ##ún ##g bá ##i . V ##ậ ##y mà gần một năm nay , chẳng hiểu cái ma nào làm cụ Ú ##n ố ##m . B ##ụng cụ đau qu ##ặ ##n , l ##ắm lúc tưởng như có con dao cứ ##a mạnh vào từng khúc ru ##ột . các học trò của cụ đã nhiều lần c ##ún ##g cho th ##ầy mà bệnh tình không thu ##yên giảm . Th ##ấy cha ngày càng đau nặng , con trai cụ k ##h ##ân khoản xin đưa cụ đi bệnh viện . Anh nói mã ##i , n ##ể lời , cụ mới đi . B ##ác sĩ bảo cụ bị s ##ỏ ##i th ##ận , phải m ##ổ lấy s ##ỏ ##i ra . C ##ụ sợ m ##ổ . H ##ơn nữa , cụ không tin bác sĩ người Kinh bắt được con ma người Thái . Thế là cụ tr ##ốn về nhà , Nhưng về đến nhà , cụ lại lên cơ ##n đau qu ##ằn qu ##ại . C ##ụ bắt con mời th ##ầy Vu ##i , học trò gi ##ỏ ##i nhất của cụ , đến c ##ún ##g trừ ma . C ##ún ##g suốt ngày đêm , bệnh vẫn không lui . Sá ##ng hôm sau , b ##ỗ ##ng có hai người mặc áo trắng tất tả phi ngựa đến . Hóa ra họ là bác sĩ và y tá bệnh viện đi tìm cụ Ú ##n . B ##ác sĩ ti ##êm thuốc giảm đau , cụ Ú ##n thấy đỡ . Ng ##ồi bên gi ##ường người bệnh , ông bác sĩ ô ##n tồn giải thích . Gia đình lại đưa cụ lên bệnh viện . N ##ử ##a tháng sau , cụ Ú ##n khỏi bệnh . Về nhà , cụ nói với bà con : - Từ nay , tôi d ##ứt k ##ho ##át bỏ nghề th ##ầy c ##ún ##g . Bà con ố ##m đau nên đi bệnh viện . [SEP] C ##ụ Ú ##n làm nghề gì ? [SEP] B ##ác sĩ . [SEP]\n","INFO:__main__:input_ids: 101 140 36075 249 10115 12984 96199 77586 64761 171 20858 10240 11213 46281 10558 119 148 10237 46916 26862 21772 15194 23594 117 13265 27258 10601 11027 1674 10147 13284 71038 53989 12002 171 20858 10240 12460 97520 188 10816 10824 119 81529 11027 50522 53989 12984 77586 64761 117 171 46916 24544 13951 53989 11125 96199 171 20858 10240 74686 10116 119 159 111575 10157 15542 23594 10417 10558 21537 117 82202 75067 28296 10824 27258 12984 53989 249 10115 1674 10147 119 139 92492 53989 110175 10608 111580 10115 117 180 57022 28298 42502 12552 10601 10173 49575 43735 10113 29172 11603 26258 46729 13483 46700 119 10792 11125 32457 10447 53989 11213 13710 18581 171 20858 10240 11257 77586 64761 15542 32686 23403 11755 23886 50431 39140 119 51635 48215 18939 12137 57255 110175 47428 117 10173 34101 53989 179 10237 15218 106591 77178 25576 53989 16895 32686 27805 119 15212 22072 63432 10116 117 182 62649 34619 117 53989 18652 16895 119 139 36700 18228 22916 53989 12505 187 36091 10116 77586 29762 117 15723 181 51417 29937 187 36091 10116 11859 119 140 36075 93663 181 51417 119 145 66316 42259 117 53989 11755 21629 98709 18228 11027 44367 18019 10476 10173 10824 11027 20985 119 39556 10331 53989 32221 51221 12420 13265 117 64737 12420 12002 13265 117 53989 13148 16732 16579 10115 110175 10608 77344 10608 37513 119 140 36075 18019 10173 102668 77586 64761 100154 10116 117 11125 32457 38356 36091 10116 13346 10447 53989 117 12002 171 20858 10240 70352 10824 119 140 20858 10240 63403 12137 16823 117 32686 22666 11755 10830 119 103843 10376 108113 11731 117 170 33078 10376 10601 13080 11027 48037 85525 52666 31734 11632 36500 53786 12002 119 81621 11859 10876 10331 98709 18228 10432 193 30185 32686 27805 16895 23375 53989 249 10115 119 139 36700 18228 14382 24495 63950 39140 110175 117 53989 249 10115 19787 100156 119 72959 36825 24429 38356 38259 11027 32686 117 12660 98709 18228 274 10115 44691 16658 32272 119 42106 25309 13148 25576 53989 16732 32686 27805 119 151 66499 10113 11642 11731 117 53989 249 10115 33718 32686 119 74605 13265 117 53989 22072 11182 27083 10173 131 118 29341 21537 117 40813 172 76290 179 10758 11969 27115 96199 77586 64761 171 20858 10240 119 56138 10173 1674 10147 110175 19114 16895 32686 27805 119 102 140 36075 249 10115 12984 96199 49309 136 102 139 36700 18228 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:__main__:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:__main__:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:__main__:premis_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:__main__:hypoth_mask: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:__main__:label: A\n","INFO:__main__:*** Example ***\n","INFO:__main__:guid: train-grade_5_74-3\n","INFO:__main__:tokens: [CLS] C ##ụ Ú ##n làm nghề th ##ầy c ##ún ##g đã lâu năm . K ##h ##ắp làng xa bản gần , nhà nào có người ố ##m cũng nhờ cụ đến c ##ún ##g để đuổi t ##à ma . Nhiều người tôn cụ làm th ##ầy , c ##ắp sách theo cụ học nghề c ##ún ##g bá ##i . V ##ậ ##y mà gần một năm nay , chẳng hiểu cái ma nào làm cụ Ú ##n ố ##m . B ##ụng cụ đau qu ##ặ ##n , l ##ắm lúc tưởng như có con dao cứ ##a mạnh vào từng khúc ru ##ột . các học trò của cụ đã nhiều lần c ##ún ##g cho th ##ầy mà bệnh tình không thu ##yên giảm . Th ##ấy cha ngày càng đau nặng , con trai cụ k ##h ##ân khoản xin đưa cụ đi bệnh viện . Anh nói mã ##i , n ##ể lời , cụ mới đi . B ##ác sĩ bảo cụ bị s ##ỏ ##i th ##ận , phải m ##ổ lấy s ##ỏ ##i ra . C ##ụ sợ m ##ổ . H ##ơn nữa , cụ không tin bác sĩ người Kinh bắt được con ma người Thái . Thế là cụ tr ##ốn về nhà , Nhưng về đến nhà , cụ lại lên cơ ##n đau qu ##ằn qu ##ại . C ##ụ bắt con mời th ##ầy Vu ##i , học trò gi ##ỏ ##i nhất của cụ , đến c ##ún ##g trừ ma . C ##ún ##g suốt ngày đêm , bệnh vẫn không lui . Sá ##ng hôm sau , b ##ỗ ##ng có hai người mặc áo trắng tất tả phi ngựa đến . Hóa ra họ là bác sĩ và y tá bệnh viện đi tìm cụ Ú ##n . B ##ác sĩ ti ##êm thuốc giảm đau , cụ Ú ##n thấy đỡ . Ng ##ồi bên gi ##ường người bệnh , ông bác sĩ ô ##n tồn giải thích . Gia đình lại đưa cụ lên bệnh viện . N ##ử ##a tháng sau , cụ Ú ##n khỏi bệnh . Về nhà , cụ nói với bà con : - Từ nay , tôi d ##ứt k ##ho ##át bỏ nghề th ##ầy c ##ún ##g . Bà con ố ##m đau nên đi bệnh viện . [SEP] C ##ụ Ú ##n làm nghề gì ? [SEP] N ##ông dân . [SEP]\n","INFO:__main__:input_ids: 101 140 36075 249 10115 12984 96199 77586 64761 171 20858 10240 11213 46281 10558 119 148 10237 46916 26862 21772 15194 23594 117 13265 27258 10601 11027 1674 10147 13284 71038 53989 12002 171 20858 10240 12460 97520 188 10816 10824 119 81529 11027 50522 53989 12984 77586 64761 117 171 46916 24544 13951 53989 11125 96199 171 20858 10240 74686 10116 119 159 111575 10157 15542 23594 10417 10558 21537 117 82202 75067 28296 10824 27258 12984 53989 249 10115 1674 10147 119 139 92492 53989 110175 10608 111580 10115 117 180 57022 28298 42502 12552 10601 10173 49575 43735 10113 29172 11603 26258 46729 13483 46700 119 10792 11125 32457 10447 53989 11213 13710 18581 171 20858 10240 11257 77586 64761 15542 32686 23403 11755 23886 50431 39140 119 51635 48215 18939 12137 57255 110175 47428 117 10173 34101 53989 179 10237 15218 106591 77178 25576 53989 16895 32686 27805 119 15212 22072 63432 10116 117 182 62649 34619 117 53989 18652 16895 119 139 36700 18228 22916 53989 12505 187 36091 10116 77586 29762 117 15723 181 51417 29937 187 36091 10116 11859 119 140 36075 93663 181 51417 119 145 66316 42259 117 53989 11755 21629 98709 18228 11027 44367 18019 10476 10173 10824 11027 20985 119 39556 10331 53989 32221 51221 12420 13265 117 64737 12420 12002 13265 117 53989 13148 16732 16579 10115 110175 10608 77344 10608 37513 119 140 36075 18019 10173 102668 77586 64761 100154 10116 117 11125 32457 38356 36091 10116 13346 10447 53989 117 12002 171 20858 10240 70352 10824 119 140 20858 10240 63403 12137 16823 117 32686 22666 11755 10830 119 103843 10376 108113 11731 117 170 33078 10376 10601 13080 11027 48037 85525 52666 31734 11632 36500 53786 12002 119 81621 11859 10876 10331 98709 18228 10432 193 30185 32686 27805 16895 23375 53989 249 10115 119 139 36700 18228 14382 24495 63950 39140 110175 117 53989 249 10115 19787 100156 119 72959 36825 24429 38356 38259 11027 32686 117 12660 98709 18228 274 10115 44691 16658 32272 119 42106 25309 13148 25576 53989 16732 32686 27805 119 151 66499 10113 11642 11731 117 53989 249 10115 33718 32686 119 74605 13265 117 53989 22072 11182 27083 10173 131 118 29341 21537 117 40813 172 76290 179 10758 11969 27115 96199 77586 64761 171 20858 10240 119 56138 10173 1674 10147 110175 19114 16895 32686 27805 119 102 140 36075 249 10115 12984 96199 49309 136 102 151 32276 12486 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:__main__:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:__main__:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:__main__:premis_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:__main__:hypoth_mask: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:__main__:label: A\n","INFO:__main__:Saving features into cached file /content/drive/MyDrive/NCKH/ViMMRC_model/dataset/ViMMRC_RACE_v1/cached_train_bert-base-multilingual-cased_512_vimmrc_race\n","INFO:__main__:***** Running training *****\n","INFO:__main__:  Num examples = 1975\n","INFO:__main__:  Num Epochs = 7\n","INFO:__main__:  Instantaneous batch size per GPU = 4\n","INFO:__main__: Total train batch size (w. parallel, distributed & accumulation) = 32\n","INFO:__main__:  Gradient Accumulation steps = 8\n","INFO:__main__:  Total optimization steps = 427\n"]},{"output_type":"display_data","data":{"text/plain":["Epoch:   0%|          | 0/7 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2940689cc7de41cf850d8919f0d712ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Iteration:   0%|          | 0/494 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ec0aca53e0a4f11af933de5da13b5b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Iteration:   0%|          | 0/494 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a6050d7a80a418186ca621b53ab56e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Iteration:   0%|          | 0/494 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3b40f71ead844a9863f49672e0ab6c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Iteration:   0%|          | 0/494 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3141fa33153141ed84fa060ad2dc283b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Iteration:   0%|          | 0/494 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd9b52b86cd045e093dd3e67a7059972"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Iteration:   0%|          | 0/494 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"099ce8255ff24acbb7ac36cd873b662c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Iteration:   0%|          | 0/494 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52b90b04b9f4481d9604110c2ea6af95"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:__main__: global_step = 427, average loss = 1.174957918841828\n","INFO:__main__:Saving model checkpoint to /content/drive/MyDrive/NCKH/ViMMRC_model/models/mbert_man_v1\n","INFO:__main__:Evaluate the following checkpoints: ['/content/drive/MyDrive/NCKH/ViMMRC_model/models/mbert_man_v1/epoch_1', '/content/drive/MyDrive/NCKH/ViMMRC_model/models/mbert_man_v1/epoch_2', '/content/drive/MyDrive/NCKH/ViMMRC_model/models/mbert_man_v1/epoch_3', '/content/drive/MyDrive/NCKH/ViMMRC_model/models/mbert_man_v1/epoch_4', '/content/drive/MyDrive/NCKH/ViMMRC_model/models/mbert_man_v1/epoch_5', '/content/drive/MyDrive/NCKH/ViMMRC_model/models/mbert_man_v1/epoch_6', '/content/drive/MyDrive/NCKH/ViMMRC_model/models/mbert_man_v1/epoch_7', '/content/drive/MyDrive/NCKH/ViMMRC_model/models/mbert_man_v1']\n","INFO:__main__:Creating features from dataset file at /content/drive/MyDrive/NCKH/ViMMRC_model/dataset/ViMMRC_RACE_v1\n","INFO:__main__:LOOKING AT /content/drive/MyDrive/NCKH/ViMMRC_model/dataset/ViMMRC_RACE_v1 dev\n","INFO:__main__:Training number: 1176\n"]},{"output_type":"display_data","data":{"text/plain":["Convert examples to features:   0%|          | 0/1176 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd22b093c623480ca00dc166fdab0132"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:__main__:Saving features into cached file /content/drive/MyDrive/NCKH/ViMMRC_model/dataset/ViMMRC_RACE_v1/cached_dev_bert-base-multilingual-cased_512_vimmrc_race\n","INFO:__main__:***** Running evaluation for vimmrc_race on dev for epoch_1 *****\n","INFO:__main__:  Num examples = 294\n","INFO:__main__:  Batch size = 4\n"]},{"output_type":"display_data","data":{"text/plain":["Evaluating:   0%|          | 0/74 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd24d301f4d4410bb688db152851ea6d"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:__main__:***** Eval results for vimmrc_race on dev for epoch_1 *****\n","INFO:__main__:  eval_acc = 0.41496598639455784\n","INFO:__main__:  eval_loss = 1.4012470333962828\n","INFO:__main__:\n","\n","INFO:__main__:Saved /content/drive/MyDrive/NCKH/ViMMRC_model/models/mbert_man_v1/predictions_false_eval_results.json\n","INFO:__main__:Saved /content/drive/MyDrive/NCKH/ViMMRC_model/models/mbert_man_v1/predictions_false_eval_results_label.json\n","INFO:__main__:Loading features from cached file /content/drive/MyDrive/NCKH/ViMMRC_model/dataset/ViMMRC_RACE_v1/cached_dev_bert-base-multilingual-cased_512_vimmrc_race\n","INFO:__main__:***** Running evaluation for vimmrc_race on dev for epoch_2 *****\n","INFO:__main__:  Num examples = 294\n","INFO:__main__:  Batch size = 4\n"]},{"output_type":"display_data","data":{"text/plain":["Evaluating:   0%|          | 0/74 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d1e7f8641cd4a77b2a1c1e4e244cded"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:__main__:***** Eval results for vimmrc_race on dev for epoch_2 *****\n","INFO:__main__:  eval_acc = 0.43197278911564624\n","INFO:__main__:  eval_loss = 1.29548467494346\n","INFO:__main__:\n","\n","INFO:__main__:Saved /content/drive/MyDrive/NCKH/ViMMRC_model/models/mbert_man_v1/predictions_false_eval_results.json\n","INFO:__main__:Saved /content/drive/MyDrive/NCKH/ViMMRC_model/models/mbert_man_v1/predictions_false_eval_results_label.json\n","INFO:__main__:Loading features from cached file /content/drive/MyDrive/NCKH/ViMMRC_model/dataset/ViMMRC_RACE_v1/cached_dev_bert-base-multilingual-cased_512_vimmrc_race\n","INFO:__main__:***** Running evaluation for vimmrc_race on dev for epoch_3 *****\n","INFO:__main__:  Num examples = 294\n","INFO:__main__:  Batch size = 4\n"]},{"output_type":"display_data","data":{"text/plain":["Evaluating:   0%|          | 0/74 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da83ae6d44074b00aec5cb3996b7256b"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:__main__:***** Eval results for vimmrc_race on dev for epoch_3 *****\n","INFO:__main__:  eval_acc = 0.5238095238095238\n","INFO:__main__:  eval_loss = 1.1527641910958935\n","INFO:__main__:\n","\n","INFO:__main__:Saved /content/drive/MyDrive/NCKH/ViMMRC_model/models/mbert_man_v1/predictions_false_eval_results.json\n","INFO:__main__:Saved /content/drive/MyDrive/NCKH/ViMMRC_model/models/mbert_man_v1/predictions_false_eval_results_label.json\n","INFO:__main__:Loading features from cached file /content/drive/MyDrive/NCKH/ViMMRC_model/dataset/ViMMRC_RACE_v1/cached_dev_bert-base-multilingual-cased_512_vimmrc_race\n","INFO:__main__:***** Running evaluation for vimmrc_race on dev for epoch_4 *****\n","INFO:__main__:  Num examples = 294\n","INFO:__main__:  Batch size = 4\n"]},{"output_type":"display_data","data":{"text/plain":["Evaluating:   0%|          | 0/74 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76dd0b4cb337499ab626890cd149034f"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:__main__:***** Eval results for vimmrc_race on dev for epoch_4 *****\n","INFO:__main__:  eval_acc = 0.5476190476190477\n","INFO:__main__:  eval_loss = 1.0944845853625118\n","INFO:__main__:\n","\n","INFO:__main__:Saved /content/drive/MyDrive/NCKH/ViMMRC_model/models/mbert_man_v1/predictions_false_eval_results.json\n","INFO:__main__:Saved /content/drive/MyDrive/NCKH/ViMMRC_model/models/mbert_man_v1/predictions_false_eval_results_label.json\n","INFO:__main__:Loading features from cached file /content/drive/MyDrive/NCKH/ViMMRC_model/dataset/ViMMRC_RACE_v1/cached_dev_bert-base-multilingual-cased_512_vimmrc_race\n","INFO:__main__:***** Running evaluation for vimmrc_race on dev for epoch_5 *****\n","INFO:__main__:  Num examples = 294\n","INFO:__main__:  Batch size = 4\n"]},{"output_type":"display_data","data":{"text/plain":["Evaluating:   0%|          | 0/74 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfa73fc351d24b85a599105ca53f1b09"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:__main__:***** Eval results for vimmrc_race on dev for epoch_5 *****\n","INFO:__main__:  eval_acc = 0.6190476190476191\n","INFO:__main__:  eval_loss = 1.0752710394964025\n","INFO:__main__:\n","\n","INFO:__main__:Saved /content/drive/MyDrive/NCKH/ViMMRC_model/models/mbert_man_v1/predictions_false_eval_results.json\n","INFO:__main__:Saved /content/drive/MyDrive/NCKH/ViMMRC_model/models/mbert_man_v1/predictions_false_eval_results_label.json\n","INFO:__main__:Loading features from cached file /content/drive/MyDrive/NCKH/ViMMRC_model/dataset/ViMMRC_RACE_v1/cached_dev_bert-base-multilingual-cased_512_vimmrc_race\n","INFO:__main__:***** Running evaluation for vimmrc_race on dev for epoch_6 *****\n","INFO:__main__:  Num examples = 294\n","INFO:__main__:  Batch size = 4\n"]},{"output_type":"display_data","data":{"text/plain":["Evaluating:   0%|          | 0/74 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1988c63161b0458d8b5d52305dcc002d"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:__main__:***** Eval results for vimmrc_race on dev for epoch_6 *****\n","INFO:__main__:  eval_acc = 0.6360544217687075\n","INFO:__main__:  eval_loss = 1.083429496634651\n","INFO:__main__:\n","\n","INFO:__main__:Saved /content/drive/MyDrive/NCKH/ViMMRC_model/models/mbert_man_v1/predictions_false_eval_results.json\n","INFO:__main__:Saved /content/drive/MyDrive/NCKH/ViMMRC_model/models/mbert_man_v1/predictions_false_eval_results_label.json\n","INFO:__main__:Loading features from cached file /content/drive/MyDrive/NCKH/ViMMRC_model/dataset/ViMMRC_RACE_v1/cached_dev_bert-base-multilingual-cased_512_vimmrc_race\n","INFO:__main__:***** Running evaluation for vimmrc_race on dev for epoch_7 *****\n","INFO:__main__:  Num examples = 294\n","INFO:__main__:  Batch size = 4\n"]},{"output_type":"display_data","data":{"text/plain":["Evaluating:   0%|          | 0/74 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c764720ff024cb1b374685dad174144"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:__main__:***** Eval results for vimmrc_race on dev for epoch_7 *****\n","INFO:__main__:  eval_acc = 0.6054421768707483\n","INFO:__main__:  eval_loss = 1.1041088905688878\n","INFO:__main__:\n","\n","INFO:__main__:Saved /content/drive/MyDrive/NCKH/ViMMRC_model/models/mbert_man_v1/predictions_false_eval_results.json\n","INFO:__main__:Saved /content/drive/MyDrive/NCKH/ViMMRC_model/models/mbert_man_v1/predictions_false_eval_results_label.json\n","INFO:__main__:Loading features from cached file /content/drive/MyDrive/NCKH/ViMMRC_model/dataset/ViMMRC_RACE_v1/cached_dev_bert-base-multilingual-cased_512_vimmrc_race\n","INFO:__main__:***** Running evaluation for vimmrc_race on dev for mbert_man_v1 *****\n","INFO:__main__:  Num examples = 294\n","INFO:__main__:  Batch size = 4\n"]},{"output_type":"display_data","data":{"text/plain":["Evaluating:   0%|          | 0/74 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b31d111f95664fdcbb90679800f65d6b"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:__main__:***** Eval results for vimmrc_race on dev for mbert_man_v1 *****\n","INFO:__main__:  eval_acc = 0.6054421768707483\n","INFO:__main__:  eval_loss = 1.1041088905688878\n","INFO:__main__:\n","\n","INFO:__main__:Saved /content/drive/MyDrive/NCKH/ViMMRC_model/models/mbert_man_v1/predictions_false_eval_results.json\n","INFO:__main__:Saved /content/drive/MyDrive/NCKH/ViMMRC_model/models/mbert_man_v1/predictions_false_eval_results_label.json\n"]}],"source":["if __name__ == \"__main__\":\n","    main(args)"]},{"cell_type":"markdown","metadata":{"id":"Ix8RCrSpZSoL"},"source":["## Test"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1676646439848,"user":{"displayName":"Trọng Khôi Hoàng","userId":"12933784847383051959"},"user_tz":-420},"id":"z4Vc0QbZ1XY7","outputId":"bc35ea9a-2e3e-443a-d9ba-9856de73cc81"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'data_dir': '/content/drive/MyDrive/NCKH/ViMMRC_model/dataset/ViMMRC_RACE_v1',\n"," 'model_type': 'bert-man',\n"," 'model_name_or_path': 'bert-base-multilingual-cased',\n"," 'task_name': 'vimmrc_race',\n"," 'output_predictions': True,\n"," 'output_dir': '/content/drive/MyDrive/NCKH/ViMMRC_model/models/mbert_man_v1',\n"," 'freeze_embeddings': False,\n"," 'freeze_layers': None,\n"," 'tb_log_dir': '',\n"," 'config_name': '',\n"," 'tokenizer_name': '',\n"," 'cache_dir': '/content/drive/MyDrive/NCKH/ViMMRC_model/models/cached',\n"," 'max_seq_length': 512,\n"," 'do_train': False,\n"," 'do_eval': False,\n"," 'do_test': True,\n"," 'evaluate_during_training': True,\n"," 'do_lower_case': False,\n"," 'per_gpu_train_batch_size': 4,\n"," 'per_gpu_eval_batch_size': 4,\n"," 'gradient_accumulation_steps': 8,\n"," 'learning_rate': 3e-05,\n"," 'weight_decay': 0.01,\n"," 'max_grad_norm': 1.0,\n"," 'num_train_epochs': 7,\n"," 'max_steps': -1,\n"," 'warmup_proportion': 0.1,\n"," 'eval_all_checkpoints': True,\n"," 'no_cuda': False,\n"," 'seed': 42,\n"," 'local_rank': -1,\n"," 'f': '/root/.local/share/jupyter/runtime/kernel-5a49dc75-99d7-4dd9-abd6-f69bcc3207c1.json'}"]},"metadata":{},"execution_count":17}],"source":["parser = argparse.ArgumentParser()\n","\n","if True:\n","    # Required parameters\n","    parser.add_argument(\n","        \"--data_dir\", default=\"{}/dataset/ViMMRC_RACE_v1\".format(PRJ_DIR), type=str,\n","        help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\",\n","    )\n","\n","    parser.add_argument(\n","        \"--model_type\", default=\"bert-man\", type=str,\n","        help=\"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys()),\n","    )\n","    parser.add_argument(\n","        \"--model_name_or_path\", default=\"bert-base-multilingual-cased\", type=str,\n","        help=\"Path to pre-trained model or shortcut name selected in the list: \" + \", \".join(ALL_MODELS),\n","    )\n","    \n","    parser.add_argument(\n","        \"--task_name\", default=\"vimmrc_race\", type=str,\n","        help=\"The name of the task to train selected in the list: \" + \", \".join(processors.keys()),\n","    )\n","    # parser.add_argument(\n","    #     \"--para_type\", default=\"per_choice\", type=str,\n","    #     choices=[\"per_choice\", \"concat_choices\", \"ignore\"],\n","    #     help=\"Paragraph building strategy for ARC (default: %(default)s)\",\n","    # )\n","    parser.add_argument(\n","        \"--output_predictions\", default=True, type=bool, help=\"Whether to export the predictions from the eval step.\",\n","    )\n","    parser.add_argument(\n","        \"--output_dir\", default=OUTPUT_DIR, type=str, help=\"The output directory where the model predictions and checkpoints will be written.\",\n","    )\n","    parser.add_argument(\"--freeze_embeddings\", default=False, action=\"store_true\", help=\"Whether to freeze the embeeding layer.\",)\n","    parser.add_argument(\"--freeze_layers\", nargs=\"*\", help=\"Whether to freeze the embeeding layer.\",)\n","\n","    # Other parameters\n","    parser.add_argument(\"--tb_log_dir\", default=\"\", type=str, help=\"Tensorboard log dir for the current experiment\")\n","    parser.add_argument(\"--config_name\", default=\"\", type=str, help=\"Pretrained config name or path if not the same as model_name\")\n","    parser.add_argument(\"--tokenizer_name\", default=\"\", type=str, help=\"Pretrained tokenizer name or path if not the same as model_name\")\n","    parser.add_argument(\n","        \"--cache_dir\", default=\"{}/models/cached\".format(PRJ_DIR), type=str, help=\"Where do you want to store the pre-trained models downloaded from s3\",\n","    )\n","    parser.add_argument(\n","        \"--max_seq_length\", default=MAX_SEQ_LENGTHS['vimmrc'], type=int,\n","        help=\"The maximum total input sequence length after tokenization. Sequences longer than this will be truncated, sequences shorter will be padded.\",\n","    )\n","    parser.add_argument(\"--do_train\", default = False, action=\"store_true\", help=\"Whether to run training.\")\n","    parser.add_argument(\"--do_eval\", default = False, action=\"store_true\", help=\"Whether to run eval on the dev set.\")\n","    parser.add_argument(\"--do_test\", default = True, action=\"store_true\", help=\"Whether to run test on the test set\")\n","    parser.add_argument(\n","        \"--evaluate_during_training\", action=\"store_true\", default = True, help=\"Run evaluation during training at each logging step.\",\n","    )\n","    parser.add_argument(\n","        \"--do_lower_case\", action=\"store_true\", default = False, help=\"Set this flag if you are using an uncased model.\",\n","    )\n","\n","    parser.add_argument(\"--per_gpu_train_batch_size\", default=4, type=int, help=\"Batch size per GPU/CPU for training.\",)\n","    parser.add_argument(\"--per_gpu_eval_batch_size\", default=4, type=int, help=\"Batch size per GPU/CPU for evaluation.\",)\n","    parser.add_argument(\n","        \"--gradient_accumulation_steps\", type=int, default=8, help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n","    )\n","    parser.add_argument(\"--learning_rate\", default=3e-5, type=float, help=\"The initial learning rate for Adam.\")\n","    parser.add_argument(\"--weight_decay\", default=0.01, type=float, help=\"Weight decay if we apply some.\")\n","    # parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n","    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n","    parser.add_argument(\"--num_train_epochs\", default=7, type=float, help=\"Total number of training epochs to perform.\")\n","    parser.add_argument(\"--max_steps\", default=-1, type=int, help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\")\n","    parser.add_argument(\"--warmup_proportion\", default=0.1, type=float, help=\"Linear warmup over warmup_proportion.\")\n","\n","    # parser.add_argument(\"--logging_steps\", type=int, default=100, help=\"Log every X updates steps.\")\n","    # parser.add_argument(\"--save_steps\", type=int, default=500, help=\"Save checkpoint every X updates steps.\")\n","    parser.add_argument(\n","        \"--eval_all_checkpoints\", default=True, action=\"store_true\",\n","        help=\"Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number\",\n","    )\n","    parser.add_argument(\"--no_cuda\", default=False, action=\"store_true\", help=\"Avoid using CUDA when available\")\n","    # parser.add_argument(\n","    #     \"--overwrite_output_dir\", default = True, action=\"store_true\", help=\"Overwrite the content of the output directory\",\n","    # )\n","    # parser.add_argument(\n","    #     \"--overwrite_cache\", default = True, action=\"store_true\", help=\"Overwrite the cached training and evaluation sets\",\n","    # )\n","    parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization\")\n","\n","    # parser.add_argument(\n","    #     \"--fp16\", action=\"store_true\", help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n","    # )\n","    # parser.add_argument(\n","    #     \"--fp16_opt_level\", type=str, default=\"O1\",\n","    #     help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n","    #     \"See details at https://nvidia.github.io/apex/amp.html\",\n","    # )\n","    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n","    # parser.add_argument(\"--server_ip\", type=str, default=\"\", help=\"For distant debugging.\")\n","    # parser.add_argument(\"--server_port\", type=str, default=\"\", help=\"For distant debugging.\")\n","\n","    parser.add_argument('-f')\n","\n","args = parser.parse_args()\n","vars(args)"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":483,"referenced_widgets":["a4423f9ae07d4c7c96224cb2f88af809","f1e2438948374d8b8f30a47c6af902a8","e318fbfb01dd4566858df6966000fb41","47eb21a3e5194155bc69490dd1a79ca8","ee8b7ff75aab4a6b8f9946be8359ff9d","1c721e4a32b7484ba7993a8cca52b32b","b7bf0c31aab0454aa19879021c5d46ff","6505fa8271bc4718b3dd2985da218497","ed9843382b0048959ee6f733111c978f","371be7948fef4cb496919b29bad5e386","ad3d3266fe2e4beab1e9a93d0531e21e","cc00ee2d05e247e0b1f62c2cd8092d6a","fe2f5021c9394560b0a492302d2468d4","20f6f9779b13419798f96bd5c871eb60","bdcd0abcbfa14779bc97f8932f9d8fb4","13efed2941da4e919880c3cb33578d53","7e99dd881e3f42c983991cc588cafa3d","46ff0f6255ae4d108be6d0ae60a1e284","00594cf7686544cf887fc75ff0ff7dc3","1ed91dee7f314d88855ce986a54ed8a7","39034254b9044586ba30fe949b645982","d927c08cd12c4a8380978478d2cd08fa"]},"executionInfo":{"elapsed":103314,"status":"ok","timestamp":1676646996675,"user":{"displayName":"Trọng Khôi Hoàng","userId":"12933784847383051959"},"user_tz":-420},"id":"Q_0j2qEMx2Ry","outputId":"3e00634f-7776-4578-a86a-86a3a7c279a4"},"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:__main__:device: cuda, n_gpu: 1, distributed training False\n","Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMultipleChoice_SAN: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMultipleChoice_SAN from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMultipleChoice_SAN from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForMultipleChoice_SAN were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['out_proj.0.rnn.bias_ih', 'out_proj.0.query_wsum.att.linear.weight', 'out_proj.0.attn.score_func.linear.weight', 'out_proj.0.rnn.weight_ih', 'out_proj.0.attn.score_func.linear.bias', 'out_proj.0.rnn.weight_hh', 'out_proj.0.alpha', 'out_proj.0.query_wsum.att.linear.bias', 'out_proj.0.rnn.bias_hh', 'out_proj.0.classifier.proj.bias', 'out_proj.0.classifier.proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","INFO:__main__:Training/evaluation parameters Namespace(cache_dir='/content/drive/MyDrive/NCKH/ViMMRC_model/models/cached', config_name='', data_dir='/content/drive/MyDrive/NCKH/ViMMRC_model/dataset/ViMMRC_RACE_v1', device=device(type='cuda'), do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_all_checkpoints=True, evaluate_during_training=True, f='/root/.local/share/jupyter/runtime/kernel-5a49dc75-99d7-4dd9-abd6-f69bcc3207c1.json', freeze_embeddings=False, freeze_layers=None, gradient_accumulation_steps=8, learning_rate=3e-05, local_rank=-1, max_grad_norm=1.0, max_seq_length=512, max_steps=-1, model_name_or_path='bert-base-multilingual-cased', model_type='bert-man', n_gpu=1, no_cuda=False, num_train_epochs=7, output_dir='/content/drive/MyDrive/NCKH/ViMMRC_model/models/mbert_man_v1', output_predictions=True, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=4, seed=42, task_name='vimmrc_race', tb_log_dir='', tokenizer_name='', warmup_proportion=0.1, weight_decay=0.01)\n","INFO:__main__:Evaluate the following checkpoints: ['/content/drive/MyDrive/NCKH/ViMMRC_model/models/mbert_man_v1']\n","INFO:__main__:Creating features from dataset file at /content/drive/MyDrive/NCKH/ViMMRC_model/dataset/ViMMRC_RACE_v1\n","INFO:__main__:LOOKING AT /content/drive/MyDrive/NCKH/ViMMRC_model/dataset/ViMMRC_RACE_v1 test\n","INFO:__main__:Training number: 2056\n"]},{"output_type":"display_data","data":{"text/plain":["Convert examples to features:   0%|          | 0/2056 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4423f9ae07d4c7c96224cb2f88af809"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:__main__:Saving features into cached file /content/drive/MyDrive/NCKH/ViMMRC_model/dataset/ViMMRC_RACE_v1/cached_test_bert-base-multilingual-cased_512_vimmrc_race\n","INFO:__main__:***** Running evaluation for vimmrc_race on test for mbert_man_v1 *****\n","INFO:__main__:  Num examples = 514\n","INFO:__main__:  Batch size = 4\n"]},{"output_type":"display_data","data":{"text/plain":["Evaluating:   0%|          | 0/129 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc00ee2d05e247e0b1f62c2cd8092d6a"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:__main__:***** Eval results for vimmrc_race on test for mbert_man_v1 *****\n","INFO:__main__:  eval_acc = 0.5875486381322957\n","INFO:__main__:  eval_loss = 1.225590803017912\n","INFO:__main__:\n","\n","INFO:__main__:Saved /content/drive/MyDrive/NCKH/ViMMRC_model/models/mbert_man_v1/predictions_true_eval_results.json\n","INFO:__main__:Saved /content/drive/MyDrive/NCKH/ViMMRC_model/models/mbert_man_v1/predictions_true_eval_results_label.json\n"]}],"source":["if __name__ == \"__main__\":\n","    main(args)"]},{"cell_type":"markdown","source":["### Epoch X"],"metadata":{"id":"W10Uix578c1q"}},{"cell_type":"code","source":["parser = argparse.ArgumentParser()\n","\n","if True:\n","    # Required parameters\n","    parser.add_argument(\n","        \"--data_dir\", default=\"{}/dataset/ViMMRC_RACE_v1\".format(PRJ_DIR), type=str,\n","        help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\",\n","    )\n","\n","    parser.add_argument(\n","        \"--model_type\", default=\"bert-man\", type=str,\n","        help=\"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys()),\n","    )\n","    parser.add_argument(\n","        \"--model_name_or_path\", default=\"bert-base-multilingual-cased\", type=str,\n","        help=\"Path to pre-trained model or shortcut name selected in the list: \" + \", \".join(ALL_MODELS),\n","    )\n","    \n","    parser.add_argument(\n","        \"--task_name\", default=\"vimmrc_race\", type=str,\n","        help=\"The name of the task to train selected in the list: \" + \", \".join(processors.keys()),\n","    )\n","    # parser.add_argument(\n","    #     \"--para_type\", default=\"per_choice\", type=str,\n","    #     choices=[\"per_choice\", \"concat_choices\", \"ignore\"],\n","    #     help=\"Paragraph building strategy for ARC (default: %(default)s)\",\n","    # )\n","    parser.add_argument(\n","        \"--output_predictions\", default=True, type=bool, help=\"Whether to export the predictions from the eval step.\",\n","    )\n","    parser.add_argument(\n","        \"--output_dir\", default=OUTPUT_DIR+'/epoch_4', type=str, help=\"The output directory where the model predictions and checkpoints will be written.\",\n","    )\n","    parser.add_argument(\"--freeze_embeddings\", default=False, action=\"store_true\", help=\"Whether to freeze the embeeding layer.\",)\n","    parser.add_argument(\"--freeze_layers\", nargs=\"*\", help=\"Whether to freeze the embeeding layer.\",)\n","\n","    # Other parameters\n","    parser.add_argument(\"--tb_log_dir\", default=\"\", type=str, help=\"Tensorboard log dir for the current experiment\")\n","    parser.add_argument(\"--config_name\", default=\"\", type=str, help=\"Pretrained config name or path if not the same as model_name\")\n","    parser.add_argument(\"--tokenizer_name\", default=\"\", type=str, help=\"Pretrained tokenizer name or path if not the same as model_name\")\n","    parser.add_argument(\n","        \"--cache_dir\", default=\"{}/models/cached\".format(PRJ_DIR), type=str, help=\"Where do you want to store the pre-trained models downloaded from s3\",\n","    )\n","    parser.add_argument(\n","        \"--max_seq_length\", default=MAX_SEQ_LENGTHS['vimmrc'], type=int,\n","        help=\"The maximum total input sequence length after tokenization. Sequences longer than this will be truncated, sequences shorter will be padded.\",\n","    )\n","    parser.add_argument(\"--do_train\", default = False, action=\"store_true\", help=\"Whether to run training.\")\n","    parser.add_argument(\"--do_eval\", default = False, action=\"store_true\", help=\"Whether to run eval on the dev set.\")\n","    parser.add_argument(\"--do_test\", default = True, action=\"store_true\", help=\"Whether to run test on the test set\")\n","    parser.add_argument(\n","        \"--evaluate_during_training\", action=\"store_true\", default = True, help=\"Run evaluation during training at each logging step.\",\n","    )\n","    parser.add_argument(\n","        \"--do_lower_case\", action=\"store_true\", default = False, help=\"Set this flag if you are using an uncased model.\",\n","    )\n","\n","    parser.add_argument(\"--per_gpu_train_batch_size\", default=4, type=int, help=\"Batch size per GPU/CPU for training.\",)\n","    parser.add_argument(\"--per_gpu_eval_batch_size\", default=4, type=int, help=\"Batch size per GPU/CPU for evaluation.\",)\n","    parser.add_argument(\n","        \"--gradient_accumulation_steps\", type=int, default=8, help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n","    )\n","    parser.add_argument(\"--learning_rate\", default=3e-5, type=float, help=\"The initial learning rate for Adam.\")\n","    parser.add_argument(\"--weight_decay\", default=0.01, type=float, help=\"Weight decay if we apply some.\")\n","    # parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n","    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n","    parser.add_argument(\"--num_train_epochs\", default=7, type=float, help=\"Total number of training epochs to perform.\")\n","    parser.add_argument(\"--max_steps\", default=-1, type=int, help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\")\n","    parser.add_argument(\"--warmup_proportion\", default=0.1, type=float, help=\"Linear warmup over warmup_proportion.\")\n","\n","    # parser.add_argument(\"--logging_steps\", type=int, default=100, help=\"Log every X updates steps.\")\n","    # parser.add_argument(\"--save_steps\", type=int, default=500, help=\"Save checkpoint every X updates steps.\")\n","    parser.add_argument(\n","        \"--eval_all_checkpoints\", default=True, action=\"store_true\",\n","        help=\"Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number\",\n","    )\n","    parser.add_argument(\"--no_cuda\", default=False, action=\"store_true\", help=\"Avoid using CUDA when available\")\n","    # parser.add_argument(\n","    #     \"--overwrite_output_dir\", default = True, action=\"store_true\", help=\"Overwrite the content of the output directory\",\n","    # )\n","    # parser.add_argument(\n","    #     \"--overwrite_cache\", default = True, action=\"store_true\", help=\"Overwrite the cached training and evaluation sets\",\n","    # )\n","    parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization\")\n","\n","    # parser.add_argument(\n","    #     \"--fp16\", action=\"store_true\", help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n","    # )\n","    # parser.add_argument(\n","    #     \"--fp16_opt_level\", type=str, default=\"O1\",\n","    #     help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n","    #     \"See details at https://nvidia.github.io/apex/amp.html\",\n","    # )\n","    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n","    # parser.add_argument(\"--server_ip\", type=str, default=\"\", help=\"For distant debugging.\")\n","    # parser.add_argument(\"--server_port\", type=str, default=\"\", help=\"For distant debugging.\")\n","\n","    parser.add_argument('-f')\n","\n","args = parser.parse_args()\n","vars(args)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bwFETtr_8cKa","executionInfo":{"status":"ok","timestamp":1676647225386,"user_tz":-420,"elapsed":2169,"user":{"displayName":"Trọng Khôi Hoàng","userId":"12933784847383051959"}},"outputId":"eb51f8a7-f81a-4c16-ac89-57700bf1b84a"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'data_dir': '/content/drive/MyDrive/NCKH/ViMMRC_model/dataset/ViMMRC_RACE_v1',\n"," 'model_type': 'bert-man',\n"," 'model_name_or_path': 'bert-base-multilingual-cased',\n"," 'task_name': 'vimmrc_race',\n"," 'output_predictions': True,\n"," 'output_dir': '/content/drive/MyDrive/NCKH/ViMMRC_model/models/mbert_man_v1/epoch_4',\n"," 'freeze_embeddings': False,\n"," 'freeze_layers': None,\n"," 'tb_log_dir': '',\n"," 'config_name': '',\n"," 'tokenizer_name': '',\n"," 'cache_dir': '/content/drive/MyDrive/NCKH/ViMMRC_model/models/cached',\n"," 'max_seq_length': 512,\n"," 'do_train': False,\n"," 'do_eval': False,\n"," 'do_test': True,\n"," 'evaluate_during_training': True,\n"," 'do_lower_case': False,\n"," 'per_gpu_train_batch_size': 4,\n"," 'per_gpu_eval_batch_size': 4,\n"," 'gradient_accumulation_steps': 8,\n"," 'learning_rate': 3e-05,\n"," 'weight_decay': 0.01,\n"," 'max_grad_norm': 1.0,\n"," 'num_train_epochs': 7,\n"," 'max_steps': -1,\n"," 'warmup_proportion': 0.1,\n"," 'eval_all_checkpoints': True,\n"," 'no_cuda': False,\n"," 'seed': 42,\n"," 'local_rank': -1,\n"," 'f': '/root/.local/share/jupyter/runtime/kernel-5a49dc75-99d7-4dd9-abd6-f69bcc3207c1.json'}"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    main(args)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":399,"referenced_widgets":["86c75e57a2114bffbd7d5bf1fc2f08d0","d9713448789b46c9b88acdd435fc2aa4","fab0906dccbc466bb5b2dec10a07f7ef","63231f26782140cf9e8b1f056894bfde","514378b05948457c8f5ec8c2900aad22","8a7627120aad483a8975abda329c0040","f0294e671c0a4791bcd1e450a488c6c9","084ba9605e614cc88aa3348c96dccf5b","9d3499157846495f9600218aa2686e7a","37c7c9895f7345faadc517ddd2f05be4","a529b6684d6547eb9344d40427650ef6"]},"id":"t8D5TNbI8geu","executionInfo":{"status":"ok","timestamp":1676647320095,"user_tz":-420,"elapsed":90094,"user":{"displayName":"Trọng Khôi Hoàng","userId":"12933784847383051959"}},"outputId":"968d3429-544d-417e-e505-9b7c2b00a58c"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:__main__:device: cuda, n_gpu: 1, distributed training False\n","Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMultipleChoice_SAN: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMultipleChoice_SAN from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMultipleChoice_SAN from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForMultipleChoice_SAN were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['out_proj.0.rnn.bias_ih', 'out_proj.0.query_wsum.att.linear.weight', 'out_proj.0.attn.score_func.linear.weight', 'out_proj.0.rnn.weight_ih', 'out_proj.0.attn.score_func.linear.bias', 'out_proj.0.rnn.weight_hh', 'out_proj.0.alpha', 'out_proj.0.query_wsum.att.linear.bias', 'out_proj.0.rnn.bias_hh', 'out_proj.0.classifier.proj.bias', 'out_proj.0.classifier.proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","INFO:__main__:Training/evaluation parameters Namespace(cache_dir='/content/drive/MyDrive/NCKH/ViMMRC_model/models/cached', config_name='', data_dir='/content/drive/MyDrive/NCKH/ViMMRC_model/dataset/ViMMRC_RACE_v1', device=device(type='cuda'), do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_all_checkpoints=True, evaluate_during_training=True, f='/root/.local/share/jupyter/runtime/kernel-5a49dc75-99d7-4dd9-abd6-f69bcc3207c1.json', freeze_embeddings=False, freeze_layers=None, gradient_accumulation_steps=8, learning_rate=3e-05, local_rank=-1, max_grad_norm=1.0, max_seq_length=512, max_steps=-1, model_name_or_path='bert-base-multilingual-cased', model_type='bert-man', n_gpu=1, no_cuda=False, num_train_epochs=7, output_dir='/content/drive/MyDrive/NCKH/ViMMRC_model/models/mbert_man_v1/epoch_4', output_predictions=True, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=4, seed=42, task_name='vimmrc_race', tb_log_dir='', tokenizer_name='', warmup_proportion=0.1, weight_decay=0.01)\n","INFO:__main__:Evaluate the following checkpoints: ['/content/drive/MyDrive/NCKH/ViMMRC_model/models/mbert_man_v1/epoch_4']\n","INFO:__main__:Loading features from cached file /content/drive/MyDrive/NCKH/ViMMRC_model/dataset/ViMMRC_RACE_v1/cached_test_bert-base-multilingual-cased_512_vimmrc_race\n","INFO:__main__:***** Running evaluation for vimmrc_race on test for epoch_4 *****\n","INFO:__main__:  Num examples = 514\n","INFO:__main__:  Batch size = 4\n"]},{"output_type":"display_data","data":{"text/plain":["Evaluating:   0%|          | 0/129 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86c75e57a2114bffbd7d5bf1fc2f08d0"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:__main__:***** Eval results for vimmrc_race on test for epoch_4 *****\n","INFO:__main__:  eval_acc = 0.5428015564202334\n","INFO:__main__:  eval_loss = 1.2174161503943361\n","INFO:__main__:\n","\n","INFO:__main__:Saved /content/drive/MyDrive/NCKH/ViMMRC_model/models/mbert_man_v1/epoch_4/predictions_true_eval_results.json\n","INFO:__main__:Saved /content/drive/MyDrive/NCKH/ViMMRC_model/models/mbert_man_v1/epoch_4/predictions_true_eval_results_label.json\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["K_Pv9CIYm7BZ","ioQPuCNdks1x","uWwt6WujlC1Q","yoiGjVs5kuwD","fyV2VX7Vk1RA","42dvg6DqZPg2","Ix8RCrSpZSoL","W10Uix578c1q"],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.0"},"vscode":{"interpreter":{"hash":"5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"546db4339d1e4727bd290c60132d4adf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ee50a8d1e6fe4f8cbe8f2cc593aab03f","IPY_MODEL_273c6d792bc248a7a31e935612415204","IPY_MODEL_187555ddecd7498a87de85d75566672f"],"layout":"IPY_MODEL_8f72c5e2754f44b686aa120bf14f92e7"}},"ee50a8d1e6fe4f8cbe8f2cc593aab03f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0d868db0effb481e80ee2e7c4545caee","placeholder":"​","style":"IPY_MODEL_4710682448cf47f99d105af224a82980","value":"Convert 7899 of 7900 example to features: 100%"}},"273c6d792bc248a7a31e935612415204":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6e5e26e895184a0d9b3497d392bc3962","max":7900,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a32fcc1870f44b059896e7a2d83ddc1f","value":7900}},"187555ddecd7498a87de85d75566672f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_051484f892f940149ae6fdfa33c99f1b","placeholder":"​","style":"IPY_MODEL_5376825aec0545e1b234f71d32f9249c","value":" 7900/7900 [00:55&lt;00:00, 144.18it/s]"}},"8f72c5e2754f44b686aa120bf14f92e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0d868db0effb481e80ee2e7c4545caee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4710682448cf47f99d105af224a82980":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6e5e26e895184a0d9b3497d392bc3962":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a32fcc1870f44b059896e7a2d83ddc1f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"051484f892f940149ae6fdfa33c99f1b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5376825aec0545e1b234f71d32f9249c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2940689cc7de41cf850d8919f0d712ed":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f1aebffc384b49fa9a382cdb103747e7","IPY_MODEL_06080b5ef0a749dd802417b1bda24bfd","IPY_MODEL_f145af19e69f4074bc7d76b974366625"],"layout":"IPY_MODEL_cf6c19cbcf79448e921638b7f544bdb1"}},"f1aebffc384b49fa9a382cdb103747e7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d6dcdde3c6cc4747bf074a9ef382392a","placeholder":"​","style":"IPY_MODEL_eb0a86b6d2fd47ef9140ae8e1f98a20f","value":"Epoch: 100%"}},"06080b5ef0a749dd802417b1bda24bfd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac40fa7a54cf4b0fa98d0256348b2310","max":7,"min":0,"orientation":"horizontal","style":"IPY_MODEL_51bc97d7704140f388109bfb6aeacd8c","value":7}},"f145af19e69f4074bc7d76b974366625":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d9605b5d70614b4b9ea50c336940708d","placeholder":"​","style":"IPY_MODEL_4f4cb984ab244caf9dc386b0be6f83f5","value":" 7/7 [1:34:26&lt;00:00, 809.07s/it]"}},"cf6c19cbcf79448e921638b7f544bdb1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6dcdde3c6cc4747bf074a9ef382392a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb0a86b6d2fd47ef9140ae8e1f98a20f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ac40fa7a54cf4b0fa98d0256348b2310":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"51bc97d7704140f388109bfb6aeacd8c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d9605b5d70614b4b9ea50c336940708d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4f4cb984ab244caf9dc386b0be6f83f5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0ec0aca53e0a4f11af933de5da13b5b6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_72e182b0c07f41ad8b3e97557d997e35","IPY_MODEL_2d97ff95304b4c4d8cff6c55deb0b169","IPY_MODEL_bd77177cae774499bbbfd5b2b2845d7a"],"layout":"IPY_MODEL_119da493c48d4894965e7389c6b54643"}},"72e182b0c07f41ad8b3e97557d997e35":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9892208a74e941ef9da5b38cac841628","placeholder":"​","style":"IPY_MODEL_c2fe6ad7461a4e0ea4ec4f925efcce15","value":"train loss: 0.04015798702775586: 100%"}},"2d97ff95304b4c4d8cff6c55deb0b169":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_63da01f947e44a979919d287422d5d81","max":494,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6aad4b3ebe5745a9b6344e5e01659143","value":494}},"bd77177cae774499bbbfd5b2b2845d7a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_446f0bef69b74a4cac5acbc9ce7865c1","placeholder":"​","style":"IPY_MODEL_d1b122e85f0842a2888bb7e5bcbb95e0","value":" 494/494 [13:29&lt;00:00,  1.52s/it]"}},"119da493c48d4894965e7389c6b54643":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9892208a74e941ef9da5b38cac841628":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2fe6ad7461a4e0ea4ec4f925efcce15":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"63da01f947e44a979919d287422d5d81":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6aad4b3ebe5745a9b6344e5e01659143":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"446f0bef69b74a4cac5acbc9ce7865c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d1b122e85f0842a2888bb7e5bcbb95e0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2a6050d7a80a418186ca621b53ab56e7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2965f767f0584da2ba136408579a1124","IPY_MODEL_9fe8b74309cc41ba87d4875b7a7b8244","IPY_MODEL_248bb769141d4ef9884020516fe80a27"],"layout":"IPY_MODEL_dbc0fc1886d34275be9ba0f3c9addaa5"}},"2965f767f0584da2ba136408579a1124":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ab5015d859fb49b0a7b75c3d5cae9899","placeholder":"​","style":"IPY_MODEL_e9955434aa024c728c212d04f85b7788","value":"train loss: 0.041526173271874946: 100%"}},"9fe8b74309cc41ba87d4875b7a7b8244":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7fb0ace73ec42ea855e41a0747411a9","max":494,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7ff4b5a03dce496389520be1f5c263ee","value":494}},"248bb769141d4ef9884020516fe80a27":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea1e38b1cb214eafb2cfc3fb6fa46839","placeholder":"​","style":"IPY_MODEL_3b5933ab448a41bc8aabe665f25deade","value":" 494/494 [13:25&lt;00:00,  1.51s/it]"}},"dbc0fc1886d34275be9ba0f3c9addaa5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab5015d859fb49b0a7b75c3d5cae9899":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9955434aa024c728c212d04f85b7788":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f7fb0ace73ec42ea855e41a0747411a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ff4b5a03dce496389520be1f5c263ee":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ea1e38b1cb214eafb2cfc3fb6fa46839":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3b5933ab448a41bc8aabe665f25deade":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d3b40f71ead844a9863f49672e0ab6c1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_72d1adce85234f36afafc55433178ef4","IPY_MODEL_242e829edba9406699209b345e8301e0","IPY_MODEL_ac6a683badfe48c990d07f6e56feaaf0"],"layout":"IPY_MODEL_9f269c1632744432a6316e475252e7ef"}},"72d1adce85234f36afafc55433178ef4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c56c6947078a457692baf82e891a8b1f","placeholder":"​","style":"IPY_MODEL_01a81d72fec645f88d80cbf44a1ee0f1","value":"train loss: 0.04114436827680522: 100%"}},"242e829edba9406699209b345e8301e0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2df61e6c32ec4019bdd4d80e490dc722","max":494,"min":0,"orientation":"horizontal","style":"IPY_MODEL_846616ce04cd48f0a6a11083fb5b9029","value":494}},"ac6a683badfe48c990d07f6e56feaaf0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3139c978ae9b464d956d1f4998c354e5","placeholder":"​","style":"IPY_MODEL_578a456d08bb40bc90e946665dc80faa","value":" 494/494 [13:26&lt;00:00,  1.52s/it]"}},"9f269c1632744432a6316e475252e7ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c56c6947078a457692baf82e891a8b1f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"01a81d72fec645f88d80cbf44a1ee0f1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2df61e6c32ec4019bdd4d80e490dc722":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"846616ce04cd48f0a6a11083fb5b9029":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3139c978ae9b464d956d1f4998c354e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"578a456d08bb40bc90e946665dc80faa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3141fa33153141ed84fa060ad2dc283b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ad049bc1898a4f8c8c4f235b393bb2bc","IPY_MODEL_b67ef86936d04fc6bb76edeea312bcc1","IPY_MODEL_a9f860f663f44d558d3709f148dee995"],"layout":"IPY_MODEL_1703ec9b9081412cb7152807d09d99df"}},"ad049bc1898a4f8c8c4f235b393bb2bc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a045170a184f4799a0340fcfe8ca2a05","placeholder":"​","style":"IPY_MODEL_ff31eeb17fda428bb54ad3f885d1ed71","value":"train loss: 0.0401987720762527: 100%"}},"b67ef86936d04fc6bb76edeea312bcc1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_480ee467c3d446618ff20ed8ac67dcf6","max":494,"min":0,"orientation":"horizontal","style":"IPY_MODEL_81fc311babe849b2ae9f7663361e4a63","value":494}},"a9f860f663f44d558d3709f148dee995":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9ce5349e9d364331a7ace1869548330f","placeholder":"​","style":"IPY_MODEL_3694d4f3b6684038bb35ee4bae617fda","value":" 494/494 [13:25&lt;00:00,  1.51s/it]"}},"1703ec9b9081412cb7152807d09d99df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a045170a184f4799a0340fcfe8ca2a05":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff31eeb17fda428bb54ad3f885d1ed71":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"480ee467c3d446618ff20ed8ac67dcf6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"81fc311babe849b2ae9f7663361e4a63":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9ce5349e9d364331a7ace1869548330f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3694d4f3b6684038bb35ee4bae617fda":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dd9b52b86cd045e093dd3e67a7059972":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_16ce1d6987104277af83623ec505f477","IPY_MODEL_e04dd6fa176340cd8d50d742b9c45bb9","IPY_MODEL_6313512a60714a82a8fc9cc0d488ed8f"],"layout":"IPY_MODEL_f22a79c2c7d2494fa5b3dc3756e0b1b7"}},"16ce1d6987104277af83623ec505f477":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_325ba93e81074345bd5b695aa43b4465","placeholder":"​","style":"IPY_MODEL_0aaea4cd386d48d0bc5991aae8d5d30e","value":"train loss: 0.03879219270245258: 100%"}},"e04dd6fa176340cd8d50d742b9c45bb9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_936c7716cff34116b74383c74e29ac6f","max":494,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4237a825b3794004b7c61511ae44e4f0","value":494}},"6313512a60714a82a8fc9cc0d488ed8f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_136045282cf746e38a2f9dd3b001a116","placeholder":"​","style":"IPY_MODEL_2db613782f3d40d1b0abcf8c06f1e942","value":" 494/494 [13:25&lt;00:00,  1.52s/it]"}},"f22a79c2c7d2494fa5b3dc3756e0b1b7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"325ba93e81074345bd5b695aa43b4465":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0aaea4cd386d48d0bc5991aae8d5d30e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"936c7716cff34116b74383c74e29ac6f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4237a825b3794004b7c61511ae44e4f0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"136045282cf746e38a2f9dd3b001a116":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2db613782f3d40d1b0abcf8c06f1e942":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"099ce8255ff24acbb7ac36cd873b662c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_86cc5c153f464a6a889d18e551f80cad","IPY_MODEL_4fa46cefdc024cf4bde647b5f081d7c0","IPY_MODEL_943d38b8df0845c0a612bf74a2987652"],"layout":"IPY_MODEL_ff9296b6d4374fd0855de9e84d6ec8a6"}},"86cc5c153f464a6a889d18e551f80cad":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2e797e7e59dd4f208f801a0856820591","placeholder":"​","style":"IPY_MODEL_45d13e195f64437591b4561a8f39d878","value":"train loss: 0.03752168167317702: 100%"}},"4fa46cefdc024cf4bde647b5f081d7c0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_42ec193b62a64a95bb33fc9949adad81","max":494,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c0832a7dbedb46f2bb98b92deff9ad57","value":494}},"943d38b8df0845c0a612bf74a2987652":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2d7ff008b1614fea89de947edfff804a","placeholder":"​","style":"IPY_MODEL_e2c8bfffc03b4405ba2f293a0da36cac","value":" 494/494 [13:26&lt;00:00,  1.51s/it]"}},"ff9296b6d4374fd0855de9e84d6ec8a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e797e7e59dd4f208f801a0856820591":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"45d13e195f64437591b4561a8f39d878":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"42ec193b62a64a95bb33fc9949adad81":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0832a7dbedb46f2bb98b92deff9ad57":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2d7ff008b1614fea89de947edfff804a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2c8bfffc03b4405ba2f293a0da36cac":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"52b90b04b9f4481d9604110c2ea6af95":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_34155e5c85a841c983614b021bb172a1","IPY_MODEL_541c46bc191944bcb65d9bd70f6ca613","IPY_MODEL_a97267aa1aec4d0db1baed3a9b9ddb89"],"layout":"IPY_MODEL_11bd9ca46cb7461f9532a9cfe240dc1b"}},"34155e5c85a841c983614b021bb172a1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_305f3577f9aa49f6aa41d5dc305ccde0","placeholder":"​","style":"IPY_MODEL_aa7f8491592d4244b81eb5d14016d2d7","value":"train loss: 0.03628937584018401: 100%"}},"541c46bc191944bcb65d9bd70f6ca613":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3019de181a0745db8180a39543c34516","max":494,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e2423eed7c4944a781c620eca9dea3aa","value":494}},"a97267aa1aec4d0db1baed3a9b9ddb89":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a0a519a943924a20be80919f4c570781","placeholder":"​","style":"IPY_MODEL_14fc6840c1314f54b25369096dbec376","value":" 494/494 [13:25&lt;00:00,  1.52s/it]"}},"11bd9ca46cb7461f9532a9cfe240dc1b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"305f3577f9aa49f6aa41d5dc305ccde0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aa7f8491592d4244b81eb5d14016d2d7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3019de181a0745db8180a39543c34516":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2423eed7c4944a781c620eca9dea3aa":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a0a519a943924a20be80919f4c570781":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"14fc6840c1314f54b25369096dbec376":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cd22b093c623480ca00dc166fdab0132":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_717e32d9eb4f4540becc6d28fd935043","IPY_MODEL_d047bfb936bb425b884745d47028d14d","IPY_MODEL_67e457329e2e406096769acb83bcd398"],"layout":"IPY_MODEL_97b87fca24614030a726a8b090a5cbd5"}},"717e32d9eb4f4540becc6d28fd935043":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c9057e0479584f32b62b22033d84d410","placeholder":"​","style":"IPY_MODEL_1a5d0a70d5324a50bd3340e187e17281","value":"Convert 1175 of 1176 example to features: 100%"}},"d047bfb936bb425b884745d47028d14d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_740a97c16ace439eb4002ec6b4c7fcf5","max":1176,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2e2c8a67611b4d0db666f4eee91dc06f","value":1176}},"67e457329e2e406096769acb83bcd398":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b59f46a941647b1a9080d286853f7ad","placeholder":"​","style":"IPY_MODEL_74126c9cccb94e42a62ae29f0cefe687","value":" 1176/1176 [00:07&lt;00:00, 162.80it/s]"}},"97b87fca24614030a726a8b090a5cbd5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9057e0479584f32b62b22033d84d410":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a5d0a70d5324a50bd3340e187e17281":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"740a97c16ace439eb4002ec6b4c7fcf5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e2c8a67611b4d0db666f4eee91dc06f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7b59f46a941647b1a9080d286853f7ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74126c9cccb94e42a62ae29f0cefe687":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fd24d301f4d4410bb688db152851ea6d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a6210fb2abe345c5a050ab99068de248","IPY_MODEL_2fe35dae4db0477687668fb638b01c1c","IPY_MODEL_8360cd3bed3c4bc192911ed0b68b1801"],"layout":"IPY_MODEL_8ab78b5adc074d4eb9ff721f7e2e5c31"}},"a6210fb2abe345c5a050ab99068de248":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8a3d1ae4c79b433c940f21bbb8c2c27b","placeholder":"​","style":"IPY_MODEL_7f1a7afcb45a46ca89d8b4ade843b7d7","value":"Evaluating: 100%"}},"2fe35dae4db0477687668fb638b01c1c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d22829f755884ddca67f034ed3017e89","max":74,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6d748b18bdb245fe92b0eed27b28c89a","value":74}},"8360cd3bed3c4bc192911ed0b68b1801":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8911c631ac7c44499ccc1709f30a26c7","placeholder":"​","style":"IPY_MODEL_a374e6206dfa4a53b7bccba2d3c869f9","value":" 74/74 [00:43&lt;00:00,  2.00it/s]"}},"8ab78b5adc074d4eb9ff721f7e2e5c31":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a3d1ae4c79b433c940f21bbb8c2c27b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f1a7afcb45a46ca89d8b4ade843b7d7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d22829f755884ddca67f034ed3017e89":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d748b18bdb245fe92b0eed27b28c89a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8911c631ac7c44499ccc1709f30a26c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a374e6206dfa4a53b7bccba2d3c869f9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4d1e7f8641cd4a77b2a1c1e4e244cded":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d70d5524294a4f8181b616da808d723f","IPY_MODEL_6f595a7f32fb4887ac56013177201c25","IPY_MODEL_e05085091a784db48c2dd48afb137d6b"],"layout":"IPY_MODEL_2765ee73ccbd448ba689d695f3e86f08"}},"d70d5524294a4f8181b616da808d723f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_848ac377fe9140469ea98b1aa42aa984","placeholder":"​","style":"IPY_MODEL_317fd3e2da6f4d9ca69800203037b1c8","value":"Evaluating: 100%"}},"6f595a7f32fb4887ac56013177201c25":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2adcacc94d5742a8ba940ae092ee0994","max":74,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fca588ac7a744f80a6b1d4b7a4e33dc2","value":74}},"e05085091a784db48c2dd48afb137d6b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e2fce2121b5640db8b9f269dafc1ceff","placeholder":"​","style":"IPY_MODEL_e8cea84b82a0443d931483ca71b17440","value":" 74/74 [00:42&lt;00:00,  2.01it/s]"}},"2765ee73ccbd448ba689d695f3e86f08":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"848ac377fe9140469ea98b1aa42aa984":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"317fd3e2da6f4d9ca69800203037b1c8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2adcacc94d5742a8ba940ae092ee0994":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fca588ac7a744f80a6b1d4b7a4e33dc2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e2fce2121b5640db8b9f269dafc1ceff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e8cea84b82a0443d931483ca71b17440":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"da83ae6d44074b00aec5cb3996b7256b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_67ab440769a34196af3cd15608ed8da9","IPY_MODEL_46679d1448bb4ac0909f9b681d342fcd","IPY_MODEL_99b581c03b2e43d2b67905023388f57e"],"layout":"IPY_MODEL_3e5112a923674908b84ec97785bb8387"}},"67ab440769a34196af3cd15608ed8da9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_accf3a1add7c45d6bfd0d7220dc805c1","placeholder":"​","style":"IPY_MODEL_d6511b0b2b6c4e6baf27661c21bfe2af","value":"Evaluating: 100%"}},"46679d1448bb4ac0909f9b681d342fcd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_22a51a4df34144a3b0cd022d1ac643d6","max":74,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f17ebe834ae4402a89abb04fc73c8c78","value":74}},"99b581c03b2e43d2b67905023388f57e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a837aa8db72f4bacad9de7a0f1bab126","placeholder":"​","style":"IPY_MODEL_7e6660aa22d7411ea4711b02c0b3584a","value":" 74/74 [00:43&lt;00:00,  2.02it/s]"}},"3e5112a923674908b84ec97785bb8387":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"accf3a1add7c45d6bfd0d7220dc805c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6511b0b2b6c4e6baf27661c21bfe2af":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"22a51a4df34144a3b0cd022d1ac643d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f17ebe834ae4402a89abb04fc73c8c78":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a837aa8db72f4bacad9de7a0f1bab126":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e6660aa22d7411ea4711b02c0b3584a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"76dd0b4cb337499ab626890cd149034f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7b48e40ffb4c4c198aee27c99c035ffd","IPY_MODEL_a9f546a1373d49c29fdb49245376e378","IPY_MODEL_8663e83da66f49e2985d7976cad21c9f"],"layout":"IPY_MODEL_e97c170e036543f9beb04a2029ce5d98"}},"7b48e40ffb4c4c198aee27c99c035ffd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_98c7e04e19674c4685144f23f0198a00","placeholder":"​","style":"IPY_MODEL_65b1ef44f23840428ec3e67b0db4af70","value":"Evaluating: 100%"}},"a9f546a1373d49c29fdb49245376e378":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e2d3a1f281fd4f08b54e8114464e58a9","max":74,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a1d0b1c2f2b848d2849ef0029386cebb","value":74}},"8663e83da66f49e2985d7976cad21c9f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cbd8b478659840afa9e4ea75f48e63a3","placeholder":"​","style":"IPY_MODEL_63e56a54a2824c59bd6061ebf6864e99","value":" 74/74 [00:43&lt;00:00,  2.01it/s]"}},"e97c170e036543f9beb04a2029ce5d98":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"98c7e04e19674c4685144f23f0198a00":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"65b1ef44f23840428ec3e67b0db4af70":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e2d3a1f281fd4f08b54e8114464e58a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a1d0b1c2f2b848d2849ef0029386cebb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cbd8b478659840afa9e4ea75f48e63a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"63e56a54a2824c59bd6061ebf6864e99":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dfa73fc351d24b85a599105ca53f1b09":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dd25dbcbd9c94be3a7592c6ecdf1fabf","IPY_MODEL_17e5f47e043f4c5a8588991a28af3377","IPY_MODEL_60356b25e6fc4c4393eccd7880988d58"],"layout":"IPY_MODEL_a46866d73be04c38985f2fa2cb34eebb"}},"dd25dbcbd9c94be3a7592c6ecdf1fabf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a84c9f56682343079d0d55c070d85bf2","placeholder":"​","style":"IPY_MODEL_4dd0841a3a5c4c4aba991384a38c0bb4","value":"Evaluating: 100%"}},"17e5f47e043f4c5a8588991a28af3377":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_efad1cfa61ae4f18b1d55f9cacbc4ec3","max":74,"min":0,"orientation":"horizontal","style":"IPY_MODEL_35f3f9212d8447b7a78612571a57b3f4","value":74}},"60356b25e6fc4c4393eccd7880988d58":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1261976e246b4d91a06b649c291870a3","placeholder":"​","style":"IPY_MODEL_795ccfdb7b034c7da4c580cb729ad0de","value":" 74/74 [00:42&lt;00:00,  2.02it/s]"}},"a46866d73be04c38985f2fa2cb34eebb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a84c9f56682343079d0d55c070d85bf2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4dd0841a3a5c4c4aba991384a38c0bb4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"efad1cfa61ae4f18b1d55f9cacbc4ec3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35f3f9212d8447b7a78612571a57b3f4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1261976e246b4d91a06b649c291870a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"795ccfdb7b034c7da4c580cb729ad0de":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1988c63161b0458d8b5d52305dcc002d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1446e908747b4b91a33265e59620eb21","IPY_MODEL_26d2e99082fa4ba6bdf989a7daf9b1a6","IPY_MODEL_4bf228891cb745e68228a8d06a84af7e"],"layout":"IPY_MODEL_8d49fa21622149f8a47280dfb595b4de"}},"1446e908747b4b91a33265e59620eb21":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d862600f71c54354b697221f1fe55b6f","placeholder":"​","style":"IPY_MODEL_60916cf7c18a4d92bd1cff3f15b9f105","value":"Evaluating: 100%"}},"26d2e99082fa4ba6bdf989a7daf9b1a6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_be472366e9f34cfc9088c75f2a308ccd","max":74,"min":0,"orientation":"horizontal","style":"IPY_MODEL_421812ae3eea42e9a48da9fe2e936392","value":74}},"4bf228891cb745e68228a8d06a84af7e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_44cfe1ba23ac47f895718e782a97854d","placeholder":"​","style":"IPY_MODEL_50871d7536594c818fd152eb9193ceba","value":" 74/74 [00:43&lt;00:00,  2.01it/s]"}},"8d49fa21622149f8a47280dfb595b4de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d862600f71c54354b697221f1fe55b6f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"60916cf7c18a4d92bd1cff3f15b9f105":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"be472366e9f34cfc9088c75f2a308ccd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"421812ae3eea42e9a48da9fe2e936392":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"44cfe1ba23ac47f895718e782a97854d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"50871d7536594c818fd152eb9193ceba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0c764720ff024cb1b374685dad174144":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_816ab2d516e64ca88a82e4b400689e90","IPY_MODEL_4b1dd15717d24f69a43f9cf10970a62b","IPY_MODEL_59879abf033f438b87700214ed8b482f"],"layout":"IPY_MODEL_5de9166a0d664c0ead143dc0a7250c92"}},"816ab2d516e64ca88a82e4b400689e90":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_633b5c711a02416a98ddaaa556b2e914","placeholder":"​","style":"IPY_MODEL_0d890b2e52b14fe4bdefb37e9d47ba5e","value":"Evaluating: 100%"}},"4b1dd15717d24f69a43f9cf10970a62b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b2f9dd3d50a3493397062c4527a7f5bc","max":74,"min":0,"orientation":"horizontal","style":"IPY_MODEL_931298ffb8ad43b994933ef599300375","value":74}},"59879abf033f438b87700214ed8b482f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_abe7487c647a4af2a4bc5916f2352e7e","placeholder":"​","style":"IPY_MODEL_f2db0d68ae674647af8cfb284d758a5e","value":" 74/74 [00:42&lt;00:00,  2.01it/s]"}},"5de9166a0d664c0ead143dc0a7250c92":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"633b5c711a02416a98ddaaa556b2e914":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0d890b2e52b14fe4bdefb37e9d47ba5e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b2f9dd3d50a3493397062c4527a7f5bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"931298ffb8ad43b994933ef599300375":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"abe7487c647a4af2a4bc5916f2352e7e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2db0d68ae674647af8cfb284d758a5e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b31d111f95664fdcbb90679800f65d6b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f555138f7dbf4064aa344b237657671e","IPY_MODEL_6d94d263c2be4506ac698a6737779913","IPY_MODEL_5a0a87d0c8854a6d9b0228c1b9ee955c"],"layout":"IPY_MODEL_63862220656249caa96f66efcd97ab12"}},"f555138f7dbf4064aa344b237657671e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b18b0e895aea464086ca95fa6d816589","placeholder":"​","style":"IPY_MODEL_7e1fe1baafdc4b1c9dfafcb3ceafd175","value":"Evaluating: 100%"}},"6d94d263c2be4506ac698a6737779913":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8128e0092be24bd298c1ec1f8e638c73","max":74,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7c0f7c6203cc4597b6e76182c29a80e5","value":74}},"5a0a87d0c8854a6d9b0228c1b9ee955c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_62e45f4ef44f4c799b7be1a800dd9e66","placeholder":"​","style":"IPY_MODEL_a1a628628824473cbabf514d2834d103","value":" 74/74 [00:42&lt;00:00,  2.03it/s]"}},"63862220656249caa96f66efcd97ab12":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b18b0e895aea464086ca95fa6d816589":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e1fe1baafdc4b1c9dfafcb3ceafd175":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8128e0092be24bd298c1ec1f8e638c73":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c0f7c6203cc4597b6e76182c29a80e5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"62e45f4ef44f4c799b7be1a800dd9e66":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a1a628628824473cbabf514d2834d103":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a4423f9ae07d4c7c96224cb2f88af809":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f1e2438948374d8b8f30a47c6af902a8","IPY_MODEL_e318fbfb01dd4566858df6966000fb41","IPY_MODEL_47eb21a3e5194155bc69490dd1a79ca8"],"layout":"IPY_MODEL_ee8b7ff75aab4a6b8f9946be8359ff9d"}},"f1e2438948374d8b8f30a47c6af902a8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1c721e4a32b7484ba7993a8cca52b32b","placeholder":"​","style":"IPY_MODEL_b7bf0c31aab0454aa19879021c5d46ff","value":"Convert 2055 of 2056 example to features: 100%"}},"e318fbfb01dd4566858df6966000fb41":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6505fa8271bc4718b3dd2985da218497","max":2056,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ed9843382b0048959ee6f733111c978f","value":2056}},"47eb21a3e5194155bc69490dd1a79ca8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_371be7948fef4cb496919b29bad5e386","placeholder":"​","style":"IPY_MODEL_ad3d3266fe2e4beab1e9a93d0531e21e","value":" 2056/2056 [00:14&lt;00:00, 113.11it/s]"}},"ee8b7ff75aab4a6b8f9946be8359ff9d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c721e4a32b7484ba7993a8cca52b32b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b7bf0c31aab0454aa19879021c5d46ff":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6505fa8271bc4718b3dd2985da218497":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed9843382b0048959ee6f733111c978f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"371be7948fef4cb496919b29bad5e386":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad3d3266fe2e4beab1e9a93d0531e21e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cc00ee2d05e247e0b1f62c2cd8092d6a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fe2f5021c9394560b0a492302d2468d4","IPY_MODEL_20f6f9779b13419798f96bd5c871eb60","IPY_MODEL_bdcd0abcbfa14779bc97f8932f9d8fb4"],"layout":"IPY_MODEL_13efed2941da4e919880c3cb33578d53"}},"fe2f5021c9394560b0a492302d2468d4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e99dd881e3f42c983991cc588cafa3d","placeholder":"​","style":"IPY_MODEL_46ff0f6255ae4d108be6d0ae60a1e284","value":"Evaluating: 100%"}},"20f6f9779b13419798f96bd5c871eb60":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_00594cf7686544cf887fc75ff0ff7dc3","max":129,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1ed91dee7f314d88855ce986a54ed8a7","value":129}},"bdcd0abcbfa14779bc97f8932f9d8fb4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_39034254b9044586ba30fe949b645982","placeholder":"​","style":"IPY_MODEL_d927c08cd12c4a8380978478d2cd08fa","value":" 129/129 [01:09&lt;00:00,  2.08it/s]"}},"13efed2941da4e919880c3cb33578d53":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e99dd881e3f42c983991cc588cafa3d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"46ff0f6255ae4d108be6d0ae60a1e284":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"00594cf7686544cf887fc75ff0ff7dc3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ed91dee7f314d88855ce986a54ed8a7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"39034254b9044586ba30fe949b645982":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d927c08cd12c4a8380978478d2cd08fa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"86c75e57a2114bffbd7d5bf1fc2f08d0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d9713448789b46c9b88acdd435fc2aa4","IPY_MODEL_fab0906dccbc466bb5b2dec10a07f7ef","IPY_MODEL_63231f26782140cf9e8b1f056894bfde"],"layout":"IPY_MODEL_514378b05948457c8f5ec8c2900aad22"}},"d9713448789b46c9b88acdd435fc2aa4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8a7627120aad483a8975abda329c0040","placeholder":"​","style":"IPY_MODEL_f0294e671c0a4791bcd1e450a488c6c9","value":"Evaluating: 100%"}},"fab0906dccbc466bb5b2dec10a07f7ef":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_084ba9605e614cc88aa3348c96dccf5b","max":129,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9d3499157846495f9600218aa2686e7a","value":129}},"63231f26782140cf9e8b1f056894bfde":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_37c7c9895f7345faadc517ddd2f05be4","placeholder":"​","style":"IPY_MODEL_a529b6684d6547eb9344d40427650ef6","value":" 129/129 [01:15&lt;00:00,  2.08it/s]"}},"514378b05948457c8f5ec8c2900aad22":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a7627120aad483a8975abda329c0040":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f0294e671c0a4791bcd1e450a488c6c9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"084ba9605e614cc88aa3348c96dccf5b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d3499157846495f9600218aa2686e7a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"37c7c9895f7345faadc517ddd2f05be4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a529b6684d6547eb9344d40427650ef6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}