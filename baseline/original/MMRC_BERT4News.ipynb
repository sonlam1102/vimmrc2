{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26173,"status":"ok","timestamp":1675438050785,"user":{"displayName":"Sơn Lưu Thanh","userId":"09824077883060402796"},"user_tz":-420},"id":"n8OLTA2cqdHk","outputId":"79ab844f-07c8-455d-8dbd-62a73a5869d0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"K_Pv9CIYm7BZ"},"source":["# Library\n","\n","https://github.com/mhardalov/exams-qa"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":109046,"status":"ok","timestamp":1675438159826,"user":{"displayName":"Sơn Lưu Thanh","userId":"09824077883060402796"},"user_tz":-420},"id":"XHmheZ4im2cY","outputId":"fdc30b36-88e8-4526-a9af-8a891ac4ef8f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://download.pytorch.org/whl/torch_stable.html\n","Collecting torch==1.9.0+cu111\n","  Downloading https://download.pytorch.org/whl/cu111/torch-1.9.0%2Bcu111-cp38-cp38-linux_x86_64.whl (2041.3 MB)\n","\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.0/2.0 GB\u001b[0m \u001b[31m107.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0mtcmalloc: large alloc 2041339904 bytes == 0x34bc000 @  0x7f58f601d680 0x7f58f603e824 0x5b3128 0x5bbc90 0x5f714c 0x64d800 0x527022 0x504866 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x5f5ee6 0x56bbe1 0x569d8a 0x5f60c3 0x56cc92 0x569d8a 0x5f60c3\n","tcmalloc: large alloc 2551676928 bytes == 0x7cf82000 @  0x7f58f601d680 0x7f58f603dda2 0x5f714c 0x64d800 0x527022 0x504866 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x5f5ee6 0x56bbe1 0x569d8a 0x5f60c3 0x56cc92 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a\n","\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.0/2.0 GB\u001b[0m \u001b[31m98.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0mtcmalloc: large alloc 2041339904 bytes == 0x34bc000 @  0x7f58f601d680 0x7f58f603e824 0x5f97c1 0x5f8ecc 0x504866 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x5f60c3 0x50b32c 0x5f6b7b 0x66731d 0x5f6706 0x571143 0x50b22e 0x570b82 0x569d8a 0x50b3a0 0x570b82 0x569d8a 0x50b3a0 0x56cc92 0x501044 0x56be83 0x501044 0x56be83 0x501044 0x56be83 0x5f5ee6\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 GB\u001b[0m \u001b[31m986.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torchvision==0.10.0+cu111\n","  Downloading https://download.pytorch.org/whl/cu111/torchvision-0.10.0%2Bcu111-cp38-cp38-linux_x86_64.whl (23.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torchaudio==0.9.0\n","  Downloading torchaudio-0.9.0-cp38-cp38-manylinux1_x86_64.whl (1.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.9.0+cu111) (4.4.0)\n","Requirement already satisfied: pillow\u003e=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision==0.10.0+cu111) (7.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision==0.10.0+cu111) (1.21.6)\n","Installing collected packages: torch, torchvision, torchaudio\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.13.1+cu116\n","    Uninstalling torch-1.13.1+cu116:\n","      Successfully uninstalled torch-1.13.1+cu116\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.14.1+cu116\n","    Uninstalling torchvision-0.14.1+cu116:\n","      Successfully uninstalled torchvision-0.14.1+cu116\n","  Attempting uninstall: torchaudio\n","    Found existing installation: torchaudio 0.13.1+cu116\n","    Uninstalling torchaudio-0.13.1+cu116:\n","      Successfully uninstalled torchaudio-0.13.1+cu116\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchtext 0.14.1 requires torch==1.13.1, but you have torch 1.9.0+cu111 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed torch-1.9.0+cu111 torchaudio-0.9.0 torchvision-0.10.0+cu111\n"]}],"source":["pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6391,"status":"ok","timestamp":1675438166208,"user":{"displayName":"Sơn Lưu Thanh","userId":"09824077883060402796"},"user_tz":-420},"id":"ksz16DuhgUtk","outputId":"65541fc0-ea77-4827-d593-6bdd9f9210c9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers==3.0.2\n","  Downloading transformers-3.0.2-py3-none-any.whl (769 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m769.0/769.0 KB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sentencepiece!=0.1.92\n","  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==3.0.2) (4.64.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==3.0.2) (2.25.1)\n","Collecting tokenizers==0.8.1.rc1\n","  Downloading tokenizers-0.8.1rc1-cp38-cp38-manylinux1_x86_64.whl (3.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from transformers==3.0.2) (1.21.6)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from transformers==3.0.2) (23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==3.0.2) (3.9.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==3.0.2) (2022.6.2)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests-\u003etransformers==3.0.2) (2022.12.7)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.8/dist-packages (from requests-\u003etransformers==3.0.2) (2.10)\n","Requirement already satisfied: chardet\u003c5,\u003e=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests-\u003etransformers==3.0.2) (4.0.0)\n","Requirement already satisfied: urllib3\u003c1.27,\u003e=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests-\u003etransformers==3.0.2) (1.24.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses-\u003etransformers==3.0.2) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses-\u003etransformers==3.0.2) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses-\u003etransformers==3.0.2) (1.2.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=c06b076d0fbfe819f4eb5fd5a9ab7424bba11d809cf875c971b0640742e61f80\n","  Stored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n","Successfully installed sacremoses-0.0.53 sentencepiece-0.1.97 tokenizers-0.8.1rc1 transformers-3.0.2\n"]}],"source":["# !pip install transformers==2.8.0\n","!pip install transformers==3.0.2"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3854,"status":"ok","timestamp":1675438170052,"user":{"displayName":"Sơn Lưu Thanh","userId":"09824077883060402796"},"user_tz":-420},"id":"wEpYtDaLsYnO","outputId":"7a335bbd-13b9-4250-ba4d-6c7c489ea344"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tqdm==4.50.0\n","  Downloading tqdm-4.50.0-py2.py3-none-any.whl (70 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.7/70.7 KB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tqdm\n","  Attempting uninstall: tqdm\n","    Found existing installation: tqdm 4.64.1\n","    Uninstalling tqdm-4.64.1:\n","      Successfully uninstalled tqdm-4.64.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchtext 0.14.1 requires torch==1.13.1, but you have torch 1.9.0+cu111 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed tqdm-4.50.0\n"]}],"source":["pip install tqdm==4.50.0"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11595,"status":"ok","timestamp":1675438181641,"user":{"displayName":"Sơn Lưu Thanh","userId":"09824077883060402796"},"user_tz":-420},"id":"F5j-Ge8eMhPF","outputId":"32e13fab-32d8-4d66-feb9-a98ab5044a20"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/lanpa/tensorboardX.git\n","  Cloning https://github.com/lanpa/tensorboardX.git to /tmp/pip-req-build-wrf7msj_\n","  Running command git clone --filter=blob:none --quiet https://github.com/lanpa/tensorboardX.git /tmp/pip-req-build-wrf7msj_\n","  Resolved https://github.com/lanpa/tensorboardX.git to commit da08432ab6f78cbf0b86f532d046858867c8ac4c\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from tensorboardX==2.5.1+da08432) (1.21.6)\n","Requirement already satisfied: protobuf\u003c4,\u003e=3.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorboardX==2.5.1+da08432) (3.19.6)\n","Building wheels for collected packages: tensorboardX\n","  Building wheel for tensorboardX (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for tensorboardX: filename=tensorboardX-2.5.1-py2.py3-none-any.whl size=114483 sha256=d2913eda9b79b3b13a8bd1078094d3de8630f0e310834d1c425f67116fcca194\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-kkpi6fe4/wheels/12/5f/44/eea2cfeebe3401dbe0725409cea3d9823052f4292428a8c8d1\n","Successfully built tensorboardX\n","Installing collected packages: tensorboardX\n","Successfully installed tensorboardX-2.5.1\n"]}],"source":["pip install  --upgrade  git+https://github.com/lanpa/tensorboardX.git"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14959,"status":"ok","timestamp":1675438196594,"user":{"displayName":"Sơn Lưu Thanh","userId":"09824077883060402796"},"user_tz":-420},"id":"6bDRd9ERMmpc","outputId":"75be43bc-6b23-4966-c35b-9b2089fe31b3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/pytorch/ignite.git\n","  Cloning https://github.com/pytorch/ignite.git to /tmp/pip-req-build-mf4vcp9q\n","  Running command git clone --filter=blob:none --quiet https://github.com/pytorch/ignite.git /tmp/pip-req-build-mf4vcp9q\n","  Resolved https://github.com/pytorch/ignite.git to commit a720dfa7dee9dc0a8ad2a7323e6c33f101aca9c7\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torch\u003c3,\u003e=1.3 in /usr/local/lib/python3.8/dist-packages (from pytorch-ignite==0.5.0) (1.9.0+cu111)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from pytorch-ignite==0.5.0) (23.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch\u003c3,\u003e=1.3-\u003epytorch-ignite==0.5.0) (4.4.0)\n","Building wheels for collected packages: pytorch-ignite\n","  Building wheel for pytorch-ignite (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pytorch-ignite: filename=pytorch_ignite-0.5.0-py3-none-any.whl size=263517 sha256=069b460fce7418f8a2032dd17c504d47702cc35639904102ed14ef76693bae7c\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-ilz7svwb/wheels/30/c5/b6/a33c1fa72cf29f2460e45d69552e991bac7b4ad328f832005a\n","Successfully built pytorch-ignite\n","Installing collected packages: pytorch-ignite\n","Successfully installed pytorch-ignite-0.5.0\n"]}],"source":["pip install --upgrade git+https://github.com/pytorch/ignite.git"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3507,"status":"ok","timestamp":1675438200095,"user":{"displayName":"Sơn Lưu Thanh","userId":"09824077883060402796"},"user_tz":-420},"id":"pnAdrRKG0mOk","outputId":"e1b04b69-d0fa-4f96-abb0-0a88b40eb437"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting jsonlines\n","  Downloading jsonlines-3.1.0-py3-none-any.whl (8.6 kB)\n","Requirement already satisfied: attrs\u003e=19.2.0 in /usr/local/lib/python3.8/dist-packages (from jsonlines) (22.2.0)\n","Installing collected packages: jsonlines\n","Successfully installed jsonlines-3.1.0\n"]}],"source":["pip install jsonlines"]},{"cell_type":"markdown","metadata":{"id":"ioQPuCNdks1x"},"source":["# Models"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1675438200095,"user":{"displayName":"Sơn Lưu Thanh","userId":"09824077883060402796"},"user_tz":-420},"id":"MWZeTUney8dX"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":5212,"status":"ok","timestamp":1675438205301,"user":{"displayName":"Sơn Lưu Thanh","userId":"09824077883060402796"},"user_tz":-420},"id":"jO_MMOVhksP6"},"outputs":[],"source":["import torch\n","from transformers import BertForMaskedLM, RobertaForMaskedLM, XLMRobertaForMaskedLM, DistilBertForMaskedLM\n","\n","\n","class BertLMForMultipleChoice(BertForMaskedLM):\n","    def __init__(self, config):\n","        super().__init__(config)\n","\n","        self.init_weights()\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        masked_positions=None,\n","        masked_input_ids=None,\n","        labels=None,\n","    ):\n","        batch_size, num_choices, num_words = input_ids.shape\n","\n","        input_ids = input_ids.view(-1, input_ids.size(-1))\n","        masked_input_ids = input_ids.view(-1, input_ids.size(-1))\n","        attention_mask = (\n","            attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n","        )\n","        token_type_ids = (\n","            token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n","        )\n","        position_ids = (\n","            position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None\n","        )\n","\n","        prediction_scores = super().forward(\n","            masked_input_ids,\n","            attention_mask=attention_mask,\n","            # token_type_ids=token_type_ids,\n","            # position_ids=position_ids,\n","            head_mask=head_mask,\n","        )\n","\n","        predicted_probs = torch.softmax(\n","            prediction_scores[0].view(batch_size, num_choices, num_words, -1), dim=-1\n","        ).view(-1, self.config.vocab_size)\n","\n","        log_probs = torch.log(\n","            predicted_probs.gather(-1, input_ids.view(-1, 1)).view(\n","                batch_size, num_choices, num_words\n","            )\n","        ).masked_fill(masked_positions == 0, 0)\n","\n","        scores = 1 / (-log_probs.sum(dim=-1) / masked_positions.sum(axis=-1))\n","\n","        return torch.FloatTensor(0), scores\n","\n","\n","class RobertaLMForMultipleChoice(RobertaForMaskedLM):\n","    def __init__(self, config):\n","        super().__init__(config)\n","\n","        self.init_weights()\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        masked_positions=None,\n","        masked_input_ids=None,\n","        labels=None,\n","    ):\n","        batch_size, num_choices, num_words = input_ids.shape\n","\n","        input_ids = input_ids.view(-1, input_ids.size(-1))\n","        masked_input_ids = input_ids.view(-1, input_ids.size(-1))\n","        attention_mask = (\n","            attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n","        )\n","        token_type_ids = (\n","            token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n","        )\n","        position_ids = (\n","            position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None\n","        )\n","\n","        prediction_scores = super().forward(\n","            masked_input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            position_ids=position_ids,\n","            head_mask=head_mask,\n","        )\n","\n","        predicted_probs = torch.softmax(\n","            prediction_scores[0].view(batch_size, num_choices, num_words, -1), dim=-1\n","        ).view(-1, self.config.vocab_size)\n","\n","        log_probs = torch.log(\n","            predicted_probs.gather(-1, input_ids.view(-1, 1)).view(\n","                batch_size, num_choices, num_words\n","            )\n","        ).masked_fill(masked_positions == 0, 0)\n","\n","        scores = 1 / (-log_probs.sum(dim=-1) / masked_positions.sum(axis=-1))\n","\n","        return torch.FloatTensor(0), scores\n","\n","\n","class XLMRobertaLMForMultipleChoice(XLMRobertaForMaskedLM):\n","    def __init__(self, config):\n","        super().__init__(config)\n","\n","        self.init_weights()\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        masked_positions=None,\n","        masked_input_ids=None,\n","        labels=None,\n","    ):\n","        batch_size, num_choices, num_words = input_ids.shape\n","\n","        input_ids = input_ids.view(-1, input_ids.size(-1))\n","        masked_input_ids = input_ids.view(-1, input_ids.size(-1))\n","        attention_mask = (\n","            attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n","        )\n","        token_type_ids = (\n","            token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n","        )\n","        position_ids = (\n","            position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None\n","        )\n","\n","        prediction_scores = super().forward(\n","            masked_input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            position_ids=position_ids,\n","            head_mask=head_mask,\n","        )\n","\n","        predicted_probs = torch.softmax(\n","            prediction_scores[0].view(batch_size, num_choices, num_words, -1), dim=-1\n","        ).view(-1, self.config.vocab_size)\n","\n","        log_probs = torch.log(\n","            predicted_probs.gather(-1, input_ids.view(-1, 1)).view(\n","                batch_size, num_choices, num_words\n","            )\n","        ).masked_fill(masked_positions == 0, 0)\n","\n","        scores = 1 / (-log_probs.sum(dim=-1) / masked_positions.sum(axis=-1))\n","\n","        return torch.FloatTensor(0), scores\n","\n","class DistilBertLMForMultipleChoice(DistilBertForMaskedLM):\n","    def __init__(self, config):\n","        super().__init__(config)\n","\n","        self.init_weights()\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        masked_positions=None,\n","        masked_input_ids=None,\n","        labels=None,\n","    ):\n","        batch_size, num_choices, num_words = input_ids.shape\n","\n","        input_ids = input_ids.view(-1, input_ids.size(-1))\n","        masked_input_ids = input_ids.view(-1, input_ids.size(-1))\n","        attention_mask = (\n","            attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n","        )\n","        token_type_ids = (\n","            token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n","        )\n","        position_ids = (\n","            position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None\n","        )\n","\n","        prediction_scores = super().forward(\n","            masked_input_ids,\n","            # attention_mask=attention_mask,\n","            # token_type_ids=token_type_ids,\n","            # position_ids=position_ids,\n","            head_mask=head_mask,\n","        )\n","\n","        predicted_probs = torch.softmax(\n","            prediction_scores[0].view(batch_size, num_choices, num_words, -1), dim=-1\n","        ).view(-1, self.config.vocab_size)\n","\n","        log_probs = torch.log(\n","            predicted_probs.gather(-1, input_ids.view(-1, 1)).view(\n","                batch_size, num_choices, num_words\n","            )\n","        ).masked_fill(masked_positions == 0, 0)\n","\n","        scores = 1 / (-log_probs.sum(dim=-1) / masked_positions.sum(axis=-1))\n","\n","        return torch.FloatTensor(0), scores"]},{"cell_type":"markdown","metadata":{"id":"uWwt6WujlC1Q"},"source":["# Utils"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1675438205303,"user":{"displayName":"Sơn Lưu Thanh","userId":"09824077883060402796"},"user_tz":-420},"id":"HZxTWqhmlCaO"},"outputs":[],"source":["import csv\n","import glob\n","import json\n","import logging\n","import os\n","from typing import List\n","\n","import numpy as np\n","from tqdm import tqdm\n","from transformers import PreTrainedTokenizer\n","\n","logger = logging.getLogger(__name__)\n","\n","\n","class InputExample(object):\n","    \"\"\"A single training/test example for multiple choice\"\"\"\n","\n","    def __init__(self, example_id, question, contexts, endings, label=None):\n","        \"\"\"Constructs a InputExample.\n","        Args:\n","            example_id: Unique id for the example.\n","            contexts: list of str. The untokenized text of the first sequence (context of corresponding question).\n","            question: string. The untokenized text of the second sequence (question).\n","            endings: list of str. multiple choice's options. Its length must be equal to contexts' length.\n","            label: (Optional) string. The label of the example. This should be\n","            specified for train and dev examples, but not for test examples.\n","        \"\"\"\n","        self.example_id = example_id\n","        self.question = question\n","        self.contexts = contexts\n","        self.endings = endings\n","        self.label = label\n","\n","\n","class InputExampleEXAMS(InputExample):\n","    \"\"\"A single training/test example for multiple choice\"\"\"\n","\n","    def __init__(self, example_id, question, contexts, endings, options_size, label=None):\n","        super(InputExampleEXAMS, self).__init__(example_id, question, contexts, endings, label)\n","        self.options_size = options_size\n","\n","\n","class InputFeatures(object):\n","    def __init__(self, example_id, choices_features, answers_mask, label):\n","        self.example_id = example_id\n","        self.choices_features = [\n","            {\"input_ids\": input_ids, \"input_mask\": input_mask, \"segment_ids\": segment_ids}\n","            for input_ids, input_mask, segment_ids in choices_features\n","        ]\n","        self.answers_mask = answers_mask\n","        self.label = label\n","\n","\n","class DataProcessor(object):\n","    \"\"\"Base class for data converters for multiple choice data sets.\"\"\"\n","\n","    def get_train_examples(self, data_dir):\n","        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n","        raise NotImplementedError()\n","\n","    def get_dev_examples(self, data_dir):\n","        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n","        raise NotImplementedError()\n","\n","    def get_test_examples(self, data_dir):\n","        \"\"\"Gets a collection of `InputExample`s for the test set.\"\"\"\n","        raise NotImplementedError()\n","\n","    def get_labels(self):\n","        \"\"\"Gets the list of labels for this data set.\"\"\"\n","        raise NotImplementedError()\n","\n","\n","class RaceProcessor(DataProcessor):\n","    \"\"\"Processor for the RACE data set.\"\"\"\n","\n","    def get_train_examples(self, data_dir):\n","        \"\"\"See base class.\"\"\"\n","        logger.info(\"LOOKING AT {} train\".format(data_dir))\n","        high = os.path.join(data_dir, \"train_race.json\")\n","        # middle = os.path.join(data_dir, \"train/middle\")\n","        high = self._read_txt(high)\n","        # middle = self._read_txt(middle)\n","        return self._create_examples(high, \"train\")\n","\n","    def get_dev_examples(self, data_dir):\n","        \"\"\"See base class.\"\"\"\n","        logger.info(\"LOOKING AT {} dev\".format(data_dir))\n","        high = os.path.join(data_dir, \"dev_race.json\")\n","        # middle = os.path.join(data_dir, \"dev/middle\")\n","        high = self._read_txt(high)\n","        # middle = self._read_txt(middle)\n","        return self._create_examples(high, \"dev\")\n","\n","    def get_test_examples(self, data_dir):\n","        \"\"\"See base class.\"\"\"\n","        logger.info(\"LOOKING AT {} test\".format(data_dir))\n","        high = os.path.join(data_dir, \"test_race.json\")\n","        # middle = os.path.join(data_dir, \"test/middle\")\n","        high = self._read_txt(high)\n","        # middle = self._read_txt(middle)\n","        return self._create_examples(high, \"test\")\n","\n","    def get_labels(self):\n","        \"\"\"See base class.\"\"\"\n","        return [\"0\", \"1\", \"2\", \"3\"]\n","\n","    def _read_txt(self, input_dir):\n","        lines = []\n","        # files = glob.glob(input_dir + \"/*json\")\n","        # for file in tqdm(files, desc=\"read files\", disable = True):\n","        #     with open(file, \"r\", encoding=\"utf-8\") as fin:\n","        #         data_raw = json.load(fin)\n","        #         data_raw[\"race_id\"] = file\n","        #         lines.append(data_raw)\n","        with open(input_dir, \"r\", encoding=\"utf-8\") as fin:\n","            data_raw = json.load(fin)\n","        for d in data_raw:\n","            d['race_id'] = d['files']\n","            lines.append(d)\n","        return lines\n","\n","    def _create_examples(self, lines, set_type):\n","        \"\"\"Creates examples for the training and dev sets.\"\"\"\n","        examples = []\n","        for (_, data_raw) in enumerate(lines):\n","            race_id = \"%s-%s\" % (set_type, data_raw[\"race_id\"])\n","            article = data_raw[\"article\"]\n","            for i in range(len(data_raw[\"answers\"])):\n","                truth = str(ord(data_raw[\"answers\"][i]) - ord(\"A\"))\n","                # try:\n","                #     truth = str(ord(data_raw[\"answers\"][i]) - ord(\"A\"))\n","                # except Exception as e:\n","                #     print(data_raw[\"answers\"][i])\n","                #     print(\"-----\")\n","                #     print(data_raw[\"answers\"])\n","                #     raise e\n","                question = data_raw[\"questions\"][i]\n","                options = data_raw[\"options\"][i]\n","\n","                examples.append(\n","                    InputExample(\n","                        example_id=race_id,\n","                        question=question,\n","                        contexts=[\n","                            article,\n","                            article,\n","                            article,\n","                            article,\n","                        ],  # this is not efficient but convenient\n","                        endings=[options[0], options[1], options[2], options[3]],\n","                        label=truth,\n","                    )\n","                )\n","        return examples\n","\n","\n","class SwagProcessor(DataProcessor):\n","    \"\"\"Processor for the SWAG data set.\"\"\"\n","\n","    def get_train_examples(self, data_dir):\n","        \"\"\"See base class.\"\"\"\n","        logger.info(\"LOOKING AT {} train\".format(data_dir))\n","        return self._create_examples(self._read_csv(os.path.join(data_dir, \"train.csv\")), \"train\")\n","\n","    def get_dev_examples(self, data_dir):\n","        \"\"\"See base class.\"\"\"\n","        logger.info(\"LOOKING AT {} dev\".format(data_dir))\n","        return self._create_examples(self._read_csv(os.path.join(data_dir, \"val.csv\")), \"dev\")\n","\n","    def get_test_examples(self, data_dir):\n","        \"\"\"See base class.\"\"\"\n","        logger.info(\"LOOKING AT {} dev\".format(data_dir))\n","        raise ValueError(\n","            \"For swag testing, the input file does not contain a label column. It can not be tested in current code\"\n","            \"setting!\"\n","        )\n","        return self._create_examples(self._read_csv(os.path.join(data_dir, \"test.csv\")), \"test\")\n","\n","    def get_labels(self):\n","        \"\"\"See base class.\"\"\"\n","        return [\"0\", \"1\", \"2\", \"3\"]\n","\n","    def _read_csv(self, input_file):\n","        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n","            return list(csv.reader(f))\n","\n","    def _create_examples(self, lines: List[List[str]], type: str):\n","        \"\"\"Creates examples for the training and dev sets.\"\"\"\n","        if type == \"train\" and lines[0][-1] != \"label\":\n","            raise ValueError(\"For training, the input file must contain a label column.\")\n","\n","        examples = [\n","            InputExample(\n","                example_id=line[2],\n","                question=line[5],  # in the swag dataset, the\n","                # common beginning of each\n","                # choice is stored in \"sent2\".\n","                contexts=[line[4], line[4], line[4], line[4]],\n","                endings=[line[7], line[8], line[9], line[10]],\n","                label=line[11],\n","            )\n","            for line in lines[1:]  # we skip the line with the column names\n","        ]\n","\n","        return examples\n","\n","\n","class ArcProcessor(DataProcessor):\n","    \"\"\"Processor for the ARC data set (request from allennlp).\"\"\"\n","\n","    def __init__(self, para_type=\"per_choice\"):\n","        self.para_type = para_type\n","\n","    def get_train_examples(self, data_dir):\n","        \"\"\"See base class.\"\"\"\n","        logger.info(\"LOOKING AT {} train\".format(data_dir))\n","        return self._create_examples(\n","            self._read_json(os.path.join(data_dir, \"train.jsonl\")),\n","            \"train\",\n","            para_type=self.para_type,\n","        )\n","\n","    def get_dev_examples(self, data_dir):\n","        \"\"\"See base class.\"\"\"\n","        logger.info(\"LOOKING AT {} dev\".format(data_dir))\n","        return self._create_examples(\n","            self._read_json(os.path.join(data_dir, \"dev.jsonl\")), \"dev\", para_type=self.para_type\n","        )\n","\n","    def get_test_examples(self, data_dir):\n","        logger.info(\"LOOKING AT {} test\".format(data_dir))\n","        return self._create_examples(\n","            self._read_json(os.path.join(data_dir, \"test.jsonl\")), \"test\", para_type=self.para_type\n","        )\n","\n","    def get_labels(self):\n","        \"\"\"See base class.\"\"\"\n","        return [\"0\", \"1\", \"2\", \"3\"]\n","\n","    def _read_json(self, input_file):\n","        with open(input_file, \"r\", encoding=\"utf-8\") as fin:\n","            lines = fin.readlines()\n","            return lines\n","\n","    def _create_examples(self, lines, type, para_type=\"per_choice\"):\n","        \"\"\"Creates examples for the training and dev sets.\"\"\"\n","\n","        def get_contexts(data_raw, para_type):\n","            contexts = []\n","            if para_type == \"per_choice\":\n","                for option in data_raw[\"question\"][\"choices\"]:\n","                    para_context = \"\"\n","                    if \"para\" in option:\n","                        para_context = option[\"para\"].replace(\"_\", \"\")\n","                    else:\n","                        para_context = data_raw[\"para\"].replace(\"_\", \"\")\n","                    contexts.append(para_context)\n","            elif para_type == \"concat_choices\":\n","                para_context = \" \".join(\n","                    [ch[\"text\"].strip() for ch in data_raw[\"question\"][\"choices\"]]\n","                )\n","                for oi in range(len(data_raw[\"question\"][\"choices\"])):\n","                    contexts.append(para_context)\n","            else:\n","                raise Exception('para_type \"{0}\" is not supported!'.format(para_type))\n","\n","            return contexts\n","\n","        # There are two types of labels. They should be normalized\n","        def normalize(truth):\n","            if truth in \"ABCD\":\n","                return ord(truth) - ord(\"A\")\n","            elif truth in \"1234\":\n","                return int(truth) - 1\n","            else:\n","                logger.info(\"truth ERROR! %s\", str(truth))\n","                return None\n","\n","        examples = []\n","        three_choice = 0\n","        four_choice = 0\n","        five_choice = 0\n","        other_choices = 0\n","        # we deleted example which has more than or less than four choices\n","        for line in tqdm(lines, desc=\"read arc data\"):\n","            data_raw = json.loads(line.strip(\"\\n\"))\n","            if len(data_raw[\"question\"][\"choices\"]) == 3:\n","                three_choice += 1\n","                continue\n","            elif len(data_raw[\"question\"][\"choices\"]) == 5:\n","                five_choice += 1\n","                continue\n","            elif len(data_raw[\"question\"][\"choices\"]) != 4:\n","                other_choices += 1\n","                continue\n","            four_choice += 1\n","            truth = str(normalize(data_raw[\"answerKey\"]))\n","            assert truth != \"None\"\n","            question_choices = data_raw[\"question\"]\n","            question = question_choices[\"stem\"]\n","            id = data_raw[\"id\"]\n","            options = question_choices[\"choices\"]\n","            if len(options) == 4:\n","                contexts = get_contexts(data_raw, para_type)\n","                examples.append(\n","                    InputExample(\n","                        example_id=id,\n","                        question=question,\n","                        contexts=contexts,\n","                        endings=[\n","                            options[0][\"text\"],\n","                            options[1][\"text\"],\n","                            options[2][\"text\"],\n","                            options[3][\"text\"],\n","                        ],\n","                        label=truth,\n","                    )\n","                )\n","\n","        if type == \"train\":\n","            assert len(examples) \u003e 1\n","            assert examples[0].label is not None\n","\n","        logger.info(\"len examples: %s}\", str(len(examples)))\n","        logger.info(\"Three choices: %s\", str(three_choice))\n","        logger.info(\"Five choices: %s\", str(five_choice))\n","        logger.info(\"Other choices: %s\", str(other_choices))\n","        logger.info(\"four choices: %s\", str(four_choice))\n","\n","        return examples\n","\n","\n","class ExamsProcessor(ArcProcessor):\n","    def get_labels(self):\n","        \"\"\"See base class.\"\"\"\n","        return [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"]\n","\n","    def _create_examples(self, lines, type, para_type=\"per_choice\"):\n","        \"\"\"Creates examples for the training and dev sets.\"\"\"\n","\n","        def get_contexts(data_raw, para_type):\n","            contexts = []\n","            if para_type == \"per_choice\":\n","                for option in data_raw[\"question\"][\"choices\"]:\n","                    if \"para\" in option:\n","                        para_context = option[\"para\"].replace(\"_\", \"\")\n","                    else:\n","                        para_context = data_raw[\"para\"].replace(\"_\", \"\")\n","                    contexts.append(para_context)\n","            elif para_type == \"concat_choices\":\n","                para_context = \" \".join(\n","                    [ch[\"text\"].strip() for ch in data_raw[\"question\"][\"choices\"]]\n","                )\n","                for oi in range(len(data_raw[\"question\"][\"choices\"])):\n","                    contexts.append(para_context)\n","            elif para_type == \"ignore\":\n","                contexts += [\"\"] * len(data_raw[\"question\"][\"choices\"])\n","            else:\n","                raise Exception('para_type \"{0}\" is not supported!'.format(para_type))\n","\n","            return contexts\n","\n","        # There are two types of labels. They should be normalized\n","        def normalize(truth):\n","            if truth in \"ABCDE\":\n","                return ord(truth) - ord(\"A\")\n","            elif truth in \"12345\":\n","                return int(truth) - 1\n","            else:\n","                logger.info(\"truth ERROR! %s\", str(truth))\n","                return None\n","\n","        examples = []\n","        three_choice = 0\n","        four_choice = 0\n","        five_choice = 0\n","        other_choices = 0\n","        # we deleted example which has more than or less than four choices\n","        for line in tqdm(lines, desc=\"read arc data\"):\n","            data_raw = json.loads(line.strip(\"\\n\"))\n","            if len(data_raw[\"question\"][\"choices\"]) == 3:\n","                three_choice += 1\n","                # continue\n","            elif len(data_raw[\"question\"][\"choices\"]) == 5:\n","                five_choice += 1\n","                # continue\n","            elif len(data_raw[\"question\"][\"choices\"]) == 4:\n","                four_choice += 1\n","            else:\n","                other_choices += 1\n","                continue\n","\n","            truth = str(normalize(data_raw[\"answerKey\"]))\n","            if truth == \"None\":\n","                continue\n","\n","            assert truth != \"None\"\n","            question_choices = data_raw[\"question\"]\n","            question = question_choices[\"stem\"]\n","            id = data_raw[\"id\"]\n","            options = question_choices[\"choices\"]\n","\n","            contexts = get_contexts(data_raw, para_type)\n","            options_size = len(options)\n","            if options_size \u003c 5:\n","                pad_size = 5 - len(options)\n","                contexts += [\"\"] * pad_size\n","                options += [{\"text\": \"\"}] * pad_size\n","\n","            examples.append(\n","                InputExampleEXAMS(\n","                    example_id=id,\n","                    question=question,\n","                    contexts=contexts,\n","                    endings=[o[\"text\"] for o in options],\n","                    options_size=options_size,\n","                    label=truth,\n","                )\n","            )\n","\n","        if type == \"train\":\n","            assert len(examples) \u003e 1\n","            assert examples[0].label is not None\n","\n","        logger.info(\"len examples: %s}\", str(len(examples)))\n","        logger.info(\"Three choices: %s\", str(three_choice))\n","        logger.info(\"Five choices: %s\", str(five_choice))\n","        logger.info(\"Other choices: %s\", str(other_choices))\n","        logger.info(\"four choices: %s\", str(four_choice))\n","\n","        return examples\n","\n","\n","def convert_examples_to_features(\n","    examples: List[InputExample],\n","    label_list: List[str],\n","    max_length: int,\n","    tokenizer: PreTrainedTokenizer,\n","    pad_token_segment_id=0,\n","    pad_on_left=False,\n","    pad_token=0,\n","    mask_padding_with_zero=True,\n","    kb_masking=False,\n",") -\u003e List[InputFeatures]:\n","    \"\"\"\n","    Loads a data file into a list of `InputFeatures`\n","    \"\"\"\n","\n","    label_map = {label: i for i, label in enumerate(label_list)}\n","\n","    features = []\n","    for (ex_index, example) in tqdm(enumerate(examples), desc=\"convert examples to features\"):\n","        if ex_index % 10000 == 0:\n","            logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n","        choices_features = []\n","        for ending_idx, (context, ending) in enumerate(zip(example.contexts, example.endings)):\n","            if not kb_masking:\n","                text_a = context\n","                if example.question.find(\"_\") != -1:\n","                    # this is for cloze question\n","                    text_b = example.question.replace(\"_\", ending)\n","                else:\n","                    text_b = example.question + \" \" + ending\n","            else:\n","                # Hack for RoBERTa, this is a work-around for empty texts.\n","                text_b = ending if ending != \"\" else \"empty\"\n","                text_a = example.question + \" \" + context\n","\n","            inputs = tokenizer.encode_plus(\n","                text_a,\n","                text_b,\n","                add_special_tokens=True,\n","                max_length=max_length,\n","                return_token_type_ids=True,\n","            )\n","            if \"num_truncated_tokens\" in inputs and inputs[\"num_truncated_tokens\"] \u003e 0:\n","                logger.info(\n","                    \"Attention! you are cropping tokens (swag task is ok). \"\n","                    \"If you are training ARC and RACE and you are poping question + options,\"\n","                    \"you need to try to use a bigger max seq length!\"\n","                )\n","\n","            input_ids, token_type_ids = inputs[\"input_ids\"], inputs[\"token_type_ids\"]\n","\n","            # The mask has 1 for real tokens and 0 for padding tokens. Only real\n","            # tokens are attended to.\n","            attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n","\n","            # Zero-pad up to the sequence length.\n","            padding_length = max_length - len(input_ids)\n","            if pad_on_left:\n","                input_ids = ([pad_token] * padding_length) + input_ids\n","                attention_mask = (\n","                    [0 if mask_padding_with_zero else 1] * padding_length\n","                ) + attention_mask\n","                token_type_ids = ([pad_token_segment_id] * padding_length) + token_type_ids\n","            else:\n","                input_ids = input_ids + ([pad_token] * padding_length)\n","                attention_mask = attention_mask + (\n","                    [0 if mask_padding_with_zero else 1] * padding_length\n","                )\n","                token_type_ids = token_type_ids + ([pad_token_segment_id] * padding_length)\n","\n","            assert len(input_ids) == max_length\n","            assert len(attention_mask) == max_length\n","            assert len(token_type_ids) == max_length\n","            choices_features.append((input_ids, attention_mask, token_type_ids))\n","\n","        label = label_map[example.label]\n","\n","        if ex_index \u003c 2:\n","            logger.info(\"*** Example ***\")\n","            logger.info(\"race_id: {}\".format(example.example_id))\n","            for choice_idx, (input_ids, attention_mask, token_type_ids) in enumerate(\n","                choices_features\n","            ):\n","                logger.info(\"choice: {}\".format(choice_idx))\n","                logger.info(\"input_ids: {}\".format(\" \".join(map(str, input_ids))))\n","                logger.info(\"attention_mask: {}\".format(\" \".join(map(str, attention_mask))))\n","                logger.info(\"token_type_ids: {}\".format(\" \".join(map(str, token_type_ids))))\n","                logger.info(\"label: {}\".format(label))\n","\n","        answers_mask = np.ones(len(choices_features), np.int32)\n","        if isinstance(example, InputExampleEXAMS):\n","            answers_mask[example.options_size :] = 0\n","\n","        features.append(\n","            InputFeatures(\n","                example_id=example.example_id,\n","                choices_features=choices_features,\n","                answers_mask=answers_mask,\n","                label=label,\n","            )\n","        )\n","\n","    return features\n","\n","\n","processors = {\n","    \"race\": RaceProcessor,\n","    \"swag\": SwagProcessor,\n","    \"arc\": ArcProcessor,\n","    \"exams\": ExamsProcessor,\n","}\n","\n","\n","MULTIPLE_CHOICE_TASKS_NUM_LABELS = {\"race\", 4, \"swag\", 4, \"arc\", 4, \"exams\", 5}"]},{"cell_type":"markdown","metadata":{"id":"yoiGjVs5kuwD"},"source":["# Module"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1675438205303,"user":{"displayName":"Sơn Lưu Thanh","userId":"09824077883060402796"},"user_tz":-420},"id":"VG-MZ9JcbuBo"},"outputs":[],"source":["import argparse\n","import glob\n","import json\n","import logging\n","import os\n","import random\n","\n","import numpy as np\n","import torch\n","# from examsqa.models_multiple_choice import (\n","#     BertLMForMultipleChoice,\n","#     RobertaLMForMultipleChoice,\n","#     XLMRobertaLMForMultipleChoice,\n","# )\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n","from torch.utils.data.distributed import DistributedSampler\n","from tqdm import tqdm, trange\n","from transformers import (\n","    WEIGHTS_NAME,\n","    AdamW,\n","    BertConfig,\n","    BertForMultipleChoice,\n","    BertTokenizer,\n","    RobertaConfig,\n","    RobertaForMultipleChoice,\n","    RobertaTokenizer,\n","    XLMRobertaConfig,\n","    XLMRobertaForMultipleChoice,\n","    XLMRobertaTokenizer,\n","    XLNetConfig,\n","    XLNetForMultipleChoice,\n","    XLNetTokenizer,\n","    get_linear_schedule_with_warmup,\n",")\n","\n","from transformers import BERT_PRETRAINED_CONFIG_ARCHIVE_MAP, DISTILBERT_PRETRAINED_CONFIG_ARCHIVE_MAP, XLM_ROBERTA_PRETRAINED_CONFIG_ARCHIVE_MAP\n","\n","from transformers import DistilBertConfig, DistilBertForMultipleChoice, DistilBertTokenizer\n","\n","# from utils_multiple_choice import convert_examples_to_features, processors\n","\n","try:\n","    from torch.utils.tensorboard import SummaryWriter\n","except ImportError:\n","    from tensorboardX import SummaryWriter\n","\n","\n","logger = logging.getLogger(__name__)\n","\n","# ALL_MODELS = sum(\n","#     (\n","#         tuple(conf.pretrained_config_archive_map.keys())\n","#         for conf in (BertConfig, XLNetConfig, RobertaConfig)\n","#     ),\n","#     (),\n","# )\n","\n","\n","\n","ALL_MODELS = tuple(DISTILBERT_PRETRAINED_CONFIG_ARCHIVE_MAP.keys()) + tuple(BERT_PRETRAINED_CONFIG_ARCHIVE_MAP.keys()) + tuple(XLM_ROBERTA_PRETRAINED_CONFIG_ARCHIVE_MAP.keys())\n","\n","MODEL_CLASSES = {\n","    \"bert\": (BertConfig, BertForMultipleChoice, BertTokenizer),\n","    \"xlnet\": (XLNetConfig, XLNetForMultipleChoice, XLNetTokenizer),\n","    \"roberta\": (RobertaConfig, RobertaForMultipleChoice, RobertaTokenizer),\n","    \"xlm-roberta\": (XLMRobertaConfig, XLMRobertaForMultipleChoice, XLMRobertaTokenizer),\n","    \"bert-kb\": (BertConfig, BertLMForMultipleChoice, BertTokenizer),\n","    \"roberta-kb\": (RobertaConfig, RobertaLMForMultipleChoice, RobertaTokenizer),\n","    \"xlm-roberta-kb\": (XLMRobertaConfig, XLMRobertaLMForMultipleChoice, XLMRobertaTokenizer),\n","    \"distilbert\": (DistilBertConfig, DistilBertLMForMultipleChoice, DistilBertTokenizer)\n","}\n","\n","\n","def select_field(features, field):\n","    return [[choice[field] for choice in feature.choices_features] for feature in features]\n","\n","\n","def simple_accuracy(preds, labels):\n","    return (preds == labels).mean()\n","\n","\n","def set_seed(args):\n","    random.seed(args.seed)\n","    np.random.seed(args.seed)\n","    torch.manual_seed(args.seed)\n","    if args.n_gpu \u003e 0:\n","        torch.cuda.manual_seed_all(args.seed)\n","\n","\n","def train(args, train_dataset, model, tokenizer):\n","    \"\"\" Train the model \"\"\"\n","    if args.local_rank in [-1, 0]:\n","        # tb_writer = SummaryWriter(logdir=args.tb_log_dir)\n","        tb_writer = SummaryWriter()\n","        pass\n","\n","    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n","    train_sampler = (\n","        RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n","    )\n","    train_dataloader = DataLoader(\n","        train_dataset, sampler=train_sampler, batch_size=args.train_batch_size\n","    )\n","\n","    if args.max_steps \u003e 0:\n","        t_total = args.max_steps\n","        args.num_train_epochs = (\n","            args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n","        )\n","    else:\n","        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n","\n","    # Prepare optimizer and schedule (linear warmup and decay)\n","    no_decay = [\"bias\", \"LayerNorm.weight\"]\n","    optimizer_grouped_parameters = [\n","        {\n","            \"params\": [\n","                p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)\n","            ],\n","            \"weight_decay\": args.weight_decay,\n","        },\n","        {\n","            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n","            \"weight_decay\": 0.0,\n","        },\n","    ]\n","    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer,\n","        num_warmup_steps=int(args.warmup_proportion * t_total),\n","        num_training_steps=t_total,\n","    )\n","    if args.fp16:\n","        try:\n","            from apex import amp\n","        except ImportError:\n","            raise ImportError(\n","                \"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\"\n","            )\n","        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n","\n","    # multi-gpu training (should be after apex fp16 initialization)\n","    if args.n_gpu \u003e 1:\n","        model = torch.nn.DataParallel(model)\n","\n","    # Distributed training (should be after apex fp16 initialization)\n","    if args.local_rank != -1:\n","        model = torch.nn.parallel.DistributedDataParallel(\n","            model,\n","            device_ids=[args.local_rank],\n","            output_device=args.local_rank,\n","            find_unused_parameters=True,\n","        )\n","\n","    # Train!\n","    logger.info(\"***** Running training *****\")\n","    logger.info(\"  Num examples = %d\", len(train_dataset))\n","    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n","    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n","    logger.info(\n","        \"  Total train batch size (w. parallel, distributed \u0026 accumulation) = %d\",\n","        args.train_batch_size\n","        * args.gradient_accumulation_steps\n","        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n","    )\n","    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n","    logger.info(\"  Total optimization steps = %d\", t_total)\n","\n","    global_step = 0\n","    tr_loss, logging_loss, tmp_loss = 0.0, 0.0, 0.0\n","    best_dev_acc = 0.0\n","    best_steps = 0\n","    model.zero_grad()\n","    train_iterator = trange(\n","        int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n","    )\n","    set_seed(args)  # Added here for reproductibility\n","    for _ in train_iterator:\n","        epoch_iterator = tqdm(\n","            train_dataloader, desc=\"Iteration\", disable=True\n","        )\n","        for step, batch in enumerate(epoch_iterator):\n","            model.train()\n","            batch = tuple(t.to(args.device) for t in batch)\n","            inputs = {\n","                \"input_ids\": batch[0],\n","                \"attention_mask\": batch[1],\n","                \"token_type_ids\": batch[2]\n","                if args.model_type in [\"bert\", \"xlnet\"]\n","                else None,  # XLM don't use segment_ids\n","            }\n","            labels = batch[3]\n","\n","            outputs = model(**inputs)  # model outputs are always tuple in transformers (see doc)\n","\n","            logits = outputs[0].masked_fill(batch[4] == 0, -10000)\n","\n","            loss_fct = torch.nn.CrossEntropyLoss()\n","            loss = loss_fct(logits, labels)\n","\n","            if args.n_gpu \u003e 1:\n","                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n","            if args.gradient_accumulation_steps \u003e 1:\n","                loss = loss / args.gradient_accumulation_steps\n","\n","            if args.fp16:\n","                with amp.scale_loss(loss, optimizer) as scaled_loss:\n","                    scaled_loss.backward()\n","                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n","            else:\n","                loss.backward()\n","                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n","\n","            tr_loss += loss.item()\n","            if (step + 1) % args.gradient_accumulation_steps == 0:\n","\n","                optimizer.step()\n","                scheduler.step()  # Update learning rate schedule\n","                model.zero_grad()\n","                global_step += 1\n","                tb_writer.add_scalar(\"train_{}\".format(\"loss_\"), tr_loss - tmp_loss, global_step)\n","                tmp_loss = tr_loss\n","\n","                if (\n","                    args.local_rank in [-1, 0]\n","                    and args.logging_steps \u003e 0\n","                    and global_step % args.logging_steps == 0\n","                ):\n","                    # Log metrics\n","                    if (\n","                        args.local_rank == -1 and args.evaluate_during_training\n","                    ):  # Only evaluate when single GPU otherwise metrics may not average well\n","                        results = evaluate(args, model, tokenizer)\n","                        for key, value in results.items():\n","                            tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n","                        if results[\"eval_acc\"] \u003e best_dev_acc:\n","                            best_dev_acc = results[\"eval_acc\"]\n","                            best_steps = global_step\n","                            if args.do_test:\n","                                results_test = evaluate(args, model, tokenizer, test=True)\n","                                for key, value in results_test.items():\n","                                    tb_writer.add_scalar(\"test_{}\".format(key), value, global_step)\n","                                logger.info(\n","                                    \"test acc: %s, loss: %s, global steps: %s\",\n","                                    str(results_test[\"eval_acc\"]),\n","                                    str(results_test[\"eval_loss\"]),\n","                                    str(global_step),\n","                                )\n","                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n","                    tb_writer.add_scalar(\n","                        \"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step\n","                    )\n","                    logger.info(\n","                        \"Average loss: %s at global step: %s\",\n","                        str((tr_loss - logging_loss) / args.logging_steps),\n","                        str(global_step),\n","                    )\n","                    logging_loss = tr_loss\n","\n","                if (\n","                    args.local_rank in [-1, 0]\n","                    and args.save_steps \u003e 0\n","                    and global_step % args.save_steps == 0\n","                ):\n","                    # Save model checkpoint\n","                    output_dir = os.path.join(args.output_dir, \"checkpoint-{}\".format(global_step))\n","                    if not os.path.exists(output_dir):\n","                        os.makedirs(output_dir)\n","                    model_to_save = (\n","                        model.module if hasattr(model, \"module\") else model\n","                    )  # Take care of distributed/parallel training\n","                    model_to_save.save_pretrained(output_dir)\n","                    tokenizer.save_vocabulary(output_dir)\n","                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n","                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n","\n","            if args.max_steps \u003e 0 and global_step \u003e args.max_steps:\n","                epoch_iterator.close()\n","                break\n","        if args.max_steps \u003e 0 and global_step \u003e args.max_steps:\n","            train_iterator.close()\n","            break\n","\n","    if args.local_rank in [-1, 0]:\n","        tb_writer.close()\n","\n","    return global_step, tr_loss / global_step, best_steps\n","\n","\n","def evaluate(args, model, tokenizer, prefix=\"\", test=False):\n","    eval_task_names = (args.task_name,)\n","    eval_outputs_dirs = (args.output_dir,)\n","\n","    results = {}\n","    for eval_task, eval_output_dir in zip(eval_task_names, eval_outputs_dirs):\n","        eval_dataset, all_example_ids = load_and_cache_examples(\n","            args, eval_task, tokenizer, evaluate=not test, test=test\n","        )\n","\n","        if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n","            os.makedirs(eval_output_dir)\n","\n","        args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n","        # Note that DistributedSampler samples randomly\n","        eval_sampler = SequentialSampler(eval_dataset)\n","        eval_dataloader = DataLoader(\n","            eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size\n","        )\n","\n","        # multi-gpu evaluate\n","        if args.n_gpu \u003e 1:\n","            model = torch.nn.DataParallel(model)\n","\n","        # Eval!\n","        logger.info(\"***** Running evaluation {} *****\".format(prefix))\n","        logger.info(\"  Num examples = %d\", len(eval_dataset))\n","        logger.info(\"  Batch size = %d\", args.eval_batch_size)\n","        eval_loss = 0.0\n","        nb_eval_steps = 0\n","        preds = None\n","        preds_softmax = None\n","        out_label_ids = None\n","        for batch in tqdm(eval_dataloader, desc=\"Evaluating\", disable = True):\n","            model.eval()\n","            batch = tuple(t.to(args.device) for t in batch)\n","\n","            with torch.no_grad():\n","                inputs = {\n","                    \"input_ids\": batch[0],\n","                    \"attention_mask\": batch[1],\n","                    \"token_type_ids\": batch[2]\n","                    if args.model_type in [\"bert\", \"xlnet\"]\n","                    else None,  # XLM don't use segment_ids\n","                    \"labels\": batch[3],\n","                }\n","\n","                if args.model_type in [\"bert-kb\", \"roberta-kb\", \"xlm-roberta-kb\"]:\n","\n","                    masked_input_ids = batch[0].detach().cpu().numpy()\n","                    masked_input_ids = masked_input_ids.reshape(-1, masked_input_ids.shape[-1])\n","                    masked_positions = np.zeros_like(masked_input_ids, dtype=np.long)\n","                    attention_mask = batch[1].detach().cpu().numpy()\n","                    attention_mask = attention_mask.reshape(-1, attention_mask.shape[-1])\n","                    for i in range(len(masked_input_ids)):\n","                        sp = [\n","                            i\n","                            for i, x in enumerate(masked_input_ids[i])\n","                            if x == tokenizer.sep_token_id\n","                        ]\n","                        sep_start, sep_end = sp[-2:]\n","                        masked_input_ids[i, sep_start + 1 : sep_end] = tokenizer.mask_token_id\n","                        masked_positions[i, sep_start + 1 : sep_end] = 1\n","                        attention_mask[i, sp[:-1]] = 0\n","\n","                    input_shape = inputs[\"input_ids\"].shape\n","                    masked_input_ids = masked_input_ids.reshape(input_shape)\n","                    masked_positions = masked_positions.reshape(input_shape)\n","                    attention_mask = attention_mask.reshape(input_shape)\n","                    inputs[\"masked_input_ids\"] = torch.LongTensor(masked_input_ids).to(args.device)\n","                    inputs[\"masked_positions\"] = torch.LongTensor(masked_positions).to(args.device)\n","                    inputs[\"attention_mask\"] = torch.LongTensor(attention_mask).to(args.device)\n","\n","                outputs = model(**inputs)\n","                tmp_eval_loss, logits = outputs[:2]\n","                logits = logits.masked_fill(batch[4] == 0, -10000)\n","\n","                eval_loss += tmp_eval_loss.mean().item()\n","            nb_eval_steps += 1\n","            if preds is None:\n","                preds = logits.detach().cpu().numpy()\n","                preds_softmax = torch.softmax(logits, -1).detach().cpu().numpy()\n","                out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n","            else:\n","                preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n","                preds_softmax = np.append(\n","                    preds_softmax, torch.softmax(logits, -1).detach().cpu().numpy(), axis=0\n","                )\n","                out_label_ids = np.append(\n","                    out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0\n","                )\n","\n","        eval_loss = eval_loss / nb_eval_steps\n","\n","        preds_softmax = [[round(x.tolist(), 4) for x in p] for p in preds_softmax]\n","        per_id_pred = dict(zip(all_example_ids, preds_softmax))\n","\n","        preds = np.argmax(preds, axis=1)\n","        acc = simple_accuracy(preds, out_label_ids)\n","        result = {\"eval_acc\": acc, \"eval_loss\": eval_loss}\n","        results.update(result)\n","\n","        output_eval_file = os.path.join(\n","            eval_output_dir, \"is_test_\" + str(test).lower() + \"_eval_results.txt\"\n","        )\n","\n","        with open(output_eval_file, \"w\") as writer:\n","            logger.info(\"***** Eval results {} *****\".format(str(prefix) + \" is test:\" + str(test)))\n","            writer.write(\"model           =%s\\n\" % str(args.model_name_or_path))\n","            writer.write(\n","                \"total batch size=%d\\n\"\n","                % (\n","                    args.per_gpu_train_batch_size\n","                    * args.gradient_accumulation_steps\n","                    * (torch.distributed.get_world_size() if args.local_rank != -1 else 1)\n","                )\n","            )\n","            writer.write(\"train num epochs=%d\\n\" % args.num_train_epochs)\n","            writer.write(\"fp16            =%s\\n\" % args.fp16)\n","            writer.write(\"max seq length  =%d\\n\" % args.max_seq_length)\n","            for key in sorted(result.keys()):\n","                logger.info(\"  %s = %s\", key, str(result[key]))\n","                writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n","\n","        if args.output_predictions:\n","            output_pred_file = os.path.join(\n","                eval_output_dir, \"predictions_\" + str(test).lower() + \"_eval_results.json\"\n","            )\n","\n","            with open(output_pred_file, \"w\") as writer:\n","                json.dump(per_id_pred, writer, indent=4)\n","                logger.info(\"Saved {0}\".format(output_pred_file))\n","\n","\n","            output_pred_file2 = os.path.join(\n","                eval_output_dir, \"predictions_\" + str(test).lower() + \"_eval_results_labels.json\"\n","            )\n","\n","            with open(output_pred_file2, \"w\") as writer:\n","                json.dump(out_label_ids.tolist(), writer, indent=4)\n","                logger.info(\"Saved {0}\".format(output_pred_file2))\n","\n","    return results\n","\n","\n","def load_and_cache_examples(args, task, tokenizer, evaluate=False, test=False):\n","    if args.local_rank not in [-1, 0]:\n","        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n","\n","    processor = (\n","        processors[args.task_name]()\n","        if args.task_name not in [\"arc\", \"exams\"]\n","        else processors[args.task_name](para_type=args.para_type)\n","    )\n","    # Load data features from cache or dataset file\n","    if evaluate:\n","        cached_mode = \"dev\"\n","    elif test:\n","        cached_mode = \"test\"\n","    else:\n","        cached_mode = \"train\"\n","    assert not (evaluate and test)\n","    cached_features_file = os.path.join(\n","        args.data_dir,\n","        \"cached_{}_{}_{}_{}\".format(\n","            cached_mode,\n","            \"__\".join(list(filter(None, args.model_name_or_path.split(\"/\")))[-2:]),\n","            str(args.max_seq_length),\n","            str(task),\n","        ),\n","    )\n","    if os.path.exists(cached_features_file) and not args.overwrite_cache:\n","        logger.info(\"Loading features from cached file %s\", cached_features_file)\n","        features = torch.load(cached_features_file)\n","    else:\n","        logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n","        label_list = processor.get_labels()\n","        if evaluate:\n","            examples = processor.get_dev_examples(args.data_dir)\n","        elif test:\n","            examples = processor.get_test_examples(args.data_dir)\n","        else:\n","            examples = processor.get_train_examples(args.data_dir)\n","        logger.info(\"Training number: %s\", str(len(examples)))\n","        features = convert_examples_to_features(\n","            examples,\n","            label_list,\n","            args.max_seq_length,\n","            tokenizer,\n","            pad_on_left=bool(args.model_type in [\"xlnet\"]),  # pad on the left for xlnet\n","            pad_token_segment_id=tokenizer.pad_token_type_id,\n","            kb_masking=bool(args.model_type in [\"bert-kb\", \"roberta-kb\", \"xlm-roberta-kb\"]),\n","        )\n","        if args.local_rank in [-1, 0]:\n","            logger.info(\"Saving features into cached file %s\", cached_features_file)\n","            torch.save(features, cached_features_file)\n","\n","    if args.local_rank == 0:\n","        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n","\n","    # Convert to Tensors and build dataset\n","    all_input_ids = torch.tensor(select_field(features, \"input_ids\"), dtype=torch.long)\n","    all_input_mask = torch.tensor(select_field(features, \"input_mask\"), dtype=torch.long)\n","    all_segment_ids = torch.tensor(select_field(features, \"segment_ids\"), dtype=torch.long)\n","    all_label_ids = torch.tensor([f.label for f in features], dtype=torch.long)\n","    all_answers_masks = torch.tensor([f.answers_mask for f in features], dtype=torch.long)\n","\n","    dataset = TensorDataset(\n","        all_input_ids, all_input_mask, all_segment_ids, all_label_ids, all_answers_masks\n","    )\n","\n","    if evaluate or test:\n","        all_example_ids = [x.example_id for x in features]\n","\n","        return dataset, all_example_ids\n","\n","    return dataset\n"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1675438205304,"user":{"displayName":"Sơn Lưu Thanh","userId":"09824077883060402796"},"user_tz":-420},"id":"5HVqCbwDo5Md"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1675438205304,"user":{"displayName":"Sơn Lưu Thanh","userId":"09824077883060402796"},"user_tz":-420},"id":"5MMCZ6MynS1d","outputId":"57ac498a-4c17-45ef-804f-23ef3559ad0e"},"outputs":[{"name":"stdout","output_type":"stream","text":["('distilbert-base-uncased', 'distilbert-base-uncased-distilled-squad', 'distilbert-base-cased', 'distilbert-base-cased-distilled-squad', 'distilbert-base-german-cased', 'distilbert-base-multilingual-cased', 'distilbert-base-uncased-finetuned-sst-2-english', 'bert-base-uncased', 'bert-large-uncased', 'bert-base-cased', 'bert-large-cased', 'bert-base-multilingual-uncased', 'bert-base-multilingual-cased', 'bert-base-chinese', 'bert-base-german-cased', 'bert-large-uncased-whole-word-masking', 'bert-large-cased-whole-word-masking', 'bert-large-uncased-whole-word-masking-finetuned-squad', 'bert-large-cased-whole-word-masking-finetuned-squad', 'bert-base-cased-finetuned-mrpc', 'bert-base-german-dbmdz-cased', 'bert-base-german-dbmdz-uncased', 'cl-tohoku/bert-base-japanese', 'cl-tohoku/bert-base-japanese-whole-word-masking', 'cl-tohoku/bert-base-japanese-char', 'cl-tohoku/bert-base-japanese-char-whole-word-masking', 'TurkuNLP/bert-base-finnish-cased-v1', 'TurkuNLP/bert-base-finnish-uncased-v1', 'wietsedv/bert-base-dutch-cased', 'xlm-roberta-base', 'xlm-roberta-large', 'xlm-roberta-large-finetuned-conll02-dutch', 'xlm-roberta-large-finetuned-conll02-spanish', 'xlm-roberta-large-finetuned-conll03-english', 'xlm-roberta-large-finetuned-conll03-german')\n"]}],"source":["print(ALL_MODELS)"]},{"cell_type":"markdown","metadata":{"id":"fyV2VX7Vk1RA"},"source":["# Main"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1675438270991,"user":{"displayName":"Sơn Lưu Thanh","userId":"09824077883060402796"},"user_tz":-420},"id":"daHi3mJlTi69"},"outputs":[],"source":["OUTPUT_DIR = 'drive/My Drive/CODE/ViMMRC/models/bert4news_vimmrc2_1/'"]},{"cell_type":"markdown","metadata":{"id":"42dvg6DqZPg2"},"source":["## Train "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"THNDibdjb9we"},"outputs":[],"source":["import logging, sys\n","logging.disable(sys.maxsize)\n","\n","\n","def main():\n","    parser = argparse.ArgumentParser()\n","\n","    # Required parameters\n","    parser.add_argument(\n","        \"--data_dir\",\n","        default=\"drive/My Drive/CODE/ViMMRC/dataset/race_ver_2\",\n","        type=str,\n","        help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\",\n","    )\n","\n","    parser.add_argument(\n","        \"--model_type\",\n","        default=\"bert\",\n","        type=str,\n","        help=\"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys()),\n","    )\n","    parser.add_argument(\n","        \"--model_name_or_path\",\n","        default=\"NlpHUST/vibert4news-base-cased\",\n","        type=str,\n","        help=\"Path to pre-trained model or shortcut name selected in the list: \"\n","        + \", \".join(ALL_MODELS),\n","    )\n","    \n","    parser.add_argument(\n","        \"--task_name\",\n","        default=\"race\",\n","        type=str,\n","        help=\"The name of the task to train selected in the list: \" + \", \".join(processors.keys()),\n","    )\n","    parser.add_argument(\n","        \"--para_type\",\n","        default=\"per_choice\",\n","        type=str,\n","        choices=[\"per_choice\", \"concat_choices\", \"ignore\"],\n","        help=\"Paragraph building strategy for ARC (default: %(default)s)\",\n","    )\n","    parser.add_argument(\n","        \"--output_predictions\",\n","        default=True,\n","        type=bool,\n","        help=\"Whether to export the predictions from the eval step.\",\n","    )\n","    parser.add_argument(\n","        \"--output_dir\",\n","        default=OUTPUT_DIR,\n","        type=str,\n","        help=\"The output directory where the model predictions and checkpoints will be written.\",\n","    )\n","    parser.add_argument(\n","        \"--freeze_embeddings\", default=False, action=\"store_true\", help=\"Whether to freeze the embeeding layer.\",\n","    )\n","    parser.add_argument(\n","        \"--freeze_layers\", nargs=\"*\", help=\"Whether to freeze the embeeding layer.\",\n","    )\n","\n","    # Other parameters\n","    parser.add_argument(\n","        \"--tb_log_dir\",\n","        default=\"\",\n","        type=str,\n","        help=\"Tensorboard log dir for the current experiment\",\n","    )\n","\n","    parser.add_argument(\n","        \"--config_name\",\n","        default=\"\",\n","        type=str,\n","        help=\"Pretrained config name or path if not the same as model_name\",\n","    )\n","    parser.add_argument(\n","        \"--tokenizer_name\",\n","        default=\"\",\n","        type=str,\n","        help=\"Pretrained tokenizer name or path if not the same as model_name\",\n","    )\n","    parser.add_argument(\n","        \"--cache_dir\",\n","        default=OUTPUT_DIR,\n","        type=str,\n","        help=\"Where do you want to store the pre-trained models downloaded from s3\",\n","    )\n","    parser.add_argument(\n","        \"--max_seq_length\",\n","        default=300,\n","        type=int,\n","        help=\"The maximum total input sequence length after tokenization. Sequences longer \"\n","        \"than this will be truncated, sequences shorter will be padded.\",\n","    )\n","    parser.add_argument(\"--do_train\", default = True, action=\"store_true\", help=\"Whether to run training.\")\n","    parser.add_argument(\n","        \"--do_eval\", default = True, action=\"store_true\", help=\"Whether to run eval on the dev set.\"\n","    )\n","    parser.add_argument(\n","        \"--do_test\", default = False, action=\"store_true\", help=\"Whether to run test on the test set\"\n","    )\n","    parser.add_argument(\n","        \"--evaluate_during_training\",\n","        action=\"store_true\",\n","        default = True,\n","        help=\"Run evaluation during training at each logging step.\",\n","    )\n","    parser.add_argument(\n","        \"--do_lower_case\",\n","        action=\"store_true\",\n","        default = False,\n","        help=\"Set this flag if you are using an uncased model.\",\n","    )\n","\n","    parser.add_argument(\n","        \"--per_gpu_train_batch_size\",\n","        default=4,\n","        type=int,\n","        help=\"Batch size per GPU/CPU for training.\",\n","    )\n","    parser.add_argument(\n","        \"--per_gpu_eval_batch_size\",\n","        default=4,\n","        type=int,\n","        help=\"Batch size per GPU/CPU for evaluation.\",\n","    )\n","    parser.add_argument(\n","        \"--gradient_accumulation_steps\",\n","        type=int,\n","        default=4,\n","        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n","    )\n","    parser.add_argument(\n","        \"--learning_rate\", default=1e-5, type=float, help=\"The initial learning rate for Adam.\"\n","    )\n","    parser.add_argument(\n","        \"--weight_decay\", default=0.0, type=float, help=\"Weight deay if we apply some.\"\n","    )\n","    parser.add_argument(\n","        \"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\"\n","    )\n","    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n","    parser.add_argument(\n","        \"--num_train_epochs\",\n","        default=50,\n","        type=float,\n","        help=\"Total number of training epochs to perform.\",\n","    )\n","    parser.add_argument(\n","        \"--max_steps\",\n","        default=-1,\n","        type=int,\n","        help=\"If \u003e 0: set total number of training steps to perform. Override num_train_epochs.\",\n","    )\n","    parser.add_argument(\n","        \"--warmup_proportion\", default=0.1, type=float, help=\"Linear warmup over warmup_proportion.\"\n","    )\n","\n","    parser.add_argument(\"--logging_steps\", type=int, default=100, help=\"Log every X updates steps.\")\n","    parser.add_argument(\n","        \"--save_steps\", type=int, default=500, help=\"Save checkpoint every X updates steps.\"\n","    )\n","    parser.add_argument(\n","        \"--eval_all_checkpoints\",\n","        default = True,\n","        action=\"store_true\",\n","        help=\"Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number\",\n","    )\n","    parser.add_argument(\"--no_cuda\", default = False, action=\"store_true\", help=\"Avoid using CUDA when available\")\n","    parser.add_argument(\n","        \"--overwrite_output_dir\",\n","        default = True,\n","        action=\"store_true\",\n","        help=\"Overwrite the content of the output directory\",\n","    )\n","    parser.add_argument(\n","        \"--overwrite_cache\",\n","        default = True,\n","        action=\"store_true\",\n","        help=\"Overwrite the cached training and evaluation sets\",\n","    )\n","    parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization\")\n","\n","    parser.add_argument(\n","        \"--fp16\",\n","        action=\"store_true\",\n","        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n","    )\n","    parser.add_argument(\n","        \"--fp16_opt_level\",\n","        type=str,\n","        default=\"O1\",\n","        help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n","        \"See details at https://nvidia.github.io/apex/amp.html\",\n","    )\n","    parser.add_argument(\n","        \"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\"\n","    )\n","    parser.add_argument(\"--server_ip\", type=str, default=\"\", help=\"For distant debugging.\")\n","    parser.add_argument(\"--server_port\", type=str, default=\"\", help=\"For distant debugging.\")\n","\n","    parser.add_argument('-f')\n","\n","    args = parser.parse_args()\n","\n","    options_print = \"\"\n","    logger.info(\"Arg Options - input:\")\n","    for arg in vars(args):\n","        options_print += \"opt: %s=%s\\r\\n\" % (arg, getattr(args, arg))\n","    logger.info(options_print)\n","\n","    if (\n","        os.path.exists(args.output_dir)\n","        and os.listdir(args.output_dir)\n","        and args.do_train\n","        and not args.overwrite_output_dir\n","    ):\n","        raise ValueError(\n","            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n","                args.output_dir\n","            )\n","        )\n","\n","    # Setup distant debugging if needed\n","    if args.server_ip and args.server_port:\n","        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n","        import ptvsd\n","\n","        print(\"Waiting for debugger attach\")\n","        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n","        ptvsd.wait_for_attach()\n","\n","    # Setup CUDA, GPU \u0026 distributed training\n","    if args.local_rank == -1 or args.no_cuda:\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n","        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n","    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n","        torch.cuda.set_device(args.local_rank)\n","        device = torch.device(\"cuda\", args.local_rank)\n","        torch.distributed.init_process_group(backend=\"nccl\")\n","        args.n_gpu = 1\n","    args.device = device\n","\n","    # Setup logging\n","    logging.basicConfig(\n","        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n","        datefmt=\"%m/%d/%Y %H:%M:%S\",\n","        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n","    )\n","    logger.warning(\n","        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n","        args.local_rank,\n","        device,\n","        args.n_gpu,\n","        bool(args.local_rank != -1),\n","        args.fp16,\n","    )\n","\n","    # Set seed\n","    set_seed(args)\n","\n","    # Prepare GLUE task\n","    args.task_name = args.task_name.lower()\n","    if args.task_name not in processors:\n","        raise ValueError(\"Task not found: %s\" % (args.task_name))\n","    processor = (\n","        processors[args.task_name]()\n","        if args.task_name not in [\"arc\", \"exams\",]\n","        else processors[args.task_name](para_type=args.para_type)\n","    )\n","    label_list = processor.get_labels()\n","    num_labels = len(label_list)\n","\n","    # Load pretrained model and tokenizer\n","    if args.local_rank not in [-1, 0]:\n","        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model \u0026 vocab\n","\n","    args.model_type = args.model_type.lower()\n","    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n","    config = config_class.from_pretrained(\n","        args.config_name if args.config_name else args.model_name_or_path,\n","        num_labels=num_labels,\n","        early_stopping = True,\n","        finetuning_task=args.task_name,\n","        cache_dir=args.cache_dir if args.cache_dir else None,\n","    )\n","    tokenizer = tokenizer_class.from_pretrained(\n","        args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,\n","        do_lower_case=args.do_lower_case,\n","        cache_dir=args.cache_dir if args.cache_dir else None,\n","    )\n","    model = model_class.from_pretrained(\n","        args.model_name_or_path,\n","        from_tf=bool(\".ckpt\" in args.model_name_or_path),\n","        config=config,\n","        cache_dir=args.cache_dir if args.cache_dir else None,\n","    )\n","\n","    if args.local_rank == 0:\n","        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model \u0026 vocab\n","\n","    options_print = \"\"\n","    logging.info(\"Arg Options:\")\n","    for arg in vars(args):\n","        options_print += \"opt: %s=%s\\r\\n\" % (arg, getattr(args, arg))\n","    logging.info(options_print)\n","\n","    model.to(args.device)\n","\n","    def get_model_base_obj(model, model_type):\n","        if model_type == \"bert\":\n","            return model.bert\n","        elif model_type == \"xlm-roberta\" or model_type == \"roberta\":\n","            return model.roberta\n","        elif model_type == \"distilbert\":\n","            return model.distilbert    \n","        else:\n","            raise ValueError(\"model_type='{0}' is not supported!\")\n","\n","    if args.freeze_embeddings:\n","        for param in list(get_model_base_obj(model, args.model_type).embeddings.parameters()):\n","            param.requires_grad = False\n","        logger.info(\"Froze Embedding Layer\")\n","\n","    # freeze_layers is a string \"1,2,3\" representing layer number\n","    if args.freeze_layers:\n","        layer_indexes = [int(x) for x in args.freeze_layers]\n","        for layer_idx in layer_indexes:\n","            for param in list(\n","                get_model_base_obj(model, args.model_type).encoder.layer[layer_idx].parameters()\n","            ):\n","                param.requires_grad = False\n","            logger.info(\"Froze Layer: %s\", layer_idx)\n","\n","    logger.info(\"Training/evaluation parameters %s\", args)\n","    best_steps = 0\n","\n","    # Training\n","    if args.do_train:\n","        train_dataset = load_and_cache_examples(args, args.task_name, tokenizer, evaluate=False)\n","        global_step, tr_loss, best_steps = train(args, train_dataset, model, tokenizer)\n","        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n","\n","    # Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n","    if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n","        # Create output directory if needed\n","        if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n","            os.makedirs(args.output_dir)\n","\n","        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n","        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n","        # They can then be reloaded using `from_pretrained()`\n","        model_to_save = (\n","            model.module if hasattr(model, \"module\") else model\n","        )  # Take care of distributed/parallel training\n","        model_to_save.save_pretrained(args.output_dir)\n","        tokenizer.save_pretrained(args.output_dir)\n","\n","        # Good practice: save your training arguments together with the trained model\n","        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n","\n","        # Load a trained model and vocabulary that you have fine-tuned\n","        model = model_class.from_pretrained(args.output_dir)\n","        tokenizer = tokenizer_class.from_pretrained(args.output_dir)\n","        model.to(args.device)\n","\n","    # Evaluation\n","    results = {}\n","    if args.do_eval and args.local_rank in [-1, 0]:\n","        if not args.do_train:\n","            args.output_dir = args.model_name_or_path\n","        checkpoints = [args.output_dir]\n","        if args.eval_all_checkpoints:\n","            checkpoints = list(\n","                os.path.dirname(c)\n","                for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n","            )\n","            logging.getLogger(\"transformers.modeling_utils\").setLevel(\n","                logging.WARN\n","            )  # Reduce logging\n","        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n","        for checkpoint in checkpoints:\n","            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) \u003e 1 else \"\"\n","            prefix = checkpoint.split(\"/\")[-1] if checkpoint.find(\"checkpoint\") != -1 else \"\"\n","\n","            model = model_class.from_pretrained(checkpoint)\n","            model.to(args.device)\n","            result = evaluate(args, model, tokenizer, prefix=prefix)\n","            result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n","            results.update(result)\n","\n","    if args.do_test and args.local_rank in [-1, 0]:\n","        checkpoints = [args.model_name_or_path]\n","        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n","        for checkpoint in checkpoints:\n","            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) \u003e 1 else \"\"\n","            prefix = checkpoint.split(\"/\")[-1] if checkpoint.find(\"checkpoint\") != -1 else \"\"\n","\n","            model = model_class.from_pretrained(checkpoint)\n","            model.to(args.device)\n","            result = evaluate(args, model, tokenizer, prefix=prefix, test=True)\n","            result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n","            results.update(result)\n","    if best_steps:\n","        logger.info(\"best steps of eval acc is the following checkpoints: %s\", best_steps)\n","    return results\n","\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"markdown","metadata":{"id":"Ix8RCrSpZSoL"},"source":["## Test"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","output_embedded_package_id":"1_ZImqrYFFqMDAA33XeR4MXIJ0HAHDH1-"},"id":"z4Vc0QbZ1XY7","outputId":"b18b47f5-16de-491e-9f50-012c993bd0e5"},"outputs":[],"source":["def main():\n","    parser = argparse.ArgumentParser()\n","\n","    # Required parameters\n","    parser.add_argument(\n","        \"--data_dir\",\n","        default=\"drive/My Drive/CODE/ViMMRC/dataset/race_ver_1\",\n","        type=str,\n","        help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\",\n","    )\n","\n","    parser.add_argument(\n","        \"--model_type\",\n","        default=\"bert\",\n","        type=str,\n","        help=\"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys()),\n","    )\n","    parser.add_argument(\n","        \"--model_name_or_path\",\n","        default=OUTPUT_DIR,\n","        type=str,\n","        help=\"Path to pre-trained model or shortcut name selected in the list: \"\n","        + \", \".join(ALL_MODELS),\n","    )\n","    \n","    parser.add_argument(\n","        \"--task_name\",\n","        default=\"race\",\n","        type=str,\n","        help=\"The name of the task to train selected in the list: \" + \", \".join(processors.keys()),\n","    )\n","    parser.add_argument(\n","        \"--para_type\",\n","        default=\"per_choice\",\n","        type=str,\n","        choices=[\"per_choice\", \"concat_choices\", \"ignore\"],\n","        help=\"Paragraph building strategy for ARC (default: %(default)s)\",\n","    )\n","    parser.add_argument(\n","        \"--output_predictions\",\n","        default=True,\n","        type=bool,\n","        help=\"Whether to export the predictions from the eval step.\",\n","    )\n","    parser.add_argument(\n","        \"--output_dir\",\n","        default=OUTPUT_DIR,\n","        type=str,\n","        help=\"The output directory where the model predictions and checkpoints will be written.\",\n","    )\n","    parser.add_argument(\n","        \"--freeze_embeddings\", default=False, action=\"store_true\", help=\"Whether to freeze the embeeding layer.\",\n","    )\n","    parser.add_argument(\n","        \"--freeze_layers\", nargs=\"*\", help=\"Whether to freeze the embeeding layer.\",\n","    )\n","\n","    # Other parameters\n","    parser.add_argument(\n","        \"--tb_log_dir\",\n","        default=\"\",\n","        type=str,\n","        help=\"Tensorboard log dir for the current experiment\",\n","    )\n","\n","    parser.add_argument(\n","        \"--config_name\",\n","        default=\"\",\n","        type=str,\n","        help=\"Pretrained config name or path if not the same as model_name\",\n","    )\n","    parser.add_argument(\n","        \"--tokenizer_name\",\n","        default=\"\",\n","        type=str,\n","        help=\"Pretrained tokenizer name or path if not the same as model_name\",\n","    )\n","    parser.add_argument(\n","        \"--cache_dir\",\n","        default=OUTPUT_DIR,\n","        type=str,\n","        help=\"Where do you want to store the pre-trained models downloaded from s3\",\n","    )\n","    parser.add_argument(\n","        \"--max_seq_length\",\n","        default=300,\n","        type=int,\n","        help=\"The maximum total input sequence length after tokenization. Sequences longer \"\n","        \"than this will be truncated, sequences shorter will be padded.\",\n","    )\n","    parser.add_argument(\"--do_train\", default = False, action=\"store_true\", help=\"Whether to run training.\")\n","    parser.add_argument(\n","        \"--do_eval\", default = True, action=\"store_true\", help=\"Whether to run eval on the dev set.\"\n","    )\n","    parser.add_argument(\n","        \"--do_test\", default = True, action=\"store_true\", help=\"Whether to run test on the test set\"\n","    )\n","    parser.add_argument(\n","        \"--evaluate_during_training\",\n","        action=\"store_true\",\n","        default = True,\n","        help=\"Run evaluation during training at each logging step.\",\n","    )\n","    parser.add_argument(\n","        \"--do_lower_case\",\n","        action=\"store_true\",\n","        default = False,\n","        help=\"Set this flag if you are using an uncased model.\",\n","    )\n","\n","    parser.add_argument(\n","        \"--per_gpu_train_batch_size\",\n","        default=4,\n","        type=int,\n","        help=\"Batch size per GPU/CPU for training.\",\n","    )\n","    parser.add_argument(\n","        \"--per_gpu_eval_batch_size\",\n","        default=1,\n","        type=int,\n","        help=\"Batch size per GPU/CPU for evaluation.\",\n","    )\n","    parser.add_argument(\n","        \"--gradient_accumulation_steps\",\n","        type=int,\n","        default=8,\n","        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n","    )\n","    parser.add_argument(\n","        \"--learning_rate\", default=1e-5, type=float, help=\"The initial learning rate for Adam.\"\n","    )\n","    parser.add_argument(\n","        \"--weight_decay\", default=0.0, type=float, help=\"Weight deay if we apply some.\"\n","    )\n","    parser.add_argument(\n","        \"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\"\n","    )\n","    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n","    parser.add_argument(\n","        \"--num_train_epochs\",\n","        default=50,\n","        type=float,\n","        help=\"Total number of training epochs to perform.\",\n","    )\n","    parser.add_argument(\n","        \"--max_steps\",\n","        default=-1,\n","        type=int,\n","        help=\"If \u003e 0: set total number of training steps to perform. Override num_train_epochs.\",\n","    )\n","    parser.add_argument(\n","        \"--warmup_proportion\", default=0.1, type=float, help=\"Linear warmup over warmup_proportion.\"\n","    )\n","\n","    parser.add_argument(\"--logging_steps\", type=int, default=500, help=\"Log every X updates steps.\")\n","    parser.add_argument(\n","        \"--save_steps\", type=int, default=500, help=\"Save checkpoint every X updates steps.\"\n","    )\n","    parser.add_argument(\n","        \"--eval_all_checkpoints\",\n","        default = True,\n","        action=\"store_true\",\n","        help=\"Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number\",\n","    )\n","    parser.add_argument(\"--no_cuda\", default = False, action=\"store_true\", help=\"Avoid using CUDA when available\")\n","    parser.add_argument(\n","        \"--overwrite_output_dir\",\n","        default = True,\n","        action=\"store_true\",\n","        help=\"Overwrite the content of the output directory\",\n","    )\n","    parser.add_argument(\n","        \"--overwrite_cache\",\n","        default = True,\n","        action=\"store_true\",\n","        help=\"Overwrite the cached training and evaluation sets\",\n","    )\n","    parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization\")\n","\n","    parser.add_argument(\n","        \"--fp16\",\n","        action=\"store_true\",\n","        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n","    )\n","    parser.add_argument(\n","        \"--fp16_opt_level\",\n","        type=str,\n","        default=\"O1\",\n","        help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n","        \"See details at https://nvidia.github.io/apex/amp.html\",\n","    )\n","    parser.add_argument(\n","        \"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\"\n","    )\n","    parser.add_argument(\"--server_ip\", type=str, default=\"\", help=\"For distant debugging.\")\n","    parser.add_argument(\"--server_port\", type=str, default=\"\", help=\"For distant debugging.\")\n","\n","    parser.add_argument('-f')\n","\n","    args = parser.parse_args()\n","\n","    options_print = \"\"\n","    logger.info(\"Arg Options - input:\")\n","    for arg in vars(args):\n","        options_print += \"opt: %s=%s\\r\\n\" % (arg, getattr(args, arg))\n","    logger.info(options_print)\n","\n","    if (\n","        os.path.exists(args.output_dir)\n","        and os.listdir(args.output_dir)\n","        and args.do_train\n","        and not args.overwrite_output_dir\n","    ):\n","        raise ValueError(\n","            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n","                args.output_dir\n","            )\n","        )\n","\n","    # Setup distant debugging if needed\n","    if args.server_ip and args.server_port:\n","        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n","        import ptvsd\n","\n","        print(\"Waiting for debugger attach\")\n","        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n","        ptvsd.wait_for_attach()\n","\n","    # Setup CUDA, GPU \u0026 distributed training\n","    if args.local_rank == -1 or args.no_cuda:\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n","        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n","    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n","        torch.cuda.set_device(args.local_rank)\n","        device = torch.device(\"cuda\", args.local_rank)\n","        torch.distributed.init_process_group(backend=\"nccl\")\n","        args.n_gpu = 1\n","    args.device = device\n","\n","    # Setup logging\n","    logging.basicConfig(\n","        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n","        datefmt=\"%m/%d/%Y %H:%M:%S\",\n","        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n","    )\n","    logger.warning(\n","        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n","        args.local_rank,\n","        device,\n","        args.n_gpu,\n","        bool(args.local_rank != -1),\n","        args.fp16,\n","    )\n","\n","    # Set seed\n","    set_seed(args)\n","\n","    # Prepare GLUE task\n","    args.task_name = args.task_name.lower()\n","    if args.task_name not in processors:\n","        raise ValueError(\"Task not found: %s\" % (args.task_name))\n","    processor = (\n","        processors[args.task_name]()\n","        if args.task_name not in [\"arc\", \"exams\",]\n","        else processors[args.task_name](para_type=args.para_type)\n","    )\n","    label_list = processor.get_labels()\n","    num_labels = len(label_list)\n","\n","    # Load pretrained model and tokenizer\n","    if args.local_rank not in [-1, 0]:\n","        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model \u0026 vocab\n","\n","    args.model_type = args.model_type.lower()\n","    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n","    config = config_class.from_pretrained(\n","        args.config_name if args.config_name else args.model_name_or_path,\n","        num_labels=num_labels,\n","        finetuning_task=args.task_name,\n","        cache_dir=args.cache_dir if args.cache_dir else None,\n","    )\n","    tokenizer = tokenizer_class.from_pretrained(\n","        args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,\n","        do_lower_case=args.do_lower_case,\n","        cache_dir=args.cache_dir if args.cache_dir else None,\n","    )\n","    model = model_class.from_pretrained(\n","        args.model_name_or_path,\n","        from_tf=bool(\".ckpt\" in args.model_name_or_path),\n","        config=config,\n","        cache_dir=args.cache_dir if args.cache_dir else None,\n","    )\n","\n","    if args.local_rank == 0:\n","        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model \u0026 vocab\n","\n","    options_print = \"\"\n","    logging.info(\"Arg Options:\")\n","    for arg in vars(args):\n","        options_print += \"opt: %s=%s\\r\\n\" % (arg, getattr(args, arg))\n","    logging.info(options_print)\n","\n","    model.to(args.device)\n","\n","    def get_model_base_obj(model, model_type):\n","        if model_type == \"bert\":\n","            return model.bert\n","        elif model_type == \"xlm-roberta\" or model_type == \"roberta\":\n","            return model.roberta\n","        elif model_type == \"xlm-roberta\":\n","            return model.distilbert\n","        else:\n","            raise ValueError(\"model_type='{0}' is not supported!\")\n","\n","    if args.freeze_embeddings:\n","        for param in list(get_model_base_obj(model, args.model_type).embeddings.parameters()):\n","            param.requires_grad = False\n","        logger.info(\"Froze Embedding Layer\")\n","\n","    # freeze_layers is a string \"1,2,3\" representing layer number\n","    if args.freeze_layers:\n","        layer_indexes = [int(x) for x in args.freeze_layers]\n","        for layer_idx in layer_indexes:\n","            for param in list(\n","                get_model_base_obj(model, args.model_type).encoder.layer[layer_idx].parameters()\n","            ):\n","                param.requires_grad = False\n","            logger.info(\"Froze Layer: %s\", layer_idx)\n","\n","    logger.info(\"Training/evaluation parameters %s\", args)\n","    best_steps = 0\n","\n","    # Training\n","    if args.do_train:\n","        train_dataset = load_and_cache_examples(args, args.task_name, tokenizer, evaluate=False)\n","        global_step, tr_loss, best_steps = train(args, train_dataset, model, tokenizer)\n","        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n","\n","    # Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n","    if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n","        # Create output directory if needed\n","        if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n","            os.makedirs(args.output_dir)\n","\n","        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n","        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n","        # They can then be reloaded using `from_pretrained()`\n","        model_to_save = (\n","            model.module if hasattr(model, \"module\") else model\n","        )  # Take care of distributed/parallel training\n","        model_to_save.save_pretrained(args.output_dir)\n","        tokenizer.save_pretrained(args.output_dir)\n","\n","        # Good practice: save your training arguments together with the trained model\n","        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n","\n","        # Load a trained model and vocabulary that you have fine-tuned\n","        model = model_class.from_pretrained(args.output_dir)\n","        tokenizer = tokenizer_class.from_pretrained(args.output_dir)\n","        model.to(args.device)\n","\n","    # Evaluation\n","    results = {}\n","    if args.do_eval and args.local_rank in [-1, 0]:\n","        if not args.do_train:\n","            args.output_dir = args.model_name_or_path\n","        checkpoints = [args.output_dir]\n","        if args.eval_all_checkpoints:\n","            checkpoints = list(\n","                os.path.dirname(c)\n","                for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n","            )\n","            logging.getLogger(\"transformers.modeling_utils\").setLevel(\n","                logging.WARN\n","            )  # Reduce logging\n","        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n","        for checkpoint in checkpoints:\n","            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) \u003e 1 else \"\"\n","            prefix = checkpoint.split(\"/\")[-1] if checkpoint.find(\"checkpoint\") != -1 else \"\"\n","\n","            model = model_class.from_pretrained(checkpoint)\n","            model.to(args.device)\n","            result = evaluate(args, model, tokenizer, prefix=prefix)\n","            result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n","            results.update(result)\n","\n","    if args.do_test and args.local_rank in [-1, 0]:\n","        checkpoints = [args.model_name_or_path]\n","        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n","        for checkpoint in checkpoints:\n","            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) \u003e 1 else \"\"\n","            prefix = checkpoint.split(\"/\")[-1] if checkpoint.find(\"checkpoint\") != -1 else \"\"\n","\n","            model = model_class.from_pretrained(checkpoint)\n","            model.to(args.device)\n","            result = evaluate(args, model, tokenizer, prefix=prefix, test=True)\n","            result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n","            results.update(result)\n","    if best_steps:\n","        logger.info(\"best steps of eval acc is the following checkpoints: %s\", best_steps)\n","    return results\n","\n","\n","if __name__ == \"__main__\":\n","    main()"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["K_Pv9CIYm7BZ","ioQPuCNdks1x","uWwt6WujlC1Q","yoiGjVs5kuwD","42dvg6DqZPg2"],"name":"","version":""},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}