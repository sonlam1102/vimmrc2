{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1696193940057,"user":{"displayName":"Khôi Hoàng","userId":"04631266824167781561"},"user_tz":-420},"id":"kimkG7xp6wvS","outputId":"e3ff4a36-266e-41ca-8589-78def4aee6a8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sun Oct  1 20:58:59 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   63C    P8    12W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16665,"status":"ok","timestamp":1696193957841,"user":{"displayName":"Khôi Hoàng","userId":"04631266824167781561"},"user_tz":-420},"id":"n8OLTA2cqdHk","outputId":"7b5cd39d-25d6-4681-9cb9-d48443a272ec"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":580,"status":"ok","timestamp":1696193958419,"user":{"displayName":"Khôi Hoàng","userId":"04631266824167781561"},"user_tz":-420},"id":"G4x_F5OtS5V8","outputId":"88e6521b-ca83-43da-a38c-0e1518056746"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/NCKH/ViMMRC_model\n"]}],"source":["%cd '/content/drive/MyDrive/NCKH/ViMMRC_model/'"]},{"cell_type":"markdown","metadata":{"id":"K_Pv9CIYm7BZ"},"source":["# Library\n","\n","https://github.com/mhardalov/exams-qa \\\n","https://github.com/microsoft/MT-DNN \\\n","https://github.com/jind11/MMM-MCQA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7znvNnvg-0de"},"outputs":[],"source":["#%pip -q install -e git+https://github.com/microsoft/MT-DNN.git#egg=mtdnn"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":23246,"status":"ok","timestamp":1696193981663,"user":{"displayName":"Khôi Hoàng","userId":"04631266824167781561"},"user_tz":-420},"id":"ksz16DuhgUtk","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1e19a55a-7d57-438d-be62-cee5942ed4f5"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["%pip -q install torch -U\n","%pip -q install transformers -U\n","%pip -q install tqdm -U\n","# %pip -q install --upgrade  git+https://github.com/lanpa/tensorboardX.git"]},{"cell_type":"markdown","metadata":{"id":"dT2PDGixT400"},"source":["Install apex package: https://stackoverflow.com/a/74561776\n","\n","Query the version Ubuntu Colab is running on:\\\n","`!lsb_release -a`\n","\n","Get the current cuda version run:\\\n","`!nvcc --version`"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":500,"status":"ok","timestamp":1696193982159,"user":{"displayName":"Khôi Hoàng","userId":"04631266824167781561"},"user_tz":-420},"id":"qWFKkQbKTUBA","outputId":"152b44bc-fe29-417c-c324-262fe8b0f76a"},"outputs":[{"output_type":"stream","name":"stdout","text":["nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2022 NVIDIA Corporation\n","Built on Wed_Sep_21_10:33:58_PDT_2022\n","Cuda compilation tools, release 11.8, V11.8.89\n","Build cuda_11.8.r11.8/compiler.31833905_0\n"]}],"source":["!nvcc --version"]},{"cell_type":"markdown","metadata":{"id":"ioQPuCNdks1x"},"source":["# Models"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"svixfajDMreB","executionInfo":{"status":"ok","timestamp":1696193982159,"user_tz":-420,"elapsed":2,"user":{"displayName":"Khôi Hoàng","userId":"04631266824167781561"}}},"outputs":[],"source":["#%pip install git+git@github.com:microsoft/mt-dnn.git@master#egg=mtdnn"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"4x7vIh8LE40U","executionInfo":{"status":"ok","timestamp":1696193985722,"user_tz":-420,"elapsed":3565,"user":{"displayName":"Khôi Hoàng","userId":"04631266824167781561"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn.parameter import Parameter\n","# from mtdnn.common.dropout_wrapper import DropoutWrapper\n","# from mtdnn.common.optimizer import weight_norm as WN\n","# from mtdnn.common.similarity import FlatSimilarityWrapper, SelfAttnWrapper, SimilarityWrapper\n","# from mtdnn.common.activation_functions import activation\n","\n","\n","# class DualAttentionWrapper(nn.Module):\n","\n","# class Classifier(nn.Module):\n","\n","\n","# # mtdnn.common.san.SANClassifier() super\n","# class SANClassifier(nn.Module):\n","#     \"\"\"Implementation of Stochastic Answer Networks for Natural Language Inference, Xiaodong Liu, Kevin Duh and Jianfeng Gao\n","#     https://arxiv.org/abs/1804.07888\n","#     \"\"\"\n","\n","# class SANClassifier2(nn.Module):\n","#     \"\"\"Implementation of Stochastic Answer Networks for Natural Language Inference, Xiaodong Liu, Kevin Duh and Jianfeng Gao\n","#     https://arxiv.org/abs/1804.07888\n","#     \"\"\""]},{"cell_type":"code","execution_count":8,"metadata":{"id":"jJZdHqpLB3B6","executionInfo":{"status":"ok","timestamp":1696193992280,"user_tz":-420,"elapsed":6563,"user":{"displayName":"Khôi Hoàng","userId":"04631266824167781561"}}},"outputs":[],"source":["# MMM-MCQA from https://github.com/jind11/MMM-MCQA\n","\n","#import copy\n","import math\n","import torch\n","import torch.nn as nn\n","from torch.nn import CrossEntropyLoss, NLLLoss\n","from transformers.modeling_utils import prune_linear_layer\n","from transformers.models.bert.modeling_bert import (\n","    BertPreTrainedModel,\n","    BertEmbeddings,\n","    BertSelfOutput,\n","    BertIntermediate,\n","    BertOutput,\n","    BertPooler,\n","    #BertModel,\n",")\n","\n","BertLayerNorm = torch.nn.LayerNorm\n"]},{"cell_type":"markdown","metadata":{"id":"uWwt6WujlC1Q"},"source":["# Utils"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"HZxTWqhmlCaO","executionInfo":{"status":"ok","timestamp":1696193992280,"user_tz":-420,"elapsed":5,"user":{"displayName":"Khôi Hoàng","userId":"04631266824167781561"}}},"outputs":[],"source":["import sys\n","import csv\n","import glob\n","import json\n","import logging\n","import os\n","from typing import List\n","\n","import numpy as np\n","from tqdm.auto import tqdm, trange\n","from transformers import PreTrainedTokenizer\n","\n","logger = logging.getLogger(__name__)\n","\n","\n","class InputExample(object):\n","    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n","\n","    def __init__(self, guid, text_a, text_b=None, label=None, text_c=None):\n","        \"\"\"Constructs a InputExample.\n","        Args:\n","            guid: Unique id for the example.\n","            text_a: string. The untokenized text of the first sequence. For single sequence tasks, only this sequence must be specified.\n","            text_b: (Optional) string. The untokenized text of the second sequence. Only must be specified for sequence pair tasks.\n","            label: (Optional) string. The label of the example. This should be specified for train and dev examples, but not for test examples.\n","        \"\"\"\n","        self.guid = guid\n","        self.text_a = text_a\n","        self.text_b = text_b\n","        self.text_c = text_c\n","        self.label = label\n","\n","\n","class InputFeatures(object):\n","    \"\"\"A single set of features of data.\"\"\"\n","\n","    def __init__(self, guid, input_ids, input_mask, segment_ids, label_id): #, premise_mask, hypothesis_mask):\n","        self.guid = guid\n","        self.input_ids = input_ids\n","        self.input_mask = input_mask\n","        self.segment_ids = segment_ids\n","        self.label_id = label_id\n","        #self.premise_mask = premise_mask\n","        #self.hypothesis_mask = hypothesis_mask\n","\n","\n","class DataProcessor(object):\n","    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n","\n","    def get_train_examples(self, data_dir):\n","        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n","        raise NotImplementedError()\n","\n","    def get_dev_examples(self, data_dir):\n","        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n","        raise NotImplementedError()\n","\n","    def get_labels(self):\n","        \"\"\"Gets the list of labels for this data set.\"\"\"\n","        raise NotImplementedError()\n","\n","    @classmethod\n","    def _read_tsv(cls, input_file, quotechar=None):\n","        \"\"\"Reads a tab separated value file.\"\"\"\n","        with open(input_file, \"r\") as f:\n","            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n","            lines = []\n","            for line in reader:\n","                lines.append(line)\n","            return lines\n","\n","    @classmethod\n","    def _read_json(cls, input_file):\n","        \"\"\"Reads a json file.\"\"\"\n","        with open(input_file, \"r\", encoding='utf-8') as fpr:\n","            raw_list = json.load(fpr)\n","            return raw_list\n","\n","    @classmethod\n","    def _read_jsonl(cls, input_file):\n","        \"\"\"Reads a json file.\"\"\"\n","        with open(input_file, \"r\", encoding='utf-8') as fpr:\n","            raw_list = list()\n","            for json_str in list(fpr):\n","                raw_list.append(json.loads(json_str))\n","            return raw_list\n","\n","\n","class VimmrcProcessor(DataProcessor):\n","    \"\"\"Processor for the MultiNLI data set (GLUE version).\"\"\"\n","    def get_train_examples(self, data_dir):\n","        \"\"\"See base class.\"\"\"\n","        return self._read_samples(data_dir, \"train\")\n","\n","    def get_dev_examples(self, data_dir):\n","        \"\"\"See base class.\"\"\"\n","        return self._read_samples(data_dir, \"dev\")\n","\n","    def get_test_examples(self, data_dir):\n","        \"\"\"See base class.\"\"\"\n","        return self._read_samples(data_dir, \"test\")\n","\n","    def get_labels(self):\n","        \"\"\"See base class.\"\"\"\n","        return [\"A\", \"B\", \"C\", \"D\"]\n","\n","    def _read_samples(self, data_dir, set_type):\n","        \"\"\"Creates examples for the training and dev sets.\"\"\"\n","        # Init reader\n","        examples = []\n","        #example_id = 0\n","        # with open(filename, 'r', encoding='utf-8') as fpr:\n","        #     raw_list = json.load(fpr)\n","        raw_list = self._read_json(os.path.join(data_dir, set_type + \".json\"))\n","\n","        for data_raw in raw_list:\n","            # data_raw = json.load(fpr)\n","            article = data_raw['content']\n","            example_id = 0\n","            title = '_'.join(data_raw['files'].split('/')[-1].split('_')[:-1])\n","            for i in range(len(data_raw['questions'])):\n","                example_id += 1\n","                #truth = str(ord(data_raw['questions'][i]['answer']) - ord('A'))\n","                truth = data_raw['questions'][i]['answer']\n","                question = data_raw['questions'][i]['question']\n","                options = data_raw['questions'][i]['options']\n","                for k in range(len(options)):\n","                    guid = \"%s-%s-%s-%s\" % (set_type, title, example_id, k)\n","                    option = list(options[k].values())[0]\n","                    examples.append(\n","                        InputExample(guid=guid, text_a=article, text_b=option, label=truth, text_c=question))\n","\n","        return examples\n","\n","\n","class VinliProcessor(DataProcessor):\n","    \"\"\"Processor for the MultiNLI data set (GLUE version).\"\"\"\n","    def get_train_examples(self, data_dir):\n","        \"\"\"See base class.\"\"\"\n","        return self._read_samples(data_dir, \"train\")\n","\n","    def get_dev_examples(self, data_dir):\n","        \"\"\"See base class.\"\"\"\n","        return self._read_samples(data_dir, \"dev\")\n","\n","    def get_test_examples(self, data_dir):\n","        \"\"\"See base class.\"\"\"\n","        return self._read_samples(data_dir, \"test\")\n","\n","    def get_labels(self):\n","        \"\"\"See base class.\"\"\"\n","        return [\"contradiction\", \"entailment\", \"neutral\"]\n","\n","    def _read_samples(self, data_dir, set_type):\n","        \"\"\"Creates examples for the training and dev sets.\"\"\"\n","        # Init reader\n","        examples = []\n","        raw_list = self._read_jsonl(os.path.join(data_dir, set_type + \".jsonl\"))\n","        for data_line in raw_list:\n","            guid = \"%s-%s\" % (set_type, data_line['pairID'])\n","            text_a = data_line['sentence1']\n","            text_b = data_line['sentence2']\n","            label = data_line['gold_label']\n","            if label in self.get_labels():\n","                examples.append(\n","                    InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n","\n","        return examples\n","\n","\n","class RaceProcessor(DataProcessor):\n","    \"\"\"Processor for the RACE data set.\"\"\"\n","\n","    def get_train_examples(self, data_dir):\n","        \"\"\"See base class.\"\"\"\n","        logger.info(\"LOOKING AT {} train\".format(data_dir))\n","        high = os.path.join(data_dir, \"train_race.json\")\n","        # middle = os.path.join(data_dir, \"train/middle\")\n","        high = self._read_txt(high)\n","        # middle = self._read_txt(middle)\n","        return self._create_examples(high, \"train\")\n","\n","    def get_dev_examples(self, data_dir):\n","        \"\"\"See base class.\"\"\"\n","        logger.info(\"LOOKING AT {} dev\".format(data_dir))\n","        high = os.path.join(data_dir, \"dev_race.json\")\n","        # middle = os.path.join(data_dir, \"dev/middle\")\n","        high = self._read_txt(high)\n","        # middle = self._read_txt(middle)\n","        return self._create_examples(high, \"dev\")\n","\n","    def get_test_examples(self, data_dir):\n","        \"\"\"See base class.\"\"\"\n","        logger.info(\"LOOKING AT {} test\".format(data_dir))\n","        high = os.path.join(data_dir, \"test_race.json\")\n","        # middle = os.path.join(data_dir, \"test/middle\")\n","        high = self._read_txt(high)\n","        # middle = self._read_txt(middle)\n","        return self._create_examples(high, \"test\")\n","\n","    def get_labels(self):\n","        \"\"\"See base class.\"\"\"\n","        return [\"A\", \"B\", \"C\", \"D\"]\n","\n","    def _read_txt(self, input_dir):\n","        lines = []\n","        # files = glob.glob(input_dir + \"/*json\")\n","        # for file in tqdm(files, desc=\"read files\", disable = True):\n","        #     with open(file, \"r\", encoding=\"utf-8\") as fin:\n","        #         data_raw = json.load(fin)\n","        #         data_raw[\"race_id\"] = file\n","        #         lines.append(data_raw)\n","        with open(input_dir, \"r\", encoding=\"utf-8\") as fin:\n","            data_raw = json.load(fin)\n","        for d in data_raw:\n","            d['race_id'] = d['files']\n","            lines.append(d)\n","        return lines\n","\n","    # def _create_examples(self, lines, set_type):\n","    #     \"\"\"Creates examples for the training and dev sets.\"\"\"\n","    #     examples = []\n","    #     for (_, data_raw) in enumerate(lines):\n","    #         race_id = \"%s-%s\" % (set_type, data_raw[\"race_id\"])\n","    #         article = data_raw[\"article\"]\n","    #         for i in range(len(data_raw[\"answers\"])):\n","    #             truth = str(ord(data_raw[\"answers\"][i]) - ord(\"A\"))\n","    #             # try:\n","    #             #     truth = str(ord(data_raw[\"answers\"][i]) - ord(\"A\"))\n","    #             # except Exception as e:\n","    #             #     print(data_raw[\"answers\"][i])\n","    #             #     print(\"-----\")\n","    #             #     print(data_raw[\"answers\"])\n","    #             #     raise e\n","    #             question = data_raw[\"questions\"][i]\n","    #             options = data_raw[\"options\"][i]\n","\n","    #             examples.append(\n","    #                 InputExample(\n","    #                     example_id=race_id,\n","    #                     question=question,\n","    #                     contexts=[\n","    #                         article,\n","    #                         article,\n","    #                         article,\n","    #                         article,\n","    #                     ],  # this is not efficient but convenient\n","    #                     endings=[options[0], options[1], options[2], options[3]],\n","    #                     label=truth,\n","    #                 )\n","    #             )\n","    #     return examples\n","    def _create_examples(self, lines, set_type):\n","        \"\"\"Creates examples for the training and dev sets.\"\"\"\n","        examples = []\n","        for (_, data_raw) in enumerate(lines):\n","            race_id = \"%s-%s\" % (set_type, data_raw[\"race_id\"])\n","            article = data_raw[\"article\"]\n","            for i in range(len(data_raw[\"answers\"])):\n","                truth = data_raw[\"answers\"][i]\n","                question = data_raw[\"questions\"][i]\n","                options = data_raw[\"options\"][i]\n","                for k in range(len(options)):\n","                    guid = \"%s-%s\" % (race_id, k)\n","                    examples.append(\n","                        InputExample(\n","                            guid=guid,\n","                            text_a=article,\n","                            text_b=options[k], #list(options[k].values())[0]\n","                            label=truth,\n","                            text_c=question\n","                        )\n","                    )\n","        return examples\n","\n","\n","class InfiniteDataLoader:\n","    def __init__(self, data_loader):\n","        self.data_loader = data_loader\n","        self.data_iter = iter(data_loader)\n","\n","    def get_next(self):\n","        try:\n","            data = next(self.data_iter)\n","        except StopIteration:\n","            # StopIteration is thrown if dataset ends\n","            # reinitialize data loader\n","            self.data_iter = iter(self.data_loader)\n","            data = next(self.data_iter)\n","        return data\n","\n","\n","def convert_examples_to_features(\n","    examples: List[InputExample],\n","    label_list: List[str],\n","    max_seq_length: int,\n","    tokenizer,\n","    n_class,\n","    do_lower_case,\n","    output_mode,\n","    set_type,\n","    is_multi_choice=True\n",") -> List[InputFeatures]:\n","    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n","\n","    label_map = {label: i for i, label in enumerate(label_list)}\n","\n","    if is_multi_choice:\n","        features = [[]]\n","    else:\n","        features = []\n","\n","    feature_iterator = tqdm(examples, desc=\"Convert examples to features\")\n","    for (ex_index, example) in enumerate(feature_iterator):\n","        feature_iterator.set_description(\"Convert %d of %d example to features\" % (ex_index, len(examples)))\n","\n","        tokens_a = tokenizer.tokenize(example.text_a.lower() if do_lower_case else example.text_a)  # dialogues\n","\n","        tokens_b = [] # None\n","        tokens_c = [] # None\n","\n","        if example.text_b:\n","            tokens_b = tokenizer.tokenize(example.text_b.lower() if do_lower_case else example.text_b)  # answers\n","\n","        if example.text_c:\n","            tokens_c = tokenizer.tokenize(example.text_c.lower() if do_lower_case else example.text_c)  # questions\n","\n","        if tokens_c:\n","            _truncate_seq_tuple(tokens_a, tokens_b, tokens_c, max_seq_length - 4)\n","            tokens_b = tokens_c + [\"[SEP]\"] + tokens_b\n","        elif tokens_b:\n","            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n","        else:\n","            if len(tokens_a) > max_seq_length - 2:\n","                tokens_a = tokens_a[0:(max_seq_length - 2)]\n","\n","        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n","        segment_ids = (len(tokens_a) + 2) * [0]\n","        #premise_mask = (len(tokens_a) + 2) * [1]\n","        #hypothesis_mask = (len(tokens_a) + 2) * [0]\n","\n","        if tokens_b:\n","            tokens += tokens_b + [\"[SEP]\"]\n","            segment_ids += [1] * (len(tokens_b) + 1)\n","            #premise_mask += [1] * (len(tokens_c) + 1)\n","            #premise_mask += [0] * (len(tokens_b) - len(tokens_c))\n","            #hypothesis_mask += [0] * (len(tokens_c) + 1)\n","            #hypothesis_mask += [1] * (len(tokens_b) - len(tokens_c))\n","\n","        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n","\n","        # The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.\n","        input_mask = [1] * len(input_ids)\n","\n","        # Zero-pad up to the sequence length.\n","        pad_length = max_seq_length - len(input_ids)\n","        input_ids += [0] * pad_length\n","        input_mask += [0] * pad_length\n","        segment_ids += [0] * pad_length\n","        #premise_mask += [0] * pad_length\n","        #hypothesis_mask += [0] * pad_length\n","\n","        assert len(input_ids) == max_seq_length\n","        assert len(input_mask) == max_seq_length\n","        assert len(segment_ids) == max_seq_length\n","        #assert len(premise_mask) == max_seq_length\n","        #assert len(hypothesis_mask) == max_seq_length\n","\n","        if output_mode in [\"classification\", \"multi-choice\"]:\n","            label_id = label_map[example.label]\n","        elif output_mode == \"regression\":\n","            label_id = float(example.label)\n","        else:\n","            raise KeyError(output_mode)\n","\n","        if is_multi_choice:\n","            features[-1].append(\n","                InputFeatures(\n","                    guid=example.guid,\n","                    input_ids=input_ids,\n","                    input_mask=input_mask,\n","                    segment_ids=segment_ids,\n","                    label_id=label_id,\n","                    #premise_mask = premise_mask,\n","                    #hypothesis_mask = hypothesis_mask\n","                    ))\n","            if len(features[-1]) == n_class:\n","                features.append([])\n","        else:\n","            features.append(\n","                InputFeatures(\n","                    guid=example.guid,\n","                    input_ids=input_ids,\n","                    input_mask=input_mask,\n","                    segment_ids=segment_ids,\n","                    label_id=label_id,\n","                    #premise_mask = premise_mask,\n","                    #hypothesis_mask = hypothesis_mask\n","                    ))\n","\n","        ## display some example\n","        if set_type == 'train' and ex_index < 4:\n","            logger.info(\"*** Example ***\")\n","            logger.info(f\"guid: {example.guid}\")\n","            logger.info(f\"tokens: {' '.join(tokens)}\")\n","            logger.info(f\"input_ids: {' '.join(map(str, input_ids))}\")\n","            logger.info(f\"input_mask: {' '.join(map(str, input_mask))}\")\n","            logger.info(f\"segment_ids: {' '.join(map(str, segment_ids))}\")\n","            #logger.info(f\"premis_mask: {' '.join(map(str, premise_mask))}\")\n","            #logger.info(f\"hypoth_mask: {' '.join(map(str, hypothesis_mask))}\")\n","            try:\n","                logger.info(f\"label: {example.label}\")\n","            except:\n","                pass\n","\n","    if is_multi_choice:\n","        if len(features[-1]) == 0:\n","            features = features[:-1]\n","\n","    return features\n","\n","\n","def convert_features_to_tensors(features, output_mode, is_multi_choice=True):\n","\n","    input_ids = []\n","    input_mask = []\n","    segment_ids = []\n","    label_id = []\n","    #premise_mask = []\n","    #hypothesis_mask = []\n","\n","    if is_multi_choice:\n","        n_class = len(features[0])\n","        for f in features:\n","            input_ids.append([])\n","            input_mask.append([])\n","            segment_ids.append([])\n","            #premise_mask.append([])\n","            #hypothesis_mask.append([])\n","            for i in range(n_class):\n","                input_ids[-1].append(f[i].input_ids)\n","                input_mask[-1].append(f[i].input_mask)\n","                segment_ids[-1].append(f[i].segment_ids)\n","                #premise_mask[-1].append(f[i].premise_mask)\n","                #hypothesis_mask[-1].append(f[i].hypothesis_mask)\n","\n","            label_id.append([f[0].label_id])\n","    else:\n","        for f in features:\n","            input_ids.append(f.input_ids)\n","            input_mask.append(f.input_mask)\n","            segment_ids.append(f.segment_ids)\n","            label_id.append(f.label_id)\n","            #premise_mask.append(f.premise_mask)\n","            #hypothesis_mask.append(f.hypothesis_mask)\n","\n","    all_input_ids = torch.tensor(input_ids, dtype=torch.long)\n","    all_input_mask = torch.tensor(input_mask, dtype=torch.long)\n","    all_segment_ids = torch.tensor(segment_ids, dtype=torch.long)\n","    #all_premise_mask = torch.tensor(premise_mask, dtype=torch.bool)\n","    #all_hypothesis_mask = torch.tensor(hypothesis_mask, dtype=torch.bool)\n","\n","    if output_mode in [\"classification\", \"multi-choice\"]:\n","        all_label_ids = torch.tensor(label_id, dtype=torch.long)\n","    elif output_mode == \"regression\":\n","        all_label_ids = torch.tensor(label_id, dtype=torch.float)\n","\n","    data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n","                         #all_premise_mask, all_hypothesis_mask,\n","                         #all_label_ids)\n","\n","    return data\n","\n","\n","def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n","    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n","\n","    # This is a simple heuristic which will always truncate the longer sequence\n","    # one token at a time. This makes more sense than truncating an equal percent\n","    # of tokens from each, since if one sequence is very short then each token\n","    # that's truncated likely contains more information than a longer sequence.\n","    while True:\n","        total_length = len(tokens_a) + len(tokens_b)\n","        if total_length <= max_length:\n","            break\n","        if len(tokens_a) > len(tokens_b):\n","            tokens_a.pop()\n","        else:\n","            tokens_b.pop()\n","\n","\n","def _truncate_seq_tuple(tokens_a, tokens_b, tokens_c, max_length):\n","    \"\"\"Truncates a sequence tuple in place to the maximum length.\"\"\"\n","\n","    # This is a simple heuristic which will always truncate the longer sequence\n","    # one token at a time. This makes more sense than truncating an equal percent\n","    # of tokens from each, since if one sequence is very short then each token\n","    # that's truncated likely contains more information than a longer sequence.\n","    while True:\n","        total_length = len(tokens_a) + len(tokens_b) + len(tokens_c)\n","        if total_length <= max_length:\n","            break\n","        if len(tokens_a) >= len(tokens_b) and len(tokens_a) >= len(tokens_c):\n","            tokens_a.pop()\n","        elif len(tokens_b) >= len(tokens_a) and len(tokens_b) >= len(tokens_c):\n","            tokens_b.pop()\n","        else:\n","            tokens_c.pop()\n","\n","\n","processors = {\n","    \"vimmrc\": VimmrcProcessor,\n","    \"vinli\": VinliProcessor,\n","    \"vimmrc_race\": RaceProcessor,\n","}\n","\n","output_modes = {\n","    \"vimmrc\": 'multi-choice',\n","    \"vimmrc_race\": 'multi-choice',\n","    \"vinli\": \"classification\",\n","}\n","\n","GLUE_TASKS_NUM_LABELS = { \"vimmrc\": 4, \"vimmrc_race\": 4, \"vinli\": 3 }\n","\n","MAX_SEQ_LENGTHS = { \"vimmrc\": 512, \"vimmrc_race\": 512, \"vinli\": 128 }"]},{"cell_type":"markdown","metadata":{"id":"yoiGjVs5kuwD"},"source":["# Module"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"VG-MZ9JcbuBo","executionInfo":{"status":"ok","timestamp":1696193992978,"user_tz":-420,"elapsed":702,"user":{"displayName":"Khôi Hoàng","userId":"04631266824167781561"}}},"outputs":[],"source":["import argparse\n","import glob\n","import json\n","import logging\n","import os\n","import random\n","\n","import numpy as np\n","import torch\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n","from torch.utils.data.distributed import DistributedSampler\n","#from torch.optim import AdamW\n","from tqdm.auto import tqdm, trange\n","from transformers import (\n","    WEIGHTS_NAME, CONFIG_NAME,\n","    AdamW,\n","    BertConfig,\n","    BertForMultipleChoice,\n","    BertTokenizer,\n","    RobertaConfig,\n","    RobertaForMultipleChoice,\n","    RobertaTokenizer,\n","    XLMRobertaConfig,\n","    XLMRobertaForMultipleChoice,\n","    XLMRobertaTokenizer,\n","    XLNetConfig,\n","    XLNetForMultipleChoice,\n","    XLNetTokenizer,\n","    get_linear_schedule_with_warmup,\n",")\n","\n","from transformers import BERT_PRETRAINED_CONFIG_ARCHIVE_MAP, DISTILBERT_PRETRAINED_CONFIG_ARCHIVE_MAP, XLM_ROBERTA_PRETRAINED_CONFIG_ARCHIVE_MAP\n","from transformers import DistilBertConfig, DistilBertForMultipleChoice, DistilBertTokenizer\n","\n","# try:\n","#     from torch.utils.tensorboard import SummaryWriter\n","# except ImportError:\n","#     from tensorboardX import SummaryWriter\n","\n","logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s', datefmt='%m/%d/%Y %H:%M:%S', level=logging.INFO)\n","logger = logging.getLogger(__name__)\n","logger.setLevel(logging.INFO)\n","\n","ALL_MODELS = tuple(DISTILBERT_PRETRAINED_CONFIG_ARCHIVE_MAP.keys()) + tuple(BERT_PRETRAINED_CONFIG_ARCHIVE_MAP.keys()) + tuple(XLM_ROBERTA_PRETRAINED_CONFIG_ARCHIVE_MAP.keys())\n","\n","MODEL_CLASSES = {\n","    \"bert\": (BertConfig, BertForMultipleChoice, BertTokenizer),\n","    #\"bert-man\": (BertConfig, BertForMultipleChoice_SAN, BertTokenizer),\n","    #\"roberta\": (RobertaConfig, RobertaForMultipleChoice, RobertaTokenizer),\n","    #\"xlm-roberta\": (XLMRobertaConfig, XLMRobertaForMultipleChoice, XLMRobertaTokenizer)\n","}"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"9Iow6D0wkEez","executionInfo":{"status":"ok","timestamp":1696193992979,"user_tz":-420,"elapsed":7,"user":{"displayName":"Khôi Hoàng","userId":"04631266824167781561"}}},"outputs":[],"source":["def select_field(features, field):\n","    return [[choice[field] for choice in feature.choices_features] for feature in features]\n","\n","\n","def simple_accuracy(preds, labels):\n","    return (preds == labels).mean()\n","\n","\n","def set_seed(args):\n","    random.seed(args.seed)\n","    np.random.seed(args.seed)\n","    torch.manual_seed(args.seed)\n","    if args.n_gpu > 0:\n","        torch.cuda.manual_seed_all(args.seed)\n","\n","\n","def load_and_cache_examples(args, task, tokenizer, set_type='train'):\n","    if args.local_rank not in [-1, 0]:\n","        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n","\n","    output_mode = output_modes[task]\n","    is_multi_choice = True if output_mode == 'multi-choice' else False\n","    processor = processors[task]()\n","    # Load data features from cache or dataset file\n","    cached_features_file = os.path.join(\n","        args.data_dir,\n","        'cached_{}_{}_{}_{}'.format(\n","            set_type,\n","            list(filter(None, args.model_name_or_path.split('/'))).pop(),\n","            str(MAX_SEQ_LENGTHS[task]),\n","            str(task)\n","        )\n","    )\n","    if os.path.exists(cached_features_file):\n","        logger.info(\"Loading features from cached file %s\", cached_features_file)\n","        features = torch.load(cached_features_file)\n","    else:\n","        logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n","        label_list = processor.get_labels()\n","        if set_type == 'train':\n","            examples = processor.get_train_examples(args.data_dir)\n","        elif set_type == 'dev':\n","            examples = processor.get_dev_examples(args.data_dir)\n","        else:\n","            examples = processor.get_test_examples(args.data_dir)\n","        logger.info(\"Training number: %s\", str(len(examples)))\n","        features = convert_examples_to_features(\n","            examples,\n","            label_list,\n","            MAX_SEQ_LENGTHS[task],\n","            tokenizer,\n","            len(label_list),\n","            output_mode=output_mode,\n","            set_type=set_type,\n","            do_lower_case=args.do_lower_case,\n","            is_multi_choice=is_multi_choice\n","        )\n","        if args.local_rank in [-1, 0]:\n","            logger.info(\"Saving features into cached file %s\", cached_features_file)\n","            torch.save(features, cached_features_file)\n","\n","    # Convert to Tensors and build dataset\n","    dataset = convert_features_to_tensors(\n","        features, output_mode, is_multi_choice=is_multi_choice\n","    )\n","\n","    if set_type == 'dev' or set_type == 'test':\n","        all_example_ids = [x.guid for feature_set in features for x in feature_set]#[x.example_id for x in features]\n","        return dataset, all_example_ids\n","    else:\n","        return dataset"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"2gdQTXe8k1Cf","executionInfo":{"status":"ok","timestamp":1696193992979,"user_tz":-420,"elapsed":6,"user":{"displayName":"Khôi Hoàng","userId":"04631266824167781561"}}},"outputs":[],"source":["def train(args, train_datasets, model, tokenizer):\n","    \"\"\" Train the model \"\"\"\n","    # if args.local_rank in [-1, 0]:\n","    #     tb_writer = SummaryWriter()\n","    #     pass\n","\n","    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n","    train_iters = []\n","    tr_batches = []\n","    #for idx, train_dataset in enumerate(train_datasets):\n","    train_sampler = RandomSampler(train_datasets) if args.local_rank == -1 else DistributedSampler(train_datasets)\n","    train_dataloader = DataLoader(train_datasets, sampler=train_sampler, batch_size=args.train_batch_size)\n","    train_iters.append(InfiniteDataLoader(train_dataloader))\n","    tr_batches.append(len(train_dataloader))\n","\n","    ## set sampling proportion\n","    total_n_tr_batches = sum(tr_batches)\n","    sampling_prob = [float(n_batches) / total_n_tr_batches for n_batches in tr_batches]\n","    t_total = total_n_tr_batches // args.gradient_accumulation_steps * args.num_train_epochs\n","\n","    # Prepare optimizer and schedule (linear warmup and decay)\n","    no_decay = ['bias', 'LayerNorm.weight']\n","    optimizer_grouped_parameters = [\n","        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","         'weight_decay': args.weight_decay},\n","        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n","         'weight_decay': 0.0}\n","    ]\n","\n","    # optimizer = BertAdam(\n","    #     optimizer_grouped_parameters,\n","    #     lr=args.learning_rate,\n","    #     warmup=args.warmup_proportion,\n","    #     max_grad_norm=args.max_grad_norm, t_total=t_total)\n","\n","    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, correct_bias=False, no_deprecation_warning=True) # To reproduce BertAdam specific behavior set correct_bias=False\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer,\n","        num_warmup_steps=int(args.warmup_proportion * t_total),\n","        num_training_steps=t_total,\n","    ) # PyTorch scheduler\n","\n","    # Train!\n","    logger.info(\"***** Running training *****\")\n","    logger.info(\"  Num examples = %d\", len(train_datasets))\n","    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n","    logger.info(\"  Instantaneous batch size per GPU = %s\", args.per_gpu_train_batch_size)\n","    logger.info(\n","        \" Total train batch size (w. parallel, distributed & accumulation) = %d\",\n","        args.train_batch_size\n","        * args.gradient_accumulation_steps\n","        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n","    )\n","    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n","    logger.info(\"  Total optimization steps = %d\", t_total)\n","\n","    global_step = 0\n","    tr_loss, logging_loss, tmp_loss = 0.0, 0.0, 0.0\n","    best_dev_acc = 0.0\n","    best_steps = 0\n","    nb_tr_examples = 0\n","    model.zero_grad()\n","    train_iterator = tqdm(range(int(args.num_train_epochs)), desc=\"Epoch\") #, disable=args.local_rank not in [-1, 0])\n","    set_seed(args)  # Added here for reproductibility (even between python 2 and 3)\n","    for epoch, _ in enumerate(train_iterator):\n","        epoch_iterator = tqdm(range(total_n_tr_batches), desc=\"Iteration\") #, disable=args.local_rank not in [-1, 0])\n","        for step, _ in enumerate(epoch_iterator):\n","            epoch_iterator.set_description(\n","                \"train loss: {}\".format(tr_loss / nb_tr_examples if nb_tr_examples else tr_loss)\n","            )\n","            model.train()\n","            # select task id\n","            task_id = np.argmax(np.random.multinomial(1, sampling_prob))\n","            batch = train_iters[task_id].get_next()\n","            batch = tuple(t.to(args.device) for t in batch)\n","            inputs = {'input_ids':      batch[0],\n","                      'attention_mask': batch[1],\n","                      'token_type_ids': batch[2],\n","                      #'premise_mask':   batch[3],\n","                      #'hyp_mask':       batch[4],\n","                      'labels':         batch[3].view(-1),\n","                      #'task_id':        task_id\n","                      }\n","            outputs = model(**inputs)  # model outputs are always tuple in transformers (see doc)\n","            loss = outputs[0]\n","\n","            if args.n_gpu > 1:\n","                loss = loss.mean() # mean() to average on multi-gpu parallel training\n","            if args.gradient_accumulation_steps > 1:\n","                loss = loss / args.gradient_accumulation_steps\n","\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n","\n","            tr_loss += loss.item()\n","            nb_tr_examples += inputs['input_ids'].size(0)\n","            if (step + 1) % args.gradient_accumulation_steps == 0:\n","                optimizer.step()\n","                scheduler.step()  # Update learning rate schedule\n","                model.zero_grad()\n","                global_step += 1\n","                #tb_writer.add_scalar(\"train_{}\".format(\"loss_\"), tr_loss - tmp_loss, global_step)\n","                tmp_loss = tr_loss\n","\n","        # Save model checkpoint\n","        # if args.do_epoch_checkpoint:\n","        epoch_output_dir = os.path.join(args.output_dir, 'epoch_{}'.format(epoch+1))\n","        os.makedirs(epoch_output_dir, exist_ok=True)\n","        output_model_file = os.path.join(epoch_output_dir, WEIGHTS_NAME)\n","        output_config_file = os.path.join(epoch_output_dir, CONFIG_NAME)\n","        model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n","        torch.save(model_to_save.state_dict(), output_model_file)\n","        model_to_save.config.to_json_file(output_config_file)\n","        tokenizer.save_vocabulary(epoch_output_dir)\n","\n","        # evaluate(args, model, tokenizer, epoch=epoch, is_test=False)\n","        # evaluate(args, model, tokenizer, epoch=epoch, is_test=True)\n","    # if args.local_rank in [-1, 0]:\n","    #     tb_writer.close()\n","\n","    return global_step, tr_loss / global_step, best_steps"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"hUI-m6tlk3iR","executionInfo":{"status":"ok","timestamp":1696193992979,"user_tz":-420,"elapsed":6,"user":{"displayName":"Khôi Hoàng","userId":"04631266824167781561"}}},"outputs":[],"source":["def evaluate(args, model, tokenizer, checkpoint='', is_test=False, error=False):\n","    # Loop to handle MNLI double evaluation (matched, mis-matched)\n","    eval_task_names = args.task_name\n","    eval_output_dir = args.output_dir\n","    if checkpoint:\n","        eval_output_dir=checkpoint\n","\n","    set_type = 'test' if is_test else 'dev'\n","    results = {}\n","    #for task_id, eval_task in enumerate(eval_task_names):\n","    #for eval_task, eval_output_dir in zip(eval_task_names, eval_outputs_dirs):\n","    eval_dataset, all_example_ids = load_and_cache_examples(args, eval_task_names, tokenizer, set_type)\n","\n","    if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n","        os.makedirs(eval_output_dir)\n","\n","    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n","    # Note that DistributedSampler samples randomly\n","    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n","    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n","\n","    # Eval!\n","    logger.info(\"***** Running evaluation for {} on {} for {} *****\".format(eval_task_names, set_type, checkpoint))\n","    logger.info(\"  Num examples = %d\", len(eval_dataset))\n","    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n","    eval_loss = 0.0\n","    nb_eval_steps = 0\n","    logits_all = None\n","    out_label_ids = None\n","    preds_softmax = None\n","    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n","        model.eval()\n","        batch = tuple(t.to(args.device) for t in batch)\n","\n","        with torch.no_grad():\n","            inputs = {'input_ids':      batch[0],\n","                      'attention_mask': batch[1],\n","                      'token_type_ids': batch[2],\n","                      #'premise_mask':   batch[3],\n","                      #'hyp_mask':       batch[4],\n","                      'labels':         batch[3].view(-1),\n","                      #'task_id':        0\n","                      }\n","            outputs = model(**inputs)\n","            tmp_eval_loss, logits = outputs[:2]\n","            # input_ids, input_mask, segment_ids, label_ids = batch\n","            # tmp_eval_loss, logits = model(input_ids, segment_ids, input_mask, label_ids, task_id=task_id)\n","            eval_loss += tmp_eval_loss.mean().item()\n","        nb_eval_steps += 1\n","        if logits_all is None:\n","            logits_all = logits.detach().cpu().numpy()\n","            preds_softmax = torch.softmax(logits, -1).detach().cpu().numpy()\n","            out_label_ids = inputs['labels'].detach().cpu().numpy()\n","        else:\n","            logits_all = np.append(logits_all, logits.detach().cpu().numpy(), axis=0)\n","            preds_softmax = np.append(preds_softmax, torch.softmax(logits, -1).detach().cpu().numpy(), axis=0)\n","            out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n","\n","    eval_loss = eval_loss / nb_eval_steps\n","    preds_softmax = [[round(x.tolist(), 4) for x in p] for p in preds_softmax]\n","    example_ids = [id[:-2] for id in all_example_ids[::4]]\n","    per_id_pred = dict(zip(example_ids, preds_softmax))\n","    output_mode = output_modes[eval_task_names]\n","\n","    preds = np.argmax(logits_all, axis=1)\n","    acc = simple_accuracy(preds, out_label_ids.reshape(-1))\n","    #result = compute_metrics(eval_task, preds, out_label_ids.reshape(-1))\n","    result = {\"eval_acc\": acc, \"eval_loss\": eval_loss}\n","    results.update(result)\n","\n","    output_eval_file = os.path.join(eval_output_dir, \"eval_results_{}_{}.txt\".format(eval_task_names, set_type))\n","    with open(output_eval_file, \"a\") as writer:\n","        logger.info(\"***** Eval results for {} on {} for {} *****\".format(eval_task_names, set_type, checkpoint))\n","        writer.write(\"***** Eval results for {} *****\\n\".format(checkpoint))\n","        writer.write(\n","            \"total batch size=%d\\n\"\n","            % (\n","                args.per_gpu_train_batch_size\n","                * args.gradient_accumulation_steps\n","                * (torch.distributed.get_world_size() if args.local_rank != -1 else 1)\n","            )\n","        )\n","        writer.write(\"train num epochs=%d\\n\" % args.num_train_epochs)\n","        writer.write(\"max seq length  =%d\\n\" % MAX_SEQ_LENGTHS[eval_task_names])#args.max_seq_length)\n","        for key in sorted(result.keys()):\n","            logger.info(\"  %s = %s\", key, str(result[key]))\n","            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n","        logger.info(\"\\n\")\n","\n","    # import pdb\n","    # pdb.set_trace()\n","\n","    # Write pred\n","    output_pred_file = os.path.join(eval_output_dir, \"predictions_\" + str(is_test).lower() + \"_eval_results.json\")\n","    with open(output_pred_file, \"w\") as writer:\n","        json.dump(per_id_pred, writer, indent=4)\n","        logger.info(\"Saved {0}\".format(output_pred_file))\n","\n","    # Write pred with label\n","    processor = processors[args.task_name]()\n","    label_list = processor.get_labels()#['A','B','C','D']\n","    idx2label = {i: label for i, label in enumerate(label_list)}\n","    output_pred_file = os.path.join(eval_output_dir, \"predictions_\" + str(is_test).lower() + \"_eval_results_label.json\")\n","    #output_pred_file = '/content/drive/MyDrive/pred_test_labels.json'\n","    with open(output_pred_file, \"w\") as writer:\n","        json.dump({'pred': [idx2label[id] for id in preds.tolist()]}, writer, indent=4)\n","        logger.info(\"Saved {0}\".format(output_pred_file))\n","\n","    # Get error idx\n","    if error:\n","        correct_idx = np.argwhere(preds == out_label_ids).tolist()\n","        wrong_idx = np.argwhere(preds != out_label_ids).tolist()\n","        wrong_idx_dict = {\n","            'correct': correct_idx, 'wrong': wrong_idx,\n","            'preds': preds.tolist(), 'logits': logits_all.tolist(),\n","            'labels': out_label_ids.tolist()\n","        }\n","        output_error_file = os.path.join(eval_output_dir,\"error_idx_{}_{}.json\".format(eval_task_names, set_type))\n","        with open(output_error_file, \"w\") as writer:\n","            json.dump(wrong_idx_dict, writer)\n","            logger.info(\"Saved {0}\".format(output_error_file))\n","\n","    return results"]},{"cell_type":"markdown","metadata":{"id":"fyV2VX7Vk1RA"},"source":["# Main"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"daHi3mJlTi69","executionInfo":{"status":"ok","timestamp":1696194290794,"user_tz":-420,"elapsed":298,"user":{"displayName":"Khôi Hoàng","userId":"04631266824167781561"}}},"outputs":[],"source":["PRJ_DIR = '/content/drive/MyDrive/NCKH/ViMMRC_model'\n","OUTPUT_DIR = PRJ_DIR + '/models/mbert_nli_base_v1'"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"Cu3qVbPBh_7F","executionInfo":{"status":"ok","timestamp":1696194293243,"user_tz":-420,"elapsed":320,"user":{"displayName":"Khôi Hoàng","userId":"04631266824167781561"}}},"outputs":[],"source":["def main(args):\n","\n","    # Setup distant debugging if needed\n","    # if args.server_ip and args.server_port:\n","    #     # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n","    #     import ptvsd\n","\n","    #     print(\"Waiting for debugger attach\")\n","    #     ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n","    #     ptvsd.wait_for_attach()\n","\n","    # Setup CUDA, GPU & distributed training\n","    if args.local_rank == -1 or args.no_cuda:\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n","        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n","    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n","        torch.cuda.set_device(args.local_rank)\n","        device = torch.device(\"cuda\", args.local_rank)\n","        torch.distributed.init_process_group(backend=\"nccl\")\n","        args.n_gpu = 1\n","    logger.info(\"device: %s, n_gpu: %d, distributed training %r\", device, args.n_gpu, bool(args.local_rank != -1))\n","    args.device = device\n","\n","    if args.gradient_accumulation_steps < 1:\n","        raise ValueError(\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\".format(\n","            args.gradient_accumulation_steps))\n","\n","    if os.path.exists(args.output_dir) and os.listdir(args.output_dir):\n","        if args.do_train:\n","            print(\"Output directory ({}) already exists and is not empty.\".format(args.output_dir))\n","    else:\n","        os.makedirs(args.output_dir, exist_ok=True)\n","\n","    # Set seed\n","    set_seed(args)\n","\n","    # Prepare GLUE task\n","    args.task_name = args.task_name.lower()\n","    if args.task_name not in processors:\n","        raise ValueError(\"Task not found: %s\" % (args.task_name))\n","    processor = processors[args.task_name]()\n","    label_list = processor.get_labels()\n","    num_labels = len(label_list)\n","\n","    # Load pretrained model and tokenizer\n","    if args.local_rank not in [-1, 0]:\n","        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n","\n","    args.model_type = args.model_type.lower()\n","    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n","    config = config_class.from_pretrained(\n","        args.config_name if args.config_name else args.model_name_or_path,\n","        num_labels=num_labels,\n","        early_stopping = True,\n","        finetuning_task=args.task_name,\n","        cache_dir=args.cache_dir if args.cache_dir else None,\n","    )\n","    tokenizer = tokenizer_class.from_pretrained(\n","        args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,\n","        do_lower_case=args.do_lower_case,\n","        cache_dir=args.cache_dir if args.cache_dir else None,\n","    )\n","    model = model_class.from_pretrained(\n","        args.model_name_or_path,\n","        from_tf=bool(\".ckpt\" in args.model_name_or_path),\n","        config=config,\n","        cache_dir=args.cache_dir if args.cache_dir else None,\n","        ignore_mismatched_sizes=True\n","    )\n","\n","    if args.local_rank == 0:\n","        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n","\n","    options_print = \"\"\n","    logging.info(\"Arg Options:\")\n","    for arg in vars(args):\n","        options_print += \"opt: %s=%s\\r\\n\" % (arg, getattr(args, arg))\n","    logging.info(options_print)\n","\n","    model.to(args.device)\n","\n","    def get_model_base_obj(model, model_type):\n","        if model_type == \"bert\":\n","            return model.bert\n","        elif model_type == \"xlm-roberta\" or model_type == \"roberta\":\n","            return model.roberta\n","        elif model_type == \"distilbert\":\n","            return model.distilbert\n","        else:\n","            raise ValueError(\"model_type='{0}' is not supported!\")\n","\n","    if args.freeze_embeddings:\n","        for param in list(get_model_base_obj(model, args.model_type).embeddings.parameters()):\n","            param.requires_grad = False\n","        logger.info(\"Froze Embedding Layer\")\n","\n","    # freeze_layers is a string \"1,2,3\" representing layer number\n","    if args.freeze_layers:\n","        layer_indexes = [int(x) for x in args.freeze_layers]\n","        for layer_idx in layer_indexes:\n","            for param in list(\n","                get_model_base_obj(model, args.model_type).encoder.layer[layer_idx].parameters()\n","            ):\n","                param.requires_grad = False\n","            logger.info(\"Froze Layer: %s\", layer_idx)\n","\n","    logger.info(\"Training/evaluation parameters %s\", args)\n","    best_steps = 0\n","\n","    # Training\n","    if args.do_train:\n","        train_dataset = load_and_cache_examples(args, args.task_name, tokenizer, set_type='train')\n","        global_step, tr_loss, best_steps = train(args, train_dataset, model, tokenizer)\n","        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n","\n","    # Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n","    if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n","        # Create output directory if needed\n","        if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n","            os.makedirs(args.output_dir)\n","\n","        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n","        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n","        # They can then be reloaded using `from_pretrained()`\n","        model_to_save = (\n","            model.module if hasattr(model, \"module\") else model\n","        )  # Take care of distributed/parallel training\n","        model_to_save.save_pretrained(args.output_dir)\n","        tokenizer.save_pretrained(args.output_dir)\n","\n","        # Good practice: save your training arguments together with the trained model\n","        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n","\n","        # Load a trained model and vocabulary that you have fine-tuned\n","        model = model_class.from_pretrained(args.output_dir)\n","        tokenizer = tokenizer_class.from_pretrained(args.output_dir)\n","        model.to(args.device)\n","\n","    # Evaluation\n","    results = {}\n","    if args.do_eval and args.local_rank in [-1, 0]:\n","        if not args.do_train:\n","            args.output_dir = args.model_name_or_path\n","        checkpoints = [args.output_dir]\n","        if args.eval_all_checkpoints:\n","            checkpoints = list(\n","                os.path.dirname(c)\n","                for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n","            )\n","            logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n","        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n","        for checkpoint in checkpoints:\n","            global_step = checkpoint.split(\"/\")[-1] if len(checkpoints) > 1 else \"\"\n","            model = model_class.from_pretrained(checkpoint)\n","            model.to(args.device)\n","            result = evaluate(args, model, tokenizer, global_step)\n","            result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n","            results.update(result)\n","\n","    if args.do_test and args.local_rank in [-1, 0]:\n","        checkpoints = [args.output_dir]\n","        if args.eval_all_checkpoints:\n","            checkpoints = list(\n","                os.path.dirname(c)\n","                for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n","            )\n","        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n","        for checkpoint in checkpoints:\n","            global_step = checkpoint.split(\"/\")[-1]\n","            model = model_class.from_pretrained(checkpoint)\n","            model.to(args.device)\n","            result = evaluate(args, model, tokenizer, global_step, is_test=True)\n","            result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n","            results.update(result)\n","    # if best_steps:\n","    #     logger.info(\"best steps of eval acc is the following checkpoints: %s\", best_steps)\n","    return results\n"]},{"cell_type":"markdown","metadata":{"id":"Ix8RCrSpZSoL"},"source":["## Test"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1696190201809,"user":{"displayName":"Khôi Hoàng","userId":"04631266824167781561"},"user_tz":-420},"id":"z4Vc0QbZ1XY7","outputId":"1d9cdfab-e54c-43d1-d992-2da1c391f5a8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'data_dir': '/content/drive/MyDrive/NCKH/ViMMRC_model/dataset/ViMMRC_RACE_v1',\n"," 'model_type': 'bert',\n"," 'model_name_or_path': '/content/drive/MyDrive/NCKH/ViMMRC_model/models/vibert_nli_base',\n"," 'task_name': 'vimmrc_race',\n"," 'output_predictions': True,\n"," 'output_dir': '/content/drive/MyDrive/NCKH/ViMMRC_model/models/vibert_nli_base_v1',\n"," 'freeze_embeddings': False,\n"," 'freeze_layers': None,\n"," 'tb_log_dir': '',\n"," 'config_name': '',\n"," 'tokenizer_name': '',\n"," 'cache_dir': '/content/drive/MyDrive/NCKH/ViMMRC_model/models/cached',\n"," 'max_seq_length': 512,\n"," 'do_train': False,\n"," 'do_eval': False,\n"," 'do_test': True,\n"," 'evaluate_during_training': True,\n"," 'do_lower_case': False,\n"," 'per_gpu_train_batch_size': 4,\n"," 'per_gpu_eval_batch_size': 4,\n"," 'gradient_accumulation_steps': 8,\n"," 'learning_rate': 3e-05,\n"," 'weight_decay': 0.01,\n"," 'max_grad_norm': 1.0,\n"," 'num_train_epochs': 10,\n"," 'max_steps': -1,\n"," 'warmup_proportion': 0.1,\n"," 'eval_all_checkpoints': True,\n"," 'no_cuda': False,\n"," 'seed': 42,\n"," 'local_rank': -1,\n"," 'f': '/root/.local/share/jupyter/runtime/kernel-c88a920f-9baa-4f49-a653-94f2d330dc59.json'}"]},"metadata":{},"execution_count":24}],"source":["parser = argparse.ArgumentParser()\n","\n","if True:\n","    # Required parameters\n","    parser.add_argument(\n","        \"--data_dir\", default=\"{}/dataset/ViMMRC_RACE_v1\".format(PRJ_DIR), type=str,\n","        help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\",\n","    )\n","\n","    parser.add_argument(\n","        \"--model_type\", default=\"bert\", type=str,\n","        help=\"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys()),\n","    )\n","    parser.add_argument(\n","        \"--model_name_or_path\", default=\"{}/models/mbert_nli_base\".format(PRJ_DIR), type=str,\n","        help=\"Path to pre-trained model or shortcut name selected in the list: \" + \", \".join(ALL_MODELS),\n","    )\n","\n","    parser.add_argument(\n","        \"--task_name\", default=\"vimmrc_race\", type=str,\n","        help=\"The name of the task to train selected in the list: \" + \", \".join(processors.keys()),\n","    )\n","    # parser.add_argument(\n","    #     \"--para_type\", default=\"per_choice\", type=str,\n","    #     choices=[\"per_choice\", \"concat_choices\", \"ignore\"],\n","    #     help=\"Paragraph building strategy for ARC (default: %(default)s)\",\n","    # )\n","    parser.add_argument(\n","        \"--output_predictions\", default=True, type=bool, help=\"Whether to export the predictions from the eval step.\",\n","    )\n","    parser.add_argument(\n","        \"--output_dir\", default=OUTPUT_DIR, type=str, help=\"The output directory where the model predictions and checkpoints will be written.\",\n","    )\n","    parser.add_argument(\"--freeze_embeddings\", default=False, action=\"store_true\", help=\"Whether to freeze the embeeding layer.\",)\n","    parser.add_argument(\"--freeze_layers\", nargs=\"*\", help=\"Whether to freeze the embeeding layer.\",)\n","\n","    # Other parameters\n","    parser.add_argument(\"--tb_log_dir\", default=\"\", type=str, help=\"Tensorboard log dir for the current experiment\")\n","    parser.add_argument(\"--config_name\", default=\"\", type=str, help=\"Pretrained config name or path if not the same as model_name\")\n","    parser.add_argument(\"--tokenizer_name\", default=\"\", type=str, help=\"Pretrained tokenizer name or path if not the same as model_name\")\n","    parser.add_argument(\n","        \"--cache_dir\", default=\"{}/models/cached\".format(PRJ_DIR), type=str, help=\"Where do you want to store the pre-trained models downloaded from s3\",\n","    )\n","    parser.add_argument(\n","        \"--max_seq_length\", default=MAX_SEQ_LENGTHS['vimmrc'], type=int,\n","        help=\"The maximum total input sequence length after tokenization. Sequences longer than this will be truncated, sequences shorter will be padded.\",\n","    )\n","    parser.add_argument(\"--do_train\", default = False, action=\"store_true\", help=\"Whether to run training.\")\n","    parser.add_argument(\"--do_eval\", default = False, action=\"store_true\", help=\"Whether to run eval on the dev set.\")\n","    parser.add_argument(\"--do_test\", default = True, action=\"store_true\", help=\"Whether to run test on the test set\")\n","    parser.add_argument(\n","        \"--evaluate_during_training\", action=\"store_true\", default = True, help=\"Run evaluation during training at each logging step.\",\n","    )\n","    parser.add_argument(\n","        \"--do_lower_case\", action=\"store_true\", default = False, help=\"Set this flag if you are using an uncased model.\",\n","    )\n","\n","    parser.add_argument(\"--per_gpu_train_batch_size\", default=4, type=int, help=\"Batch size per GPU/CPU for training.\",)\n","    parser.add_argument(\"--per_gpu_eval_batch_size\", default=4, type=int, help=\"Batch size per GPU/CPU for evaluation.\",)\n","    parser.add_argument(\n","        \"--gradient_accumulation_steps\", type=int, default=8, help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n","    )\n","    parser.add_argument(\"--learning_rate\", default=3e-5, type=float, help=\"The initial learning rate for Adam.\")\n","    parser.add_argument(\"--weight_decay\", default=0.01, type=float, help=\"Weight decay if we apply some.\")\n","    # parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n","    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n","    parser.add_argument(\"--num_train_epochs\", default=10, type=float, help=\"Total number of training epochs to perform.\")\n","    parser.add_argument(\"--max_steps\", default=-1, type=int, help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\")\n","    parser.add_argument(\"--warmup_proportion\", default=0.1, type=float, help=\"Linear warmup over warmup_proportion.\")\n","\n","    # parser.add_argument(\"--logging_steps\", type=int, default=100, help=\"Log every X updates steps.\")\n","    # parser.add_argument(\"--save_steps\", type=int, default=500, help=\"Save checkpoint every X updates steps.\")\n","    parser.add_argument(\n","        \"--eval_all_checkpoints\", default=True, action=\"store_true\",\n","        help=\"Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number\",\n","    )\n","    parser.add_argument(\"--no_cuda\", default=False, action=\"store_true\", help=\"Avoid using CUDA when available\")\n","    # parser.add_argument(\n","    #     \"--overwrite_output_dir\", default = True, action=\"store_true\", help=\"Overwrite the content of the output directory\",\n","    # )\n","    # parser.add_argument(\n","    #     \"--overwrite_cache\", default = True, action=\"store_true\", help=\"Overwrite the cached training and evaluation sets\",\n","    # )\n","    parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization\")\n","\n","    # parser.add_argument(\n","    #     \"--fp16\", action=\"store_true\", help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n","    # )\n","    # parser.add_argument(\n","    #     \"--fp16_opt_level\", type=str, default=\"O1\",\n","    #     help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n","    #     \"See details at https://nvidia.github.io/apex/amp.html\",\n","    # )\n","    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n","    # parser.add_argument(\"--server_ip\", type=str, default=\"\", help=\"For distant debugging.\")\n","    # parser.add_argument(\"--server_port\", type=str, default=\"\", help=\"For distant debugging.\")\n","\n","    parser.add_argument('-f')\n","\n","args = parser.parse_args()\n","vars(args)\n","# options_print = \"\"\n","# logger.info(\"Arg Options - input:\")\n","# for arg in vars(args):\n","#     options_print += \"opt: %s=%s\\r\\n\" % (arg, getattr(args, arg))\n","# logger.info(options_print)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q_0j2qEMx2Ry","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1696190220404,"user_tz":-420,"elapsed":9707,"user":{"displayName":"Khôi Hoàng","userId":"04631266824167781561"}},"outputId":"b275ecb1-ef34-4a45-9d9a-031a790f24e0"},"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:__main__:device: cuda, n_gpu: 1, distributed training False\n","INFO:__main__:Training/evaluation parameters Namespace(data_dir='/content/drive/MyDrive/NCKH/ViMMRC_model/dataset/ViMMRC_RACE_v1', model_type='bert', model_name_or_path='/content/drive/MyDrive/NCKH/ViMMRC_model/models/vibert_nli_base', task_name='vimmrc_race', output_predictions=True, output_dir='/content/drive/MyDrive/NCKH/ViMMRC_model/models/vibert_nli_base_v1', freeze_embeddings=False, freeze_layers=None, tb_log_dir='', config_name='', tokenizer_name='', cache_dir='/content/drive/MyDrive/NCKH/ViMMRC_model/models/cached', max_seq_length=512, do_train=False, do_eval=False, do_test=True, evaluate_during_training=True, do_lower_case=False, per_gpu_train_batch_size=4, per_gpu_eval_batch_size=4, gradient_accumulation_steps=8, learning_rate=3e-05, weight_decay=0.01, max_grad_norm=1.0, num_train_epochs=10, max_steps=-1, warmup_proportion=0.1, eval_all_checkpoints=True, no_cuda=False, seed=42, local_rank=-1, f='/root/.local/share/jupyter/runtime/kernel-c88a920f-9baa-4f49-a653-94f2d330dc59.json', n_gpu=1, device=device(type='cuda'))\n","INFO:__main__:Evaluate the following checkpoints: []\n"]}],"source":["if __name__ == \"__main__\":\n","    main(args)"]},{"cell_type":"markdown","metadata":{"id":"WK1rLBmKOz8L"},"source":["### epoch x test"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":306,"status":"ok","timestamp":1696194024144,"user":{"displayName":"Khôi Hoàng","userId":"04631266824167781561"},"user_tz":-420},"id":"_6FhM-fbO1xW","outputId":"810548a7-ffa0-4e08-bec4-4b3c956cd7a5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'data_dir': '/content/drive/MyDrive/NCKH/ViMMRC_model/dataset/ViMMRC_RACE_v1',\n"," 'model_type': 'bert',\n"," 'model_name_or_path': '/content/drive/MyDrive/NCKH/ViMMRC_model/models/mbert_nli',\n"," 'task_name': 'vimmrc_race',\n"," 'output_predictions': True,\n"," 'output_dir': '/content/drive/MyDrive/NCKH/ViMMRC_model/models/mbert_nli_base/epoch_8',\n"," 'freeze_embeddings': False,\n"," 'freeze_layers': None,\n"," 'tb_log_dir': '',\n"," 'config_name': '',\n"," 'tokenizer_name': '',\n"," 'cache_dir': '/content/drive/MyDrive/NCKH/ViMMRC_model/models/cached',\n"," 'max_seq_length': 512,\n"," 'do_train': False,\n"," 'do_eval': False,\n"," 'do_test': True,\n"," 'evaluate_during_training': True,\n"," 'do_lower_case': False,\n"," 'per_gpu_train_batch_size': 4,\n"," 'per_gpu_eval_batch_size': 4,\n"," 'gradient_accumulation_steps': 8,\n"," 'learning_rate': 3e-05,\n"," 'weight_decay': 0.01,\n"," 'max_grad_norm': 1.0,\n"," 'num_train_epochs': 10,\n"," 'max_steps': -1,\n"," 'warmup_proportion': 0.1,\n"," 'eval_all_checkpoints': True,\n"," 'no_cuda': False,\n"," 'seed': 42,\n"," 'local_rank': -1,\n"," 'f': '/root/.local/share/jupyter/runtime/kernel-08a5fa07-d22f-49b5-9e98-9f65bf9522c0.json'}"]},"metadata":{},"execution_count":17}],"source":["parser = argparse.ArgumentParser()\n","\n","if True:\n","    # Required parameters\n","    parser.add_argument(\n","        \"--data_dir\", default=\"{}/dataset/ViMMRC_RACE_v1\".format(PRJ_DIR), type=str,\n","        help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\",\n","    )\n","\n","    parser.add_argument(\n","        \"--model_type\", default=\"bert\", type=str,\n","        help=\"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys()),\n","    )\n","    parser.add_argument(\n","        \"--model_name_or_path\", default=\"{}/models/mbert_nli\".format(PRJ_DIR), type=str,\n","        help=\"Path to pre-trained model or shortcut name selected in the list: \" + \", \".join(ALL_MODELS),\n","    )\n","\n","    parser.add_argument(\n","        \"--task_name\", default=\"vimmrc_race\", type=str,\n","        help=\"The name of the task to train selected in the list: \" + \", \".join(processors.keys()),\n","    )\n","    # parser.add_argument(\n","    #     \"--para_type\", default=\"per_choice\", type=str,\n","    #     choices=[\"per_choice\", \"concat_choices\", \"ignore\"],\n","    #     help=\"Paragraph building strategy for ARC (default: %(default)s)\",\n","    # )\n","    parser.add_argument(\n","        \"--output_predictions\", default=True, type=bool, help=\"Whether to export the predictions from the eval step.\",\n","    )\n","    parser.add_argument(\n","        \"--output_dir\", default=PRJ_DIR + '/models/mbert_nli_base'+'/epoch_8', type=str, help=\"The output directory where the model predictions and checkpoints will be written.\",\n","    )\n","    parser.add_argument(\"--freeze_embeddings\", default=False, action=\"store_true\", help=\"Whether to freeze the embeeding layer.\",)\n","    parser.add_argument(\"--freeze_layers\", nargs=\"*\", help=\"Whether to freeze the embeeding layer.\",)\n","\n","    # Other parameters\n","    parser.add_argument(\"--tb_log_dir\", default=\"\", type=str, help=\"Tensorboard log dir for the current experiment\")\n","    parser.add_argument(\"--config_name\", default=\"\", type=str, help=\"Pretrained config name or path if not the same as model_name\")\n","    parser.add_argument(\"--tokenizer_name\", default=\"\", type=str, help=\"Pretrained tokenizer name or path if not the same as model_name\")\n","    parser.add_argument(\n","        \"--cache_dir\", default=\"{}/models/cached\".format(PRJ_DIR), type=str, help=\"Where do you want to store the pre-trained models downloaded from s3\",\n","    )\n","    parser.add_argument(\n","        \"--max_seq_length\", default=MAX_SEQ_LENGTHS['vimmrc'], type=int,\n","        help=\"The maximum total input sequence length after tokenization. Sequences longer than this will be truncated, sequences shorter will be padded.\",\n","    )\n","    parser.add_argument(\"--do_train\", default = False, action=\"store_true\", help=\"Whether to run training.\")\n","    parser.add_argument(\"--do_eval\", default = False, action=\"store_true\", help=\"Whether to run eval on the dev set.\")\n","    parser.add_argument(\"--do_test\", default = True, action=\"store_true\", help=\"Whether to run test on the test set\")\n","    parser.add_argument(\n","        \"--evaluate_during_training\", action=\"store_true\", default = True, help=\"Run evaluation during training at each logging step.\",\n","    )\n","    parser.add_argument(\n","        \"--do_lower_case\", action=\"store_true\", default = False, help=\"Set this flag if you are using an uncased model.\",\n","    )\n","\n","    parser.add_argument(\"--per_gpu_train_batch_size\", default=4, type=int, help=\"Batch size per GPU/CPU for training.\",)\n","    parser.add_argument(\"--per_gpu_eval_batch_size\", default=4, type=int, help=\"Batch size per GPU/CPU for evaluation.\",)\n","    parser.add_argument(\n","        \"--gradient_accumulation_steps\", type=int, default=8, help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n","    )\n","    parser.add_argument(\"--learning_rate\", default=3e-5, type=float, help=\"The initial learning rate for Adam.\")\n","    parser.add_argument(\"--weight_decay\", default=0.01, type=float, help=\"Weight decay if we apply some.\")\n","    # parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n","    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n","    parser.add_argument(\"--num_train_epochs\", default=10, type=float, help=\"Total number of training epochs to perform.\")\n","    parser.add_argument(\"--max_steps\", default=-1, type=int, help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\")\n","    parser.add_argument(\"--warmup_proportion\", default=0.1, type=float, help=\"Linear warmup over warmup_proportion.\")\n","\n","    # parser.add_argument(\"--logging_steps\", type=int, default=100, help=\"Log every X updates steps.\")\n","    # parser.add_argument(\"--save_steps\", type=int, default=500, help=\"Save checkpoint every X updates steps.\")\n","    parser.add_argument(\n","        \"--eval_all_checkpoints\", default=True, action=\"store_true\",\n","        help=\"Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number\",\n","    )\n","    parser.add_argument(\"--no_cuda\", default=False, action=\"store_true\", help=\"Avoid using CUDA when available\")\n","    # parser.add_argument(\n","    #     \"--overwrite_output_dir\", default = True, action=\"store_true\", help=\"Overwrite the content of the output directory\",\n","    # )\n","    # parser.add_argument(\n","    #     \"--overwrite_cache\", default = True, action=\"store_true\", help=\"Overwrite the cached training and evaluation sets\",\n","    # )\n","    parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization\")\n","\n","    # parser.add_argument(\n","    #     \"--fp16\", action=\"store_true\", help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n","    # )\n","    # parser.add_argument(\n","    #     \"--fp16_opt_level\", type=str, default=\"O1\",\n","    #     help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n","    #     \"See details at https://nvidia.github.io/apex/amp.html\",\n","    # )\n","    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n","    # parser.add_argument(\"--server_ip\", type=str, default=\"\", help=\"For distant debugging.\")\n","    # parser.add_argument(\"--server_port\", type=str, default=\"\", help=\"For distant debugging.\")\n","\n","    parser.add_argument('-f')\n","\n","args = parser.parse_args()\n","vars(args)"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":381,"referenced_widgets":["cc4aae1f008648a9b20a8646f18aaeac","27683c3e60ab4a9eb47b094228c47d96","ecd4f322a67e4ac4929a721c3f4f2bc3","36173649bfc440a2847d1de92cec51cc","662941fc540f4137970c885f9d0f9cca","f4a7f7233ba14ce09f0e5635d32fe509","f72b901bd894430986e51063c1b1cd04","7b3825d3e1404304b436b4bf53e21c66","09110b89485c49bc8861b5f37a5f8c1b","6465d54421f54d319298ee8267dfd094","9865d0bd868a4978b1b56bd911f46644"]},"id":"QgnGrKe1O7Hr","outputId":"0c00ed17-7b3a-4395-8eed-2e36b29960f4","executionInfo":{"status":"ok","timestamp":1696194140216,"user_tz":-420,"elapsed":112007,"user":{"displayName":"Khôi Hoàng","userId":"04631266824167781561"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:__main__:device: cuda, n_gpu: 1, distributed training False\n","Some weights of BertForMultipleChoice were not initialized from the model checkpoint at /content/drive/MyDrive/NCKH/ViMMRC_model/models/mbert_nli and are newly initialized because the shapes did not match:\n","- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n","- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([1]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","INFO:__main__:Training/evaluation parameters Namespace(data_dir='/content/drive/MyDrive/NCKH/ViMMRC_model/dataset/ViMMRC_RACE_v1', model_type='bert', model_name_or_path='/content/drive/MyDrive/NCKH/ViMMRC_model/models/mbert_nli', task_name='vimmrc_race', output_predictions=True, output_dir='/content/drive/MyDrive/NCKH/ViMMRC_model/models/mbert_nli_base/epoch_8', freeze_embeddings=False, freeze_layers=None, tb_log_dir='', config_name='', tokenizer_name='', cache_dir='/content/drive/MyDrive/NCKH/ViMMRC_model/models/cached', max_seq_length=512, do_train=False, do_eval=False, do_test=True, evaluate_during_training=True, do_lower_case=False, per_gpu_train_batch_size=4, per_gpu_eval_batch_size=4, gradient_accumulation_steps=8, learning_rate=3e-05, weight_decay=0.01, max_grad_norm=1.0, num_train_epochs=10, max_steps=-1, warmup_proportion=0.1, eval_all_checkpoints=True, no_cuda=False, seed=42, local_rank=-1, f='/root/.local/share/jupyter/runtime/kernel-08a5fa07-d22f-49b5-9e98-9f65bf9522c0.json', n_gpu=1, device=device(type='cuda'))\n","INFO:__main__:Evaluate the following checkpoints: ['/content/drive/MyDrive/NCKH/ViMMRC_model/models/mbert_nli_base/epoch_8']\n","INFO:__main__:Loading features from cached file /content/drive/MyDrive/NCKH/ViMMRC_model/dataset/ViMMRC_RACE_v1/cached_test_mbert_nli_512_vimmrc_race\n","INFO:__main__:***** Running evaluation for vimmrc_race on test for epoch_8 *****\n","INFO:__main__:  Num examples = 514\n","INFO:__main__:  Batch size = 4\n"]},{"output_type":"display_data","data":{"text/plain":["Evaluating:   0%|          | 0/129 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc4aae1f008648a9b20a8646f18aaeac"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:__main__:***** Eval results for vimmrc_race on test for epoch_8 *****\n","INFO:__main__:  eval_acc = 0.7178988326848249\n","INFO:__main__:  eval_loss = 0.7739456604494778\n","INFO:__main__:\n","\n","INFO:__main__:Saved epoch_8/predictions_true_eval_results.json\n","INFO:__main__:Saved epoch_8/predictions_true_eval_results_label.json\n"]}],"source":["if __name__ == \"__main__\":\n","    main(args)"]},{"cell_type":"markdown","source":["### dev"],"metadata":{"id":"GJlEv2MSRpx1"}},{"cell_type":"code","source":["parser = argparse.ArgumentParser()\n","\n","if True:\n","    # Required parameters\n","    parser.add_argument(\n","        \"--data_dir\", default=\"{}/dataset/ViMMRC_RACE_v1\".format(PRJ_DIR), type=str,\n","        help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\",\n","    )\n","\n","    parser.add_argument(\n","        \"--model_type\", default=\"bert\", type=str,\n","        help=\"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys()),\n","    )\n","    parser.add_argument(\n","        \"--model_name_or_path\", default=\"{}/models/mbert_nli_base\".format(PRJ_DIR), type=str,\n","        help=\"Path to pre-trained model or shortcut name selected in the list: \" + \", \".join(ALL_MODELS),\n","    )\n","\n","    parser.add_argument(\n","        \"--task_name\", default=\"vimmrc_race\", type=str,\n","        help=\"The name of the task to train selected in the list: \" + \", \".join(processors.keys()),\n","    )\n","    # parser.add_argument(\n","    #     \"--para_type\", default=\"per_choice\", type=str,\n","    #     choices=[\"per_choice\", \"concat_choices\", \"ignore\"],\n","    #     help=\"Paragraph building strategy for ARC (default: %(default)s)\",\n","    # )\n","    parser.add_argument(\n","        \"--output_predictions\", default=True, type=bool, help=\"Whether to export the predictions from the eval step.\",\n","    )\n","    parser.add_argument(\n","        \"--output_dir\", default=PRJ_DIR + '/models/mbert_nli_base'+'/epoch_8', type=str, help=\"The output directory where the model predictions and checkpoints will be written.\",\n","    )\n","    parser.add_argument(\"--freeze_embeddings\", default=False, action=\"store_true\", help=\"Whether to freeze the embeeding layer.\",)\n","    parser.add_argument(\"--freeze_layers\", nargs=\"*\", help=\"Whether to freeze the embeeding layer.\",)\n","\n","    # Other parameters\n","    parser.add_argument(\"--tb_log_dir\", default=\"\", type=str, help=\"Tensorboard log dir for the current experiment\")\n","    parser.add_argument(\"--config_name\", default=\"\", type=str, help=\"Pretrained config name or path if not the same as model_name\")\n","    parser.add_argument(\"--tokenizer_name\", default=\"\", type=str, help=\"Pretrained tokenizer name or path if not the same as model_name\")\n","    parser.add_argument(\n","        \"--cache_dir\", default=\"{}/models/cached\".format(PRJ_DIR), type=str, help=\"Where do you want to store the pre-trained models downloaded from s3\",\n","    )\n","    parser.add_argument(\n","        \"--max_seq_length\", default=MAX_SEQ_LENGTHS['vimmrc'], type=int,\n","        help=\"The maximum total input sequence length after tokenization. Sequences longer than this will be truncated, sequences shorter will be padded.\",\n","    )\n","    parser.add_argument(\"--do_train\", default = False, action=\"store_true\", help=\"Whether to run training.\")\n","    parser.add_argument(\"--do_eval\", default = True, action=\"store_true\", help=\"Whether to run eval on the dev set.\")\n","    parser.add_argument(\"--do_test\", default = False, action=\"store_true\", help=\"Whether to run test on the test set\")\n","    parser.add_argument(\n","        \"--evaluate_during_training\", action=\"store_true\", default = True, help=\"Run evaluation during training at each logging step.\",\n","    )\n","    parser.add_argument(\n","        \"--do_lower_case\", action=\"store_true\", default = False, help=\"Set this flag if you are using an uncased model.\",\n","    )\n","\n","    parser.add_argument(\"--per_gpu_train_batch_size\", default=4, type=int, help=\"Batch size per GPU/CPU for training.\",)\n","    parser.add_argument(\"--per_gpu_eval_batch_size\", default=4, type=int, help=\"Batch size per GPU/CPU for evaluation.\",)\n","    parser.add_argument(\n","        \"--gradient_accumulation_steps\", type=int, default=8, help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n","    )\n","    parser.add_argument(\"--learning_rate\", default=3e-5, type=float, help=\"The initial learning rate for Adam.\")\n","    parser.add_argument(\"--weight_decay\", default=0.01, type=float, help=\"Weight decay if we apply some.\")\n","    # parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n","    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n","    parser.add_argument(\"--num_train_epochs\", default=10, type=float, help=\"Total number of training epochs to perform.\")\n","    parser.add_argument(\"--max_steps\", default=-1, type=int, help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\")\n","    parser.add_argument(\"--warmup_proportion\", default=0.1, type=float, help=\"Linear warmup over warmup_proportion.\")\n","\n","    # parser.add_argument(\"--logging_steps\", type=int, default=100, help=\"Log every X updates steps.\")\n","    # parser.add_argument(\"--save_steps\", type=int, default=500, help=\"Save checkpoint every X updates steps.\")\n","    parser.add_argument(\n","        \"--eval_all_checkpoints\", default=True, action=\"store_true\",\n","        help=\"Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number\",\n","    )\n","    parser.add_argument(\"--no_cuda\", default=False, action=\"store_true\", help=\"Avoid using CUDA when available\")\n","    # parser.add_argument(\n","    #     \"--overwrite_output_dir\", default = True, action=\"store_true\", help=\"Overwrite the content of the output directory\",\n","    # )\n","    # parser.add_argument(\n","    #     \"--overwrite_cache\", default = True, action=\"store_true\", help=\"Overwrite the cached training and evaluation sets\",\n","    # )\n","    parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization\")\n","\n","    # parser.add_argument(\n","    #     \"--fp16\", action=\"store_true\", help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n","    # )\n","    # parser.add_argument(\n","    #     \"--fp16_opt_level\", type=str, default=\"O1\",\n","    #     help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n","    #     \"See details at https://nvidia.github.io/apex/amp.html\",\n","    # )\n","    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n","    # parser.add_argument(\"--server_ip\", type=str, default=\"\", help=\"For distant debugging.\")\n","    # parser.add_argument(\"--server_port\", type=str, default=\"\", help=\"For distant debugging.\")\n","\n","    parser.add_argument('-f')\n","\n","args = parser.parse_args()\n","vars(args)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ymLyjIGLRr89","executionInfo":{"status":"ok","timestamp":1696194304978,"user_tz":-420,"elapsed":320,"user":{"displayName":"Khôi Hoàng","userId":"04631266824167781561"}},"outputId":"b4a2d54a-5354-4244-acec-de1ba218b802"},"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'data_dir': '/content/drive/MyDrive/NCKH/ViMMRC_model/dataset/ViMMRC_RACE_v1',\n"," 'model_type': 'bert',\n"," 'model_name_or_path': '/content/drive/MyDrive/NCKH/ViMMRC_model/models/mbert_nli_base',\n"," 'task_name': 'vimmrc_race',\n"," 'output_predictions': True,\n"," 'output_dir': '/content/drive/MyDrive/NCKH/ViMMRC_model/models/mbert_nli_base/epoch_8',\n"," 'freeze_embeddings': False,\n"," 'freeze_layers': None,\n"," 'tb_log_dir': '',\n"," 'config_name': '',\n"," 'tokenizer_name': '',\n"," 'cache_dir': '/content/drive/MyDrive/NCKH/ViMMRC_model/models/cached',\n"," 'max_seq_length': 512,\n"," 'do_train': False,\n"," 'do_eval': True,\n"," 'do_test': False,\n"," 'evaluate_during_training': True,\n"," 'do_lower_case': False,\n"," 'per_gpu_train_batch_size': 4,\n"," 'per_gpu_eval_batch_size': 4,\n"," 'gradient_accumulation_steps': 8,\n"," 'learning_rate': 3e-05,\n"," 'weight_decay': 0.01,\n"," 'max_grad_norm': 1.0,\n"," 'num_train_epochs': 10,\n"," 'max_steps': -1,\n"," 'warmup_proportion': 0.1,\n"," 'eval_all_checkpoints': True,\n"," 'no_cuda': False,\n"," 'seed': 42,\n"," 'local_rank': -1,\n"," 'f': '/root/.local/share/jupyter/runtime/kernel-08a5fa07-d22f-49b5-9e98-9f65bf9522c0.json'}"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    main(args)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":535,"referenced_widgets":["29131917ed334f57a513321a81eb18b0","85e17567152147caac85e77b57d042cb","5062650ad8784a7e899650fd210d625b","edb3c7ce782b441e9feb955077fe591a","3b749ade40c747248b8c8e6517c0beb4","18017945fd3849a4ae5a3a37943eb544","d671d1d76fa944efac5f3814937ef03f","347dd501ad0d4750a87bf6444bc65d50","ad913350f8d44497af52d26b337b9641","7ef3a972879e4a6b87dd84df5c0c59e4","e75acb087ce0489b9030c89ee5558f29","8bd6b20edb1e40baaf8c5f79dd3ac935","8696bd43bb2c470c8136514dcf3f7cdc","aec4b222bf224815ab3b66872a1aca46","cc14064ea5c84b2595df7a1d191cd7be","18e9f661bd514fbfa79fd55d42c052cc","dda16a6a255142848acf54870f735c4c","af88d1b6bf9d47efb252368fc4ffe5eb","8150c27f478046fb9ec97c3eea0ccb44","e16d5050f8ac4a96bd103a7ae32f8bc1","f997c3427e1e4cd48624ed699ac6a2ff","6fccc95fc0ad4dc4b8dfa9e0bace714f"]},"id":"YOi9OyinRvZZ","executionInfo":{"status":"ok","timestamp":1696194412904,"user_tz":-420,"elapsed":105790,"user":{"displayName":"Khôi Hoàng","userId":"04631266824167781561"}},"outputId":"5939b98e-754e-48ff-e84f-ab22dc216e29"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:__main__:device: cuda, n_gpu: 1, distributed training False\n","INFO:__main__:Training/evaluation parameters Namespace(data_dir='/content/drive/MyDrive/NCKH/ViMMRC_model/dataset/ViMMRC_RACE_v1', model_type='bert', model_name_or_path='/content/drive/MyDrive/NCKH/ViMMRC_model/models/mbert_nli_base', task_name='vimmrc_race', output_predictions=True, output_dir='/content/drive/MyDrive/NCKH/ViMMRC_model/models/mbert_nli_base/epoch_8', freeze_embeddings=False, freeze_layers=None, tb_log_dir='', config_name='', tokenizer_name='', cache_dir='/content/drive/MyDrive/NCKH/ViMMRC_model/models/cached', max_seq_length=512, do_train=False, do_eval=True, do_test=False, evaluate_during_training=True, do_lower_case=False, per_gpu_train_batch_size=4, per_gpu_eval_batch_size=4, gradient_accumulation_steps=8, learning_rate=3e-05, weight_decay=0.01, max_grad_norm=1.0, num_train_epochs=10, max_steps=-1, warmup_proportion=0.1, eval_all_checkpoints=True, no_cuda=False, seed=42, local_rank=-1, f='/root/.local/share/jupyter/runtime/kernel-08a5fa07-d22f-49b5-9e98-9f65bf9522c0.json', n_gpu=1, device=device(type='cuda'))\n","INFO:__main__:Evaluate the following checkpoints: ['/content/drive/MyDrive/NCKH/ViMMRC_model/models/mbert_nli_base/epoch_8', '/content/drive/MyDrive/NCKH/ViMMRC_model/models/mbert_nli_base']\n","INFO:__main__:Loading features from cached file /content/drive/MyDrive/NCKH/ViMMRC_model/dataset/ViMMRC_RACE_v1/cached_dev_mbert_nli_base_512_vimmrc_race\n","INFO:__main__:***** Running evaluation for vimmrc_race on dev for epoch_8 *****\n","INFO:__main__:  Num examples = 294\n","INFO:__main__:  Batch size = 4\n"]},{"output_type":"display_data","data":{"text/plain":["Evaluating:   0%|          | 0/74 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29131917ed334f57a513321a81eb18b0"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:__main__:***** Eval results for vimmrc_race on dev for epoch_8 *****\n","INFO:__main__:  eval_acc = 0.7857142857142857\n","INFO:__main__:  eval_loss = 0.6062577972062737\n","INFO:__main__:\n","\n","INFO:__main__:Saved epoch_8/predictions_false_eval_results.json\n","INFO:__main__:Saved epoch_8/predictions_false_eval_results_label.json\n","INFO:__main__:Loading features from cached file /content/drive/MyDrive/NCKH/ViMMRC_model/dataset/ViMMRC_RACE_v1/cached_dev_mbert_nli_base_512_vimmrc_race\n","INFO:__main__:***** Running evaluation for vimmrc_race on dev for mbert_nli_base *****\n","INFO:__main__:  Num examples = 294\n","INFO:__main__:  Batch size = 4\n"]},{"output_type":"display_data","data":{"text/plain":["Evaluating:   0%|          | 0/74 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8bd6b20edb1e40baaf8c5f79dd3ac935"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:__main__:***** Eval results for vimmrc_race on dev for mbert_nli_base *****\n","INFO:__main__:  eval_acc = 0.782312925170068\n","INFO:__main__:  eval_loss = 0.6848980618665952\n","INFO:__main__:\n","\n","INFO:__main__:Saved mbert_nli_base/predictions_false_eval_results.json\n","INFO:__main__:Saved mbert_nli_base/predictions_false_eval_results_label.json\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["ioQPuCNdks1x","uWwt6WujlC1Q","yoiGjVs5kuwD","42dvg6DqZPg2","WK1rLBmKOz8L"],"provenance":[],"gpuClass":"premium","gpuType":"T4"},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.0"},"vscode":{"interpreter":{"hash":"5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"cc4aae1f008648a9b20a8646f18aaeac":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_27683c3e60ab4a9eb47b094228c47d96","IPY_MODEL_ecd4f322a67e4ac4929a721c3f4f2bc3","IPY_MODEL_36173649bfc440a2847d1de92cec51cc"],"layout":"IPY_MODEL_662941fc540f4137970c885f9d0f9cca"}},"27683c3e60ab4a9eb47b094228c47d96":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f4a7f7233ba14ce09f0e5635d32fe509","placeholder":"​","style":"IPY_MODEL_f72b901bd894430986e51063c1b1cd04","value":"Evaluating: 100%"}},"ecd4f322a67e4ac4929a721c3f4f2bc3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b3825d3e1404304b436b4bf53e21c66","max":129,"min":0,"orientation":"horizontal","style":"IPY_MODEL_09110b89485c49bc8861b5f37a5f8c1b","value":129}},"36173649bfc440a2847d1de92cec51cc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6465d54421f54d319298ee8267dfd094","placeholder":"​","style":"IPY_MODEL_9865d0bd868a4978b1b56bd911f46644","value":" 129/129 [01:12&lt;00:00,  2.04it/s]"}},"662941fc540f4137970c885f9d0f9cca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f4a7f7233ba14ce09f0e5635d32fe509":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f72b901bd894430986e51063c1b1cd04":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7b3825d3e1404304b436b4bf53e21c66":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"09110b89485c49bc8861b5f37a5f8c1b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6465d54421f54d319298ee8267dfd094":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9865d0bd868a4978b1b56bd911f46644":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"29131917ed334f57a513321a81eb18b0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_85e17567152147caac85e77b57d042cb","IPY_MODEL_5062650ad8784a7e899650fd210d625b","IPY_MODEL_edb3c7ce782b441e9feb955077fe591a"],"layout":"IPY_MODEL_3b749ade40c747248b8c8e6517c0beb4"}},"85e17567152147caac85e77b57d042cb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_18017945fd3849a4ae5a3a37943eb544","placeholder":"​","style":"IPY_MODEL_d671d1d76fa944efac5f3814937ef03f","value":"Evaluating: 100%"}},"5062650ad8784a7e899650fd210d625b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_347dd501ad0d4750a87bf6444bc65d50","max":74,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ad913350f8d44497af52d26b337b9641","value":74}},"edb3c7ce782b441e9feb955077fe591a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7ef3a972879e4a6b87dd84df5c0c59e4","placeholder":"​","style":"IPY_MODEL_e75acb087ce0489b9030c89ee5558f29","value":" 74/74 [00:42&lt;00:00,  2.09it/s]"}},"3b749ade40c747248b8c8e6517c0beb4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"18017945fd3849a4ae5a3a37943eb544":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d671d1d76fa944efac5f3814937ef03f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"347dd501ad0d4750a87bf6444bc65d50":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad913350f8d44497af52d26b337b9641":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7ef3a972879e4a6b87dd84df5c0c59e4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e75acb087ce0489b9030c89ee5558f29":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8bd6b20edb1e40baaf8c5f79dd3ac935":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8696bd43bb2c470c8136514dcf3f7cdc","IPY_MODEL_aec4b222bf224815ab3b66872a1aca46","IPY_MODEL_cc14064ea5c84b2595df7a1d191cd7be"],"layout":"IPY_MODEL_18e9f661bd514fbfa79fd55d42c052cc"}},"8696bd43bb2c470c8136514dcf3f7cdc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dda16a6a255142848acf54870f735c4c","placeholder":"​","style":"IPY_MODEL_af88d1b6bf9d47efb252368fc4ffe5eb","value":"Evaluating: 100%"}},"aec4b222bf224815ab3b66872a1aca46":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8150c27f478046fb9ec97c3eea0ccb44","max":74,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e16d5050f8ac4a96bd103a7ae32f8bc1","value":74}},"cc14064ea5c84b2595df7a1d191cd7be":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f997c3427e1e4cd48624ed699ac6a2ff","placeholder":"​","style":"IPY_MODEL_6fccc95fc0ad4dc4b8dfa9e0bace714f","value":" 74/74 [00:41&lt;00:00,  2.03it/s]"}},"18e9f661bd514fbfa79fd55d42c052cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dda16a6a255142848acf54870f735c4c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af88d1b6bf9d47efb252368fc4ffe5eb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8150c27f478046fb9ec97c3eea0ccb44":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e16d5050f8ac4a96bd103a7ae32f8bc1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f997c3427e1e4cd48624ed699ac6a2ff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6fccc95fc0ad4dc4b8dfa9e0bace714f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}